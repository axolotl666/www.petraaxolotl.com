---
title: "Lie Algebras in Particle Physics"
Description: "Errata, Notes, and Problem Solutions to Lie Algebras in Particle Physics"
date: 2017-01-02T14:59:31-08:00
---

This is a great book. However, a better book for beginners may be Zee's _Group Theory in a Nutshell for Physicists_. Zee's book is mathematically
less rigorous and sometimes made me cringe a bit. But it has a much better overview of finite groups, which are quite important for a beginner to
build intuition. It also covers Lorentz's group, which is obviously important for special relativity and quantum field theory.

# Errata and Notes

## Chapter 1

### 1.4 Irreducible representations

#### Eq. (1.10)

It would be much easier if we were talking about (unitary) matrices instead of (unitary) operators. The former can be considered independent
entities and we are free to do similarity transformations. The latter operates on vectors, which usually have pre-defined concepts such as inner
products (and therefore orthogonality). In this sense, we cannot make a non-unitary operator unitary through a similarity transformation.
We can only make the matrix form of a non-unitary operator unitary by choosing a non-orthonormal basis. But this does not make the operator
itself unitary.

Since $$ S $$ in Eq. (1.10) is in general not unitary, two equivalent representations do not just differ by *a trivial choice of basis*
physically. 

#### Definition of reducibility

The defintion of reducible representations should be those that have a *non-trivial* invariant subspace. Obviously $$ \\{ 0 \\} $$ and the
original vector space are always invariant subspaces for any representations.

#### Eq. (1.13)

The statement that *$$ D_j(g) $$ is irreducible* implies that $$ D_j(g) $$ should be understood as an operator defined on a non-trivial
subspace $$ X_j $$ of the original space $$ X $$, and $$ X_i $$ and $$ X_j $$ are orthogonal to each other for $$ i \neq j $$
and $$ \cup_j X_j = X $$.

### 1.9 Useful theorems

#### Theorem 1.1

$$ U $$ is implicitly assumed to be unitary in Eq. (1.28) and this guarantees that $$ X $$ is hermitian in Eq. (1.31). The proof that a unitary
$$ U $$ always exists to diagonalize a hermitian matrix can be found in most QM books. In fact $$ U $$ can be simply made of orthonormal
eigenvectors of $$ S $$ juxtaposed next to each other.

#### Theorem 1.2

The proof implicitly uses the following fact.

*If two representations $$ D_1 $$ and $$ D_2 $$ are equivalent, $$ D_1 $$ is reducible if and only if $$ D_2 $$ is reducible.* The proof is
as follows.

If $$ D_1 $$ is reducible, there exists a non-trivial subspace $$ X $$ whose projector $$ P $$ satisfies Eq. (1.11)

$$ P D_1(g) P = D_1(g) P, \forall g \in G. $$

Since $$ D_1 $$ and $$ D_2 $$ are equivalent, there exists an invertible transformation $$ S $$ such that
$$ D_1(g) = S^{-1} D_2(g) S, \forall g \in G $$, therefore

$$ P S^{-1} D_2(g) S P = S^{-1} D_2(g) S P, $$

or equivalently

$$ S P S^{-1} D_2(g) S P S^{-1} = D_2(g) S P S^{-1}, $$

where $$ S P S^{-1} $$ is the projector of subspace $$ S X $$, which has the same dimensionality of the original invariant subspace $$ X $$.
Obviously $$ S X $$ is non-trivial since there exists $$ v \not \in X $$ and it is easy to show that $$ S v \not \in S X $$. This proves that
$$ D_2 $$ is also reducible.

Similarly, one can easily prove that if two representations $$ D_1 $$ and $$ D_2 $$ are equivalent, $$ D_1 $$ is completely reducible
if and only if $$ D_2 $$ is completely reducible.

### 1.10 Subgroups

The statement *Every element of $$ G $$ must belong to one and only one coset* is made without proof. The *one* part is rather obvious since
$$ e $$ must be in the subgroup. Below is the proof of the *only one* part.

Assuming that $$ x \in H g_1 $$ and $$ x \in H g_2 $$, we want to prove that $$ H g_1 = H g_2 $$ by showing that $$ H g_1 \subset H g_2 $$ and
$$ H g_2 \subset H g_1 $$.

There must exist $$ x_1 \in H $$ and $$ x_2 \in H $$ such that $$ x = x_1 g_1 = x_2 g_2 $$. This implies that $$ g_1 = {x_1}^{-1} x_2 g_2 $$.

For any $$ y \in H g_1 $$, there exists $$ y_1 \in H $$ such that $$ y = y_1 g_1 = y_1 {x_1}^{-1} x_2 g_2 $$. Note that $$ y_1 $$,
$$ {x_1}^{-1} $$ and $$ x_2 $$ are all elements of $$ H $$ and therefore $$ y_1 {x_1}^{-1} x_2 \in H $$ and $$ y \in H g_2 $$. This proves
$$ H g_1 \subset H g_2 $$.

$$ H g_2 \subset H g_1 $$ can be proved similarly.

### 1.11 Schur's lemma

#### Theorem 1.3

In the proof it is rather hand-waving to simply state *A similar argument shows that $$ A $$ vanishes if there is a $$ \langle v| $$ which
annihilates $$ A $$*, because it actually requires the irreducibility of $$ D_1^\dagger $$ rather than that of $$ D_1 $$ itself and thus
the following fact.

*A representation $$ D $$ is reducible if and only if its Hermitian conjugate $$ D^\dagger $$ is reducible.*

The proof is as follows. Note that this proof does not require $$ D $$ to be a representation of a finite $$ G $$. It is equally valid
even if $$ G $$ is infinite.

If $$ D $$ is reducible, there is a non-trivial subspace $$ S $$ such that its projector $$ P $$ satisfies $$ P D(g) P = D(g) P, \forall g \in G $$.

Notice that the complementary subspace of $$ S $$ is also non-trivial, and $$ I - P $$ is the projector onto it. We have

$$
\begin{eqnarray}
&& (I - P) D^\dagger (g) (I - P)  \\
&=& [(I - P) D(g) (I - P)]^\dagger  \\
&=& [(I - P) D(g) - I D(g) P + P D(g) P]^\dagger  \\
&=& [(I - P) D(g)]^\dagger  \\
&=& D^\dagger (g) (I - P).
\end{eqnarray}
$$

This proves that $$ D^\dagger $$ is also reducible. The *if* part can be proved similarly. 

#### Clarification on Eq. (1.46)

There can be multiple appearances of the same irreducible representation in the block diagnoal form of $$ D $$. They share the same lable $$ a $$
and is distinquished by different values of $$ x $$.

### 1.14 Eigenstates

As for why (1.104) is valid, refer to the derivation up to Eq. (1.57).

### 1.16 Example of tensor products

It is worth noting that $$ D_2 $$ as in (1.109) gives us reflections and rotations.

### 1.17 *Finding the normal modes

Eq. (1.115): The two $$ -\frac{\sqrt{3}}{6} $$ in the last column should be both $$ -\frac{1}{6} $$.

## Chapter 2

Note that when the author talks about a *unitary representation of the algebra*, he really means that the representation of the
group is unitary (and therefore the representation of the the algebra is hermitian).

### Eq. (2.36)

Note that when the group representation is unitary (or equivalent to a unitary representation), $$ T_a $$ is hermitian (or equivalent
to a hermitian matrix). So we have $$ \mathrm{Tr} (T_a T_a) \ge 0 $$.

### Eq. (2.39)

Typo. $$ \mathrm{Tr} $$ is missing before two pairs of parentheses.

# Solutions

## Chapter 1

### 1.A.

Trivial. Simply use the fact that every element appears once and only once in every row and column of the multiplication table.

### 1.B.

The only two possibilities are given below, corresponding to $$ Z_4 $$ and $$ Z_2 \times Z_2 $$ respectively.

$$
\begin{array}
\hline
 e & a & b & c  \\ \hline
 a & b & c & e  \\ \hline
 b & c & e & a  \\ \hline
 c & e & a & b  \\ \hline
\end{array}
$$

$$
\begin{array}
\hline
 e & a & b & c  \\ \hline
 a & e & c & b  \\ \hline
 b & c & e & a  \\ \hline
 c & b & a & e  \\ \hline
\end{array}
$$

### 1.C.

If the representation (1.135) of the permutation group is irreducible, (1.79) would apply.

### 1.D.

Applying Schur's lemma, we have $$ S^{-1} A = \lambda I $$, or $$ A = \lambda S $$.

### 1.E.

It is essentially the group $$ A_4 $$, made of permutations with an even number of 2-cycles. There are in total $$ 4! / 2 = 12 $$ elements.

There are four conjugacy classes.
- {e},
- {(12)(34), (13)(24), (14)(23)},
- {(123), (243), (134), (142)},
- {(132), (234), (143), (124).

From (1.74), we know the four irreducible representations must be of 1-, 1-, 1-. and 3-dimensional. By applying (1.79) and (1.87), we get the
following character table.

$$
\begin{array}
\hline
 \text{} & e & (12)(34) & (123)    & (321)     \\ \hline
 D_1     & 1 & 1        & 1        & 1         \\ \hline
 D_{1'}  & 1 & 1        & \omega   & \omega^2  \\ \hline
 D_{1''} & 1 & 1        & \omega^2 & \omega    \\ \hline
 D_3     & 3 & -1       & 0        & 0         \\ \hline
\end{array}
$$

### 1.F.

The group is a subset of $$ S_4 $$, with the following conjugacy classes.
- {e},
- {(13)(24)},
- {(13), (24)},
- {(12)(34), (14)(23)},
- {(1234), (4321)}.

Below is the character table.

$$
\begin{array}
\hline
 \text{}    & e & (13)(24) & (13)     & (12)(34) & (1234)    \\ \hline
 D_1        & 1 & 1        & 1        & 1        & 1         \\ \hline
 D_{1'}     & 1 & 1        & 1        & -1       & -1        \\ \hline
 D_{1''}    & 1 & 1        & -1       & 1        & -1        \\ \hline
 D_{1'''}   & 1 & 1        & -1       & -1       & 1         \\ \hline
 D_2        & 2 & -2       & 0        & 0        & 0         \\ \hline
\end{array}
$$

The $$ D_2 $$ representation is as follows.
- $$ \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} $$,
- $$ \begin{bmatrix} -1 & 0 \\ 0 & -1 \end{bmatrix} $$,
- $$ \begin{bmatrix} 0 & -1 \\ -1 & 0 \end{bmatrix} $$, $$ \begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix} $$
- $$ \begin{bmatrix} -1 & 0 \\ 0 & 1 \end{bmatrix} $$, $$ \begin{bmatrix} 1 & 0 \\ 0 & -1 \end{bmatrix} $$,
- $$ \begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix} $$, $$ \begin{bmatrix} 0 & 1 \\ -1 & 0 \end{bmatrix} $$.

The representation that describes the system is $$ D_4 \otimes D_2 $$, where $$ D_4 $$ is the defining representation. Its character table
is as follows.

$$
\begin{array}
\hline
 \text{} & e & (13)(24) & (13)     & (12)(34) & (1234)    \\ \hline
 D_8     & 8 & 0        & 0        & 0        & 0         \\ \hline
\end{array}
$$

Applying (1.88), we see that in $$ D_8 $$, $$ D_1 $$, $$ D_{1'} $$, $$ D_{1''} $$, and $$ D_{1'''} $$ each appears once, while $$ D_2 $$ appears twice.

The projection onto $$ D_1 $$ is

$$
\begin{eqnarray}
  P_1 &=& \frac{1}{8} \sum_{g \in G} \chi_1 (g)^* D_8(g)    \\
  &=& \frac{1}{8} \begin{bmatrix} 1 & 1 & -1 & 1 & -1 & -1 & 1 & -1 \end{bmatrix} ^T \begin{bmatrix} 1 & 1 & -1 & 1 & -1 & -1 & 1 & -1 \end{bmatrix},
\end{eqnarray}
$$

the so-called "breathing mode".

The projection onto $$ D_{1'} $$ is

$$
\begin{eqnarray}
  P_{1'} &=& \frac{1}{8} \sum_{g \in G} \chi_{1'} (g)^* D_8(g)    \\
  &=& \frac{1}{8} \begin{bmatrix} 1 & 1 & 1 & -1 & -1 & -1 & -1 & 1 \end{bmatrix} ^T \begin{bmatrix} 1 & 1 & 1 & -1 & -1 & -1 & -1 & 1 \end{bmatrix},
\end{eqnarray}
$$

where 1 and 3 are pulled away from each other while 2 and 4 are pushed towards each other (or the opposite).

The projection onto $$ D_{1''} $$ is

$$
\begin{eqnarray}
  P_{1''} &=& \frac{1}{8} \sum_{g \in G} \chi_{1''} (g)^* D_8(g)    \\
  &=& \frac{1}{8} \begin{bmatrix} 1 & -1 & -1 & -1 & -1 & 1 & 1 & 1 \end{bmatrix} ^T \begin{bmatrix} 1 & -1 & -1 & -1 & -1 & 1 & 1 & 1 \end{bmatrix},
\end{eqnarray}
$$

where the four points are pulled away from each other horizontally but pushed towards each other vertically (or the opposite).

The projection onto $$ D_{1'''} $$ is

$$
\begin{eqnarray}
  P_{1'''} &=& \frac{1}{8} \sum_{g \in G} \chi_{1'''} (g)^* D_8(g)    \\
  &=& \frac{1}{8} \begin{bmatrix} 1 & -1 & 1 & 1 & -1 & 1 & -1 & -1 \end{bmatrix} ^T \begin{bmatrix} 1 & -1 & 1 & 1 & -1 & 1 & -1 & -1 \end{bmatrix},
\end{eqnarray}
$$

which describes a rotation around the center.

The projection onto $$ D_2 $$ is

$$
\begin{eqnarray}
  P_2 &=& \frac{1}{8} \sum_{g \in G} \chi_2
   (g)^* D_8(g)    \\
  &=& \frac{1}{4} \begin{bmatrix} 1 & 1 \\ 1 & 1 \end{bmatrix} \otimes I_4,
\end{eqnarray}
$$

where $$ I_4 $$ is the four-dimensional identity matrix. It is equal to the sum of two matrices similar to (1.122) and (1.123), which describe
translations in the $$ x $$ direction and translations in the $$ y $$ direction.

## Chapter 2

### 2.A.

Taylor expanding $$ e^{i \alpha A} $$ and noticing that $$ A^n = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 0 & 0 \\ 0 & 0 & 1 \end{bmatrix} $$
with $$ n = 2, 4, \dots $$ while $$ A^n = A $$ with $$ n = 1, 3, \dots $$, we have

$$
\begin{eqnarray}
  e^{i \alpha A} &=& I - A^2 + A^2 + i \alpha A - \frac{\alpha^2 A^2}{2!} - \frac{i \alpha^2 A^3}{3!} + \cdots    \\
  &=& I - A^2 + A^2 \cos \alpha + i A \sin \alpha    \\
  &=& \begin{bmatrix} \cos \alpha & 0 & i \sin \alpha \\ 0 & 1 & 0 \\ i \sin \alpha & 0 & \cos \alpha \end{bmatrix}.
\end{eqnarray}
$$

### 2.B.

Applying (2.44), we have

$$
\begin{eqnarray}
  &&  e^{i \alpha A} B e^{-i \alpha A}    \\
  &=& B + i \alpha [A, B] - \frac{\alpha^2}{2} [A, [A, B]] - \frac{i \alpha^3}{3!} [A, [A, [A, B]]] + \cdots    \\
  &=& B + i \alpha B - \frac{\alpha^2}{2} B - \frac{i \alpha^3}{3!} B + \cdots    \\
  &=& B e^{i \alpha}.
\end{eqnarray}
$$

### 2.C.

To save some space, let's notate $$ \alpha_a X_a $$ as $$ A $$ and $$ \beta_a X_a $$ as $$ B $$.

We have

$$
\begin{eqnarray}
  K &=& e^{i A} e^{i B} - 1    \\
  &=& (1 + i A - \frac{1}{2} A^2 - \frac{i}{3!} A^3 + \cdots)    \\
  &&  (1 + i B - \frac{1}{2} B^2 - \frac{i}{3!} B^3 + \cdots) -1    \\
  &=& i A + i B - A B - \frac{1}{2} A^2 - \frac{1}{2} B^2    \\
  &&  - \frac{i}{3!} A^3 - \frac{i}{3!} B^3 - \frac{i}{2} A A B - \frac{i}{2} A B B + \cdots.    \\
\end{eqnarray}
$$

This gives

$$
\begin{eqnarray}
  i \delta_a X_a &=& K - \frac{1}{2} K^2 + \frac{1}{3} K^3    \\
  &=& i A + i B - A B - \frac{1}{2} A^2 - \frac{1}{2} B^2    \\
  &&  - \frac{i}{3!} A^3 - \frac{i}{3!} B^3 - \frac{i}{2} A A B - \frac{i}{2} A B B    \\
  &&  + \frac{1}{2}(A + B)^2 + \frac{i}{2} A A B + \frac{i}{2} A B A + \frac{i}{2} B A B + \frac{i}{2} A B B    \\
  &&  + \frac{i}{2} A^3 + \frac{i}{2} B^3 + \frac{i}{4} A B B + \frac{i}{4} B A A + \frac{i}{4} A A B + \frac{i}{4} B B A     \\
  &&  - \frac{i}{3} (A + B)^3 + \cdots     \\
  &=& i A + i B - \frac{1}{2} [A, B] + \frac{i}{12} [[A, B], A - B] + \cdots     \\
  &=& i X_a \left( \alpha_a + \beta_a - \frac{1}{2} \alpha_r \beta_s f_{rsa} - \frac{1}{12} \alpha_r \beta_s (\alpha_m - \beta_m) f_{rst} f_{tma} \right) + \cdots.     \\
\end{eqnarray}
$$

We can now write

$$ \delta_a = \alpha_a + \beta_a - \frac{1}{2} \alpha_r \beta_s f_{rsa} - \frac{1}{12} \alpha_r \beta_s (\alpha_m - \beta_m) f_{rst} f_{tma} + \cdots. $$

## Chapter 3

### 3.A.

We start with the highest weight $$ | j + s | j + s \rangle = |j, j \rangle |s, s \rangle $$ and repeatedly apply $$ J^- $$ on both
sides until they vanish. We get the spin $$ j + s $$ representation, with in total $$ 2(j + s) + 1 $$ states.

We will notice that $$ | j + s | j + s - 1 \rangle $$ is a linear combination of
$$ |j, j - 1 \rangle |s, s \rangle $$ and $$ |j, j \rangle |s, s - 1 \rangle $$, which spans a 2-dimensional space. These two vectors
can form another linear combination that is orthogonal to $$ | j + s | j + s - 1 \rangle $$, and this second linear combination can
only be $$ | j + s - 1| j + s - 1 \rangle $$. (To verify this claim, apply $$ J^+ $$ and see it vanish.) Now we can again repeatedly
apply $$ J^- $$ on both sides until they vanish. In doing so, we get the spin $$ j + s - 1 $$ representation, with in total
$$ 2(j + s) - 1 $$ states.

Now we do exactly the same with $$ | j + s - 1 | j + s - 2 \rangle $$, which is a linear combination of three vectors, which spans a
3-dimensional space. These three vectors can form a third linear combination that is orthogonal to both $$ | j + s | j + s - 2 \rangle $$
and $$ | j + s - 1 | j + s - 2 \rangle $$. This third linear combination can only be $$ | j + s - 2 | j + s - 2 \rangle $$...

The whole process stops at the spin $$ | j - s | $$ representation.

As a sanity check, both sides of the equation have $$ (2 j + 1) (2 s + 1) $$ linearly independent states. So everything works out.

### 3.B.

It is easy to show that there is a similarity transformation $$ S $$ such that

$$ S^{-1} (\hat{r} \cdot \sigma) S = \sigma_3. $$

We have

$$
\begin{eqnarray}
  && e^{i \vec{r} \cdot \vec{\sigma}}    \\
  &=& S e^{i |\vec{r}| \sigma_3} S^{-1}    \\
  &=& S \begin{bmatrix} e^{i |\vec{r}|} & 0 \\ 0 & e^{-i |\vec{r}|} \end{bmatrix} S^{-1}    \\
  &=& \cos |\vec{r}| \cdot I + i \sin |\vec{r}| \cdot (\hat{r} \cdot \sigma)
\end{eqnarray}
$$

### 3.C.

Let $$ S = \begin{bmatrix} \frac{1}{\sqrt{2}} & 0 & -\frac{1}{\sqrt{2}} \\ \frac{i}{\sqrt{2}} & 0 & \frac{i}{\sqrt{2}} \\ 0 & -1 & 0 \end{bmatrix} $$.

It is easy to verify that $$ J_i^1 = S^{-1} T_i S $$, where $$ J_i^1 $$ and $$ T_i $$ are the spin-1 representation and the adjoint
representation of the $$ i $$-th generator.

### 3.D.

$$
\begin{bmatrix}
  0 & 0 & 0 & -i \\
  0 & 0 & -i & 0 \\
  0 & i & 0 & 0 \\
  i & 0 & 0 & 0
\end{bmatrix}
$$.

### 3.E.

(a) $$ [\sigma_a, \sigma_b] \eta_c $$

(b) $$ \mathrm{Tr}(\sigma_a \cdot 2 \delta_{bd} I \sigma_c) = 4 \delta_{bd} \mathrm{Tr}(\delta_{ac} I) = 8 \delta_{ac} \delta_{bd} $$

(c) $$ \sigma_1 \sigma_2 \eta_1 \eta_2 - \sigma_2 \sigma_1 \eta_2 \eta_1 = 0 $$

## Chapter 4

### 4.A.

Simply check the [Table of Clebsch-Gordan coefficients](https://en.wikipedia.org/wiki/Table_of_Clebsch%E2%80%93Gordan_coefficients).
We see

$$ A = \langle 3/2, -1/2 | 1/2, 1, 1/2, -1 \rangle k_{\alpha \beta} = \sqrt{\frac{1}{3}} k_{\alpha \beta}. $$

What we want is

$$ \langle 3/2, -3/2 | 1/2, 1, -1/2, -1 \rangle k_{\alpha \beta} = k_{\alpha \beta} = \sqrt{3} A. $$

### 4.B.

We could repeatedly apply $$ L- $$ to both sides of $$ O_{+2} = r_{+1} r_{+1} $$. Or we could again check the
[Table of Clebsch-Gordan coefficients](https://en.wikipedia.org/wiki/Table_of_Clebsch%E2%80%93Gordan_coefficients).

Either way, we get the following.

$$
\begin{eqnarray}
  O_{+2} &=& r_{+1} r_{+1}  \\
  O_{+1} &=& \sqrt{\frac{1}{2}} r_{+1} r_{0} + \sqrt{\frac{1}{2}} r_{0} r_{+1}  \\
  O_{0}  &=& \sqrt{\frac{1}{6}} r_{+1} r_{-1} + \sqrt{\frac{2}{3}} r_{0} r_{0} + \sqrt{\frac{1}{6}} r_{-1} r_{+1}  \\
  O_{-1} &=& \sqrt{\frac{1}{2}} r_{-1} r_{0} + \sqrt{\frac{1}{2}} r_{0} r_{-1}  \\
  O_{-2} &=& r_{-1} r_{-1}
\end{eqnarray}
$$

Note the following relationships.

$$
\begin{eqnarray}
  r_{+1} &=& -(r_1 + i r_2) = -\sqrt{\frac{1}{2}} \sin \theta e^{i \phi}  \\
  r_{0}  &=& r_3 = \cos \theta  \\
  r_{-1} &=& (r_1 - i r_2) = \sqrt{\frac{1}{2}} \sin \theta e^{-i \phi}
\end{eqnarray}
$$

Compare these to the [Table of spherical harmonics](https://en.wikipedia.org/wiki/Table_of_spherical_harmonics)
with $$ l = 2 $$, we see they are identical up to a constant factor.

To generalize this construction to arbitrary $$ l $$, simply notice that

$$ [L^+, (r_{+1})^l] = 0. $$

The operator $$ (r_{+1})^l $$ is therefore the $$ O_{+l} $$ component of a spin $$ l $$ tensor operator. The rest is to
repeat the previous procedure.

### 4.C.

Note the similarity between this problem and 2.A. and 3.B.

We have

$$
\begin{eqnarray}
  &&  e^{i \alpha \hat{\alpha}_a X_a^1}    \\
  &=& I + i \alpha \hat{\alpha}_a X_a^1 - \frac{\alpha^2}{2!} (\hat{\alpha}_a X_a^1)^2 - \frac{i \alpha^3}{3!} \hat{\alpha}_a X_a^1 + \cdots   \\
  &=& I - (\hat{\alpha}_a X_a^1)^2 + i (\alpha - \frac{\alpha^3}{3!} + \cdots) \hat{\alpha}_a X_a^1 + (1 - \frac{\alpha^2}{2!} + \cdots) (\hat{\alpha}_a X_a^1)^2   \\
  &=& I - (\hat{\alpha}_a X_a^1)^2 + i \sin \alpha \hat{\alpha}_a X_a^1 + \cos \alpha (\hat{\alpha}_a X_a^1)^2.
\end{eqnarray}
$$

## Chapter 5

### 5.A.

Since pions are bosons, the full wave function (i.e. a wave function including position, spin, isospin, and whatever else) must be
symmetric. Since they are in an s-wave state, the wave function is symmetrical in the exchange of the position variables. Since pions
have spin 0, the wave function must also be symmetrical  in the exchange of isospin.

This means the total isospin has to be 2 (with 5 eigenstates) or 0 (with 1 eigenstate).

### 5.B.

It is not very clear whether it is asked to prove something similar to (5.17), or $$ [T_a, T_b] = \epsilon_{abc} T_c $$.

I assume it is the latter. (The former case is actual easier any way.)

We start with

$$
\begin{eqnarray}
  &&  [T_a, T_b]  \\
  &=& \sum_{x, m, m', \alpha} \sum_{y, n, n', \beta} [J^{j_x}_a]_{m m'} [J^{j_y}_b]_{n n'} [a^\dagger_{x, m, \alpha} a_{x, m', \alpha}, a^\dagger_{y, n, \beta} a_{y, n', \beta}].
\end{eqnarray}
$$

The commutator can be calculated as follows. (We take the commutator and plus sign if at least one of the two parts are bosons,
the anti-commutator and the plus sign if both are fermions.)

$$
\begin{eqnarray}
  &&  [a^\dagger_{x, m, \alpha} a_{x, m', \alpha}, a^\dagger_{y, n, \beta} a_{y, n', \beta}]  \\
  &=& a^\dagger_{x, m, \alpha} [a_{x, m', \alpha}, a^\dagger_{y, n, \beta} a_{y, n', \beta}] + [a^\dagger_{x, m, \alpha}, a^\dagger_{y, n, \beta} a_{y, n', \beta}] a_{x, m', \alpha}  \\
  &=& a^\dagger_{x, m, \alpha} [a_{x, m', \alpha}, a^\dagger_{y, n, \beta}]_\mp a_{y, n', \beta} \pm a^\dagger_{x, m, \alpha} a^\dagger_{y, n, \beta} [a_{x, m', \alpha}, a_{y, n', \beta}]_\mp  \\
  &&  \pm a^\dagger_{y, n, \beta} [a^\dagger_{x, m, \alpha}, a_{y, n', \beta}]_\mp a_{x, m', \alpha} + [a^\dagger_{x, m, \alpha}, a^\dagger_{y, n, \beta}]_\mp a_{y, n', \beta} a_{x, m', \alpha}  \\
  &=& \delta_{x y} \delta_{\alpha \beta} (\delta_{m' n} a^\dagger_{x, m, \alpha} a_{y, n', \beta} - \delta_{m n'} a^\dagger_{y, n, \beta} a_{x, m', \alpha}).
\end{eqnarray}
$$

We have therefore

$$
\begin{eqnarray}
  &&  [T_a, T_b]  \\
  &=& \sum_{x, m, n', \alpha} [J^{j_x}_a]_{m n} [J^{j_x}_b]_{n n'} a^\dagger_{x, m, \alpha} a_{x, n', \alpha} - [J^{j_x}_b]_{n m} [J^{j_x}_a]_{m m'} a^\dagger_{x, n, \alpha} a_{x, m', \alpha}  \\
  &=& \sum_{x, m, n', \alpha} [J^{j_x}_a, J^{j_x}_b]_{m n'} a^\dagger_{x, m, \alpha} a_{x, n', \alpha}  \\
  &=& \sum_{x, m, n', \alpha} \epsilon_{abc} [J^{j_x}_c]_{m n'} a^\dagger_{x, m, \alpha} a_{x, n', \alpha}  \\
  &=& \epsilon_{abc} T_c.
\end{eqnarray}
$$

5.C.

According to standard perturbation theory, which can be found in any QM book, the probability for a transition from $$ | i \rangle $$
to  $$ | j \rangle $$ to happen is proportional to $$ |\langle j | H | i \rangle|^2 $$.

We can assume $$ H $$ to be isospin invariant. Therefore the probability of $$ \pi^+ P \rightarrow \Delta^{++} $$ is proportional to
$$ |\langle 3/2, 3/2 | 1, 1/2, 1, 1/2 \rangle|^2 = 1 $$, while the probability of $$ \pi^- P \rightarrow \Delta^0 $$ is proportional to
$$ |\langle 3/2, -1/2 | 1, 1/2, -1, 1/2 \rangle|^2 = \frac{1}{3} $$. The ratio of the two is thus 3 to 1.

## Chapter 6

### 6.A.

In the adjoint representation,

$$
\begin{eqnarray}
  &&  H_i | [E_\alpha, E_\beta] \rangle  \\
  &=& [H_i, [E_\alpha, E_\beta]]  \\
  &=& [E_\alpha, [H_i, E_\beta]] + [[H_i, E_\alpha], E_\beta]  \\
  &=& \beta_i [E_\alpha, E_\beta] + \alpha_i [E_\alpha, E_\beta]  \\
  &=& (\alpha + \beta)_i [E_\alpha, E_\beta].
\end{eqnarray}
$$

If $$ \alpha + \beta $$ is a root, $$ [E_\alpha, E_\beta] $$ must be proportional to $$ E_{\alpha + \beta} $$
due to the uniqueness of non-zero roots. If $$ \alpha + \beta $$ is not a root, $$ [E_\alpha, E_\beta] $$ must be 0.

### 6.B.

From 6.A., $$ [E_\alpha, E_{-\alpha - \beta}] $$ is proportional to $$ E_{-\beta} $$. We can calculate the coefficient
as follows.

$$
\begin{eqnarray}
  &&  \langle E_{-\beta} | [E_\alpha, E_{-\alpha - \beta}] \rangle  \\
  &=& \lambda^{-1} \mathrm{Tr} (E^\dagger_{-\beta} [E_\alpha, E_{-\alpha - \beta}])  \\
  &=& \lambda^{-1} \mathrm{Tr} (E_{\beta} [E_\alpha, E_{-\alpha - \beta}])  \\
  &=& \lambda^{-1} \mathrm{Tr} ([E_{\beta}, E_\alpha], E_{-\alpha - \beta})  \\
  &=& -N \lambda^{-1} \mathrm{Tr} (E_{\alpha + \beta}, E^\dagger_{\alpha + \beta})  \\
  &=& -N  \\
\end{eqnarray}
$$

### 6.C.

Since $$ \sigma_3 $$ and $$ \sigma_3 \tau_3 $$ are already diagnoal, we can simply read off the four weights from these two
matrices. They are $$ (\pm 1, \pm 1) $$.

To get the roots, the foolproof method is to write down the two 10-dimensional matrices of $$ H_1 $$ and $$ H_2 $$ in the
adjoint representation and diagonalize them. In practice, the weights of the four-dimensional representation give us an
important hint. The difference between two *adjacient* weights is *often* a root. We know there are two zero roots
corresponding to $$ H_1 $$ and $$ H_2 $$ themselves. The eight remaining are thus $$ (\pm 2, 0), (0, \pm 2), (\pm 2, \pm 2) $$.

## Chapter 7

### 7.A.

We have $$ [T_1, T_4] = i \frac{1}{2} T_7 $$ and $$ [T_4, T_5] = i \frac{1}{2} T_3 + i \frac{\sqrt{3}}{2} T_8 $$, and
therefore $$ f_{147} = \frac{1}{2} $$ and $$ f_{456} = 0 $$.

### 7.B.

$$ T_1 $$, $$ T_2 $$ and $$ T_3 $$ generate an SU(2) subalgebra because they have the right commutation relationship.

Under this subalgebra, the first two eigenvectors (corresponding to the top weights in (7.11)) of the defining representation
form a doublet (spin 1) while the third eigenvectos (corresponding to the bottom weight in (7.11)) form a singlet (spin 0).

The adjoint representation breaks down into the following SU(2) representations.
- Spin 1/2: $$ E_{1/2, \sqrt{3}/2} $$ and $$ E_{-1/2, \sqrt{3}/2} $$ form a doublet.
- Spin 1/2: $$ E_{1/2, -\sqrt{3}/2} $$ and $$ E_{-1/2, -\sqrt{3}/2} $$ form a doublet.
- Spin 1: $$ E_{1, 0} $$, $$ T_{3} $$, and $$ E_{-1, 0} $$ form a triplet.
- Spin 0: $$ T_{0} $$ forms a singlet.

Remember that the generators are the states in the adjoint representation.

### 7.C.

Defining $$ J_1 $$, $$ J_2 $$, $$ J_3 $$ as $$ \lambda_2 $$, $$ \lambda_5 $$, and $$ \lambda_7 $$, we get
the commutation relationship of SU(2).

The eigenvectors of $$ J_3 $$ are $$ (0, 1, i)^T $$, $$ (- \sqrt{2}i, 0, 0)^T $$, and $$ (0, 1, -i)^T $$ with eigenvalues
1, 0, and -1. Applying $$ J^\pm = \frac{1}{\sqrt{2}} (J_1 \pm J_2) $$, we see that they form a triplet of the spin 1
representation of SU(2)

The adjoint representation breaks down into the following SU(2) representations.
- Spin 1: $$ J^+ $$, $$ -J_3 $$, and $$ -J^- $$ form a triplet.
- Spin 2: $$ \lambda_3 - 2i \lambda_6 - \sqrt{3} \lambda_8 $$, $$ 2i \lambda_1 - 2 \lambda_4 $$,
  $$ \sqrt{6} \lambda_3 + \sqrt{2} \lambda_8 $$, $$ 2i \lambda_1 + 2 \lambda_4 $$, and
  $$ \lambda_3 + 2i \lambda_6 - \sqrt{3} \lambda_8 $$ form a quintuplet, with their $$ J_3 $$ value equal to
  2, 1, 0, -1, and 2 respectively.

Remember that the generators are the states in the adjoint representation.

The foolproof way to get the correct result is to write down the matrix of $$ \lambda_7 $$ in the adjoint representation
and diagonalize it.
