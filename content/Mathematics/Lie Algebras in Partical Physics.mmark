---
title: "Lie Algebras in Particle Physics"
Description: "Errata, Notes, and Problem Solutions to Lie Algebras in Particle Physics"
date: 2017-01-02T14:59:31-08:00
---

This is a great book. However, a better book for beginners may be Zee's _Group Theory in a Nutshell for Physicists_. Zee's book is mathematically
less rigorous and sometimes made me cringe a bit. But it has a much better overview of finite groups, which are quite important for a beginner to
build intuition. It also covers Lorentz's group, which is obviously important for special relativity and quantum field theory.

# Errata and Notes

## Chapter 1

### 1.4 Irreducible representations

#### Eq. (1.10)

It would be much easier if we were talking about (unitary) matrices instead of (unitary) operators. The former can be considered independent
entities and we are free to do similarity transformations. The latter operates on vectors, which usually have pre-defined concepts such as inner
products (and therefore orthogonality). In this sense, we cannot make a non-unitary operator unitary through a similarity transformation.
We can only make the matrix form of a non-unitary operator unitary by choosing a non-orthonormal basis. But this does not make the operator
itself unitary.

Since $$ S $$ in Eq. (1.10) is in general not unitary, two equivalent representations do not just differ by *a trivial choice of basis*
physically. 

#### Definition of reducibility

The defintion of reducible representations should be those that have a *non-trivial* invariant subspace. Obviously $$ \\{ 0 \\} $$ and the
original vector space are always invariant subspaces for any representations.

#### Eq. (1.13)

The statement that *$$ D_j(g) $$ is irreducible* implies that $$ D_j(g) $$ should be understood as an operator defined on a non-trivial
subspace $$ X_j $$ of the original space $$ X $$, and $$ X_i $$ and $$ X_j $$ are orthogonal to each other for $$ i \neq j $$
and $$ \cup_j X_j = X $$.

### 1.9 Useful theorems

#### Theorem 1.1

$$ U $$ is implicitly assumed to be unitary in Eq. (1.28) and this guarantees that $$ X $$ is hermitian in Eq. (1.31). The proof that a unitary
$$ U $$ always exists to diagonalize a hermitian matrix can be found in most QM books. In fact $$ U $$ can be simply made of orthonormal
eigenvectors of $$ S $$ juxtaposed next to each other.

#### Theorem 1.2

The proof implicitly uses the following fact.

*If two representations $$ D_1 $$ and $$ D_2 $$ are equivalent, $$ D_1 $$ is reducible if and only if $$ D_2 $$ is reducible.* The proof is
as follows.

If $$ D_1 $$ is reducible, there exists a non-trivial subspace $$ X $$ whose projector $$ P $$ satisfies Eq. (1.11)

$$ P D_1(g) P = D_1(g) P, \forall g \in G. $$

Since $$ D_1 $$ and $$ D_2 $$ are equivalent, there exists an invertible transformation $$ S $$ such that
$$ D_1(g) = S^{-1} D_2(g) S, \forall g \in G $$, therefore

$$ P S^{-1} D_2(g) S P = S^{-1} D_2(g) S P, $$

or equivalently

$$ S P S^{-1} D_2(g) S P S^{-1} = D_2(g) S P S^{-1}, $$

where $$ S P S^{-1} $$ is the projector of subspace $$ S X $$, which has the same dimensionality of the original invariant subspace $$ X $$.
Obviously $$ S X $$ is non-trivial since there exists $$ v \not \in X $$ and it is easy to show that $$ S v \not \in S X $$. This proves that
$$ D_2 $$ is also reducible.

Similarly, one can easily prove that if two representations $$ D_1 $$ and $$ D_2 $$ are equivalent, $$ D_1 $$ is completely reducible
if and only if $$ D_2 $$ is completely reducible.

### 1.10 Subgroups

The statement *Every element of $$ G $$ must belong to one and only one coset* is made without proof. The *one* part is rather obvious since
$$ e $$ must be in the subgroup. Below is the proof of the *only one* part.

Assuming that $$ x \in H g_1 $$ and $$ x \in H g_2 $$, we want to prove that $$ H g_1 = H g_2 $$ by showing that $$ H g_1 \subset H g_2 $$ and
$$ H g_2 \subset H g_1 $$.

There must exist $$ x_1 \in H $$ and $$ x_2 \in H $$ such that $$ x = x_1 g_1 = x_2 g_2 $$. This implies that $$ g_1 = {x_1}^{-1} x_2 g_2 $$.

For any $$ y \in H g_1 $$, there exists $$ y_1 \in H $$ such that $$ y = y_1 g_1 = y_1 {x_1}^{-1} x_2 g_2 $$. Note that $$ y_1 $$,
$$ {x_1}^{-1} $$ and $$ x_2 $$ are all elements of $$ H $$ and therefore $$ y_1 {x_1}^{-1} x_2 \in H $$ and $$ y \in H g_2 $$. This proves
$$ H g_1 \subset H g_2 $$.

$$ H g_2 \subset H g_1 $$ can be proved similarly.

### 1.11 Schur's lemma

#### Theorem 1.3

In the proof it is rather hand-waving to simply state *A similar argument shows that $$ A $$ vanishes if there is a $$ \langle v| $$ which
annihilates $$ A $$*, because it actually requires the irreducibility of $$ D_1^\dagger $$ rather than that of $$ D_1 $$ itself and thus
the following fact.

*A representation $$ D $$ is reducible if and only if its Hermitian conjugate $$ D^\dagger $$ is reducible.*

The proof is as follows. Note that this proof does not require $$ D $$ to be a representation of a finite $$ G $$. It is equally valid
even if $$ G $$ is infinite.

If $$ D $$ is reducible, there is a non-trivial subspace $$ S $$ such that its projector $$ P $$ satisfies $$ P D(g) P = D(g) P, \forall g \in G $$.

Notice that the complementary subspace of $$ S $$ is also non-trivial, and $$ I - P $$ is the projector onto it. We have

$$
\begin{eqnarray}
&& (I - P) D^\dagger (g) (I - P)  \\
&=& [(I - P) D(g) (I - P)]^\dagger  \\
&=& [(I - P) D(g) - I D(g) P + P D(g) P]^\dagger  \\
&=& [(I - P) D(g)]^\dagger  \\
&=& D^\dagger (g) (I - P).
\end{eqnarray}
$$

This proves that $$ D^\dagger $$ is also reducible. The *if* part can be proved similarly. 

#### Clarification on Eq. (1.46)

There can be multiple appearances of the same irreducible representation in the block diagnoal form of $$ D $$. They share the same lable $$ a $$
and is distinquished by different values of $$ x $$.

### 1.14 Eigenstates

As for why (1.104) is valid, refer to the derivation up to Eq. (1.57).

### 1.16 Example of tensor products

It is worth noting that $$ D_2 $$ as in (1.109) gives us reflections and rotations.

### 1.17 *Finding the normal modes

Eq. (1.115): The two $$ -\frac{\sqrt{3}}{6} $$ in the last column should be both $$ -\frac{1}{6} $$.

## Chapter 2

Note that when the author talks about a *unitary representation of the algebra*, he really means that the representation of the
group is unitary (and therefore the representation of the the algebra is hermitian).

### Eq. (2.36)

Note that when the group representation is unitary (or equivalent to a unitary representation), $$ T_a $$ is hermitian (or equivalent
to a hermitian matrix). So we have $$ \mathrm{Tr} (T_a T_a) \ge 0 $$.

### Eq. (2.39)

Typo. $$ \mathrm{Tr} $$ is missing before two pairs of parentheses.

## Chapter 6

### Eq. (6.2)

Note that since $$ X_{i j} := \mathrm{Tr}(H_i H_j) $$ is a real symmetric matrix (due to $$ H $$'s being hermitian), the similarity
transformation required can be chosen to be real, keeping $$ H $$'s hermitian.

### Under Eq. (6.14),

_States corresponding to different weights..._

Keep in mind that one should use (6.7) as the definition of inner products between two states, not the inner products of two 1-D
vectors, although one can prove the equivalence between the two.

### Under Eq. (6.22), _root vectors correspond to unique generators._
_root_ should be _non-zero root_. Idem for text under Eq. (8.26).

## Chapter 8

### Under Eq. (8.49)

_all 0, -1, -2 or -3..._

Too see why this is true, refer to (8.5) and (8.6).

_It is easy to see that the Cartan matrix is invertible because the a1 are complete and linearly independent._

Row $$ j $$ is simply $$ \alpha_j $$'s coordinates in the $$ e_i = 2 \alpha_i / \alpha_i^2 $$ basis. ($$ e_i $$'s are not orthonormal.)

### Eq. (8.63)

$$ 2 \sqrt{\frac{3}{2}} $$ should be $$ \sqrt{2} \sqrt{\frac{3}{2}} $$. All following equations are wrong by a factor of $$ \sqrt{2} $$.

## Chapter 9

### Under Eq. (9.11)

_But any such state with a positive $$ \phi $$..._

It is actually a bit more complicated than this. The commutator does not necessarily vanish so we may need to repeat the process
if the commutator itself gives us a positive $$ \phi $$. In the general case, we end up with a sum of (9.11) with only negative $$ \phi $$'s.

### Eq. (9.27)

$$ - n \mu^2 - m \mu^1 $$ is clearly a weight due to Weyl reflection.

### Under Eq. (9.39)

_is not a root_ should be _is not a weight_.

## Chapter 10

### Eq. (10.13)

It would be beneficial for clarity to put $$ T_a v $$ between parentheses, to indicate it is about the components of 
$$ (T_a v) $$. Then (10.13) can be easily derived from (10.8), (10.9), and (10.10).

### Under Eq. (10.16)

*the tracelessness of the $$ T_a $$s*

Actually it is the hermiticity, rather than tracelessness, is necessary (used in (10.6)).

The hermiticity is the $$ U $$ in $$ SU(3) $$, while the tracelessness is the $$ S $$.

### Under Eq. (10.32)

_The bra transforms under the algebra with an extra minus sign._

See (2.56).

### Under Eq. (10.35)

*the tensor $$ \bar{v} $$ is transformed by $$ T_a \bar{v} $$.*

This can be derived in the same way as (10.13).

### Above Eq. (10.41)

_a object_ should be _an object_.

### Eq. (10.49)

Note the $$ \mu_1 $$ and $$ \mu_2 $$ are the ones in (6.3) and have nothing to do with those in (9.3).

### Above Eq. (10.95)

$$ | s_1 - s_2 | $$ should be $$ 2 \cdot \mathrm{min}(s_1, s_2) $$.

### Under Eq. (10.96)

*the $$ k $$ index* should be *the $$ j $$ index* for both occurences.

### Eq. (10.99)

In the last line, $$ l $$ is the number of $$ 2 $$s in $$ i_1, i_2, \dots, i_k $$, or equivalently, the number of $$ 1 $$s
in $$ j_1, j_2, \dots, j_k $$.

### Under Eq. (10.114)

$$ 2 $$ should be $$ p + 2 $$.

### Eq. (10.116)

$$ s_1 + s_2 + k + m_1 + m_2 $$ should be $$ s_1 + s_2 - k + m_1 + m_2 $$

### Eq. (10.117)

$$ \begin{bmatrix} 2 s_1 + 2 s_2 - 2 \\ s_1 + s_2 - 1 + m_1 + m_2 \end{bmatrix} $$ should be
$$ \begin{bmatrix} 2 s_1 + 2 s_2 - 2 k \\ s_1 + s_2 - k + m_1 + m_2 \end{bmatrix} $$.

### Eq. (10.118)

After the first equality, $$ \frac{(2 s_1)! (2 s_2)!}{k!} $$ should be $$ \frac{(2 s_1)! (2 s_2)!}{(2 s_1 - k)! (2 s_2 - k)! k!} $$.

### Under Eq. (10.118)

_it is nice example_ should be _it is a nice example_.

### Problem 10.D.b

(10.86) should be (10.94) and $$ \langle 3/2, 1, 3/2, 1/2 | 1, 3/2, 0, 1/2 \rangle $$ should be
$$ \langle 3/2, 1/2 | 1, 3/2, 0, 1/2 \rangle $$.

### Problem 10.E

$$ SU(3) $$ should be $$ SU(2) $$.

# Solutions

## Chapter 1

### 1.A.

Trivial. Simply use the fact that every element appears once and only once in every row and column of the multiplication table.

### 1.B.

The only two possibilities are given below, corresponding to $$ Z_4 $$ and $$ Z_2 \times Z_2 $$ respectively.

$$
\begin{array}
\hline
 e & a & b & c  \\ \hline
 a & b & c & e  \\ \hline
 b & c & e & a  \\ \hline
 c & e & a & b  \\ \hline
\end{array}
$$

$$
\begin{array}
\hline
 e & a & b & c  \\ \hline
 a & e & c & b  \\ \hline
 b & c & e & a  \\ \hline
 c & b & a & e  \\ \hline
\end{array}
$$

### 1.C.

If the representation (1.135) of the permutation group is irreducible, (1.79) would apply.

### 1.D.

Applying Schur's lemma, we have $$ S^{-1} A = \lambda I $$, or $$ A = \lambda S $$.

### 1.E.

It is essentially the group $$ A_4 $$, made of permutations with an even number of 2-cycles. There are in total $$ 4! / 2 = 12 $$ elements.

There are four conjugacy classes.
- {e},
- {(12)(34), (13)(24), (14)(23)},
- {(123), (243), (134), (142)},
- {(132), (234), (143), (124).

From (1.74), we know the four irreducible representations must be of 1-, 1-, 1-. and 3-dimensional. By applying (1.79) and (1.87), we get the
following character table.

$$
\begin{array}
\hline
 \text{} & e & (12)(34) & (123)    & (321)     \\ \hline
 D_1     & 1 & 1        & 1        & 1         \\ \hline
 D_{1'}  & 1 & 1        & \omega   & \omega^2  \\ \hline
 D_{1''} & 1 & 1        & \omega^2 & \omega    \\ \hline
 D_3     & 3 & -1       & 0        & 0         \\ \hline
\end{array}
$$

### 1.F.

The group is a subset of $$ S_4 $$, with the following conjugacy classes.
- {e},
- {(13)(24)},
- {(13), (24)},
- {(12)(34), (14)(23)},
- {(1234), (4321)}.

Below is the character table.

$$
\begin{array}
\hline
 \text{}    & e & (13)(24) & (13)     & (12)(34) & (1234)    \\ \hline
 D_1        & 1 & 1        & 1        & 1        & 1         \\ \hline
 D_{1'}     & 1 & 1        & 1        & -1       & -1        \\ \hline
 D_{1''}    & 1 & 1        & -1       & 1        & -1        \\ \hline
 D_{1'''}   & 1 & 1        & -1       & -1       & 1         \\ \hline
 D_2        & 2 & -2       & 0        & 0        & 0         \\ \hline
\end{array}
$$

The $$ D_2 $$ representation is as follows.
- $$ \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} $$,
- $$ \begin{bmatrix} -1 & 0 \\ 0 & -1 \end{bmatrix} $$,
- $$ \begin{bmatrix} 0 & -1 \\ -1 & 0 \end{bmatrix} $$, $$ \begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix} $$
- $$ \begin{bmatrix} -1 & 0 \\ 0 & 1 \end{bmatrix} $$, $$ \begin{bmatrix} 1 & 0 \\ 0 & -1 \end{bmatrix} $$,
- $$ \begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix} $$, $$ \begin{bmatrix} 0 & 1 \\ -1 & 0 \end{bmatrix} $$.

The representation that describes the system is $$ D_4 \otimes D_2 $$, where $$ D_4 $$ is the defining representation. Its character table
is as follows.

$$
\begin{array}
\hline
 \text{} & e & (13)(24) & (13)     & (12)(34) & (1234)    \\ \hline
 D_8     & 8 & 0        & 0        & 0        & 0         \\ \hline
\end{array}
$$

Applying (1.88), we see that in $$ D_8 $$, $$ D_1 $$, $$ D_{1'} $$, $$ D_{1''} $$, and $$ D_{1'''} $$ each appears once, while $$ D_2 $$ appears twice.

The projection onto $$ D_1 $$ is

$$
\begin{eqnarray}
  P_1 &=& \frac{1}{8} \sum_{g \in G} \chi_1 (g)^* D_8(g)    \\
  &=& \frac{1}{8} \begin{bmatrix} 1 & 1 & -1 & 1 & -1 & -1 & 1 & -1 \end{bmatrix} ^T \begin{bmatrix} 1 & 1 & -1 & 1 & -1 & -1 & 1 & -1 \end{bmatrix},
\end{eqnarray}
$$

the so-called "breathing mode".

The projection onto $$ D_{1'} $$ is

$$
\begin{eqnarray}
  P_{1'} &=& \frac{1}{8} \sum_{g \in G} \chi_{1'} (g)^* D_8(g)    \\
  &=& \frac{1}{8} \begin{bmatrix} 1 & 1 & 1 & -1 & -1 & -1 & -1 & 1 \end{bmatrix} ^T \begin{bmatrix} 1 & 1 & 1 & -1 & -1 & -1 & -1 & 1 \end{bmatrix},
\end{eqnarray}
$$

where 1 and 3 are pulled away from each other while 2 and 4 are pushed towards each other (or the opposite).

The projection onto $$ D_{1''} $$ is

$$
\begin{eqnarray}
  P_{1''} &=& \frac{1}{8} \sum_{g \in G} \chi_{1''} (g)^* D_8(g)    \\
  &=& \frac{1}{8} \begin{bmatrix} 1 & -1 & -1 & -1 & -1 & 1 & 1 & 1 \end{bmatrix} ^T \begin{bmatrix} 1 & -1 & -1 & -1 & -1 & 1 & 1 & 1 \end{bmatrix},
\end{eqnarray}
$$

where the four points are pulled away from each other horizontally but pushed towards each other vertically (or the opposite).

The projection onto $$ D_{1'''} $$ is

$$
\begin{eqnarray}
  P_{1'''} &=& \frac{1}{8} \sum_{g \in G} \chi_{1'''} (g)^* D_8(g)    \\
  &=& \frac{1}{8} \begin{bmatrix} 1 & -1 & 1 & 1 & -1 & 1 & -1 & -1 \end{bmatrix} ^T \begin{bmatrix} 1 & -1 & 1 & 1 & -1 & 1 & -1 & -1 \end{bmatrix},
\end{eqnarray}
$$

which describes a rotation around the center.

The projection onto $$ D_2 $$ is

$$
\begin{eqnarray}
  P_2 &=& \frac{1}{8} \sum_{g \in G} \chi_2
   (g)^* D_8(g)    \\
  &=& \frac{1}{4} \begin{bmatrix} 1 & 1 \\ 1 & 1 \end{bmatrix} \otimes I_4,
\end{eqnarray}
$$

where $$ I_4 $$ is the four-dimensional identity matrix. It is equal to the sum of two matrices similar to (1.122) and (1.123), which describe
translations in the $$ x $$ direction and translations in the $$ y $$ direction.

## Chapter 2

### 2.A.

Taylor expanding $$ e^{i \alpha A} $$ and noticing that $$ A^n = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 0 & 0 \\ 0 & 0 & 1 \end{bmatrix} $$
with $$ n = 2, 4, \dots $$ while $$ A^n = A $$ with $$ n = 1, 3, \dots $$, we have

$$
\begin{eqnarray}
  e^{i \alpha A} &=& I - A^2 + A^2 + i \alpha A - \frac{\alpha^2 A^2}{2!} - \frac{i \alpha^2 A^3}{3!} + \cdots    \\
  &=& I - A^2 + A^2 \cos \alpha + i A \sin \alpha    \\
  &=& \begin{bmatrix} \cos \alpha & 0 & i \sin \alpha \\ 0 & 1 & 0 \\ i \sin \alpha & 0 & \cos \alpha \end{bmatrix}.
\end{eqnarray}
$$

### 2.B.

Applying (2.44), we have

$$
\begin{eqnarray}
  &&  e^{i \alpha A} B e^{-i \alpha A}    \\
  &=& B + i \alpha [A, B] - \frac{\alpha^2}{2} [A, [A, B]] - \frac{i \alpha^3}{3!} [A, [A, [A, B]]] + \cdots    \\
  &=& B + i \alpha B - \frac{\alpha^2}{2} B - \frac{i \alpha^3}{3!} B + \cdots    \\
  &=& B e^{i \alpha}.
\end{eqnarray}
$$

### 2.C.

To save some space, let's notate $$ \alpha_a X_a $$ as $$ A $$ and $$ \beta_a X_a $$ as $$ B $$.

We have

$$
\begin{eqnarray}
  K &=& e^{i A} e^{i B} - 1    \\
  &=& (1 + i A - \frac{1}{2} A^2 - \frac{i}{3!} A^3 + \cdots)    \\
  &&  (1 + i B - \frac{1}{2} B^2 - \frac{i}{3!} B^3 + \cdots) -1    \\
  &=& i A + i B - A B - \frac{1}{2} A^2 - \frac{1}{2} B^2    \\
  &&  - \frac{i}{3!} A^3 - \frac{i}{3!} B^3 - \frac{i}{2} A A B - \frac{i}{2} A B B + \cdots.    \\
\end{eqnarray}
$$

This gives

$$
\begin{eqnarray}
  i \delta_a X_a &=& K - \frac{1}{2} K^2 + \frac{1}{3} K^3    \\
  &=& i A + i B - A B - \frac{1}{2} A^2 - \frac{1}{2} B^2    \\
  &&  - \frac{i}{3!} A^3 - \frac{i}{3!} B^3 - \frac{i}{2} A A B - \frac{i}{2} A B B    \\
  &&  + \frac{1}{2}(A + B)^2 + \frac{i}{2} A A B + \frac{i}{2} A B A + \frac{i}{2} B A B + \frac{i}{2} A B B    \\
  &&  + \frac{i}{2} A^3 + \frac{i}{2} B^3 + \frac{i}{4} A B B + \frac{i}{4} B A A + \frac{i}{4} A A B + \frac{i}{4} B B A     \\
  &&  - \frac{i}{3} (A + B)^3 + \cdots     \\
  &=& i A + i B - \frac{1}{2} [A, B] + \frac{i}{12} [[A, B], A - B] + \cdots     \\
  &=& i X_a \left( \alpha_a + \beta_a - \frac{1}{2} \alpha_r \beta_s f_{rsa} - \frac{1}{12} \alpha_r \beta_s (\alpha_m - \beta_m) f_{rst} f_{tma} \right) + \cdots.     \\
\end{eqnarray}
$$

We can now write

$$ \delta_a = \alpha_a + \beta_a - \frac{1}{2} \alpha_r \beta_s f_{rsa} - \frac{1}{12} \alpha_r \beta_s (\alpha_m - \beta_m) f_{rst} f_{tma} + \cdots. $$

## Chapter 3

### 3.A.

We start with the highest weight $$ | j + s | j + s \rangle = |j, j \rangle |s, s \rangle $$ and repeatedly apply $$ J^- $$ on both
sides until they vanish. We get the spin $$ j + s $$ representation, with in total $$ 2(j + s) + 1 $$ states.

We will notice that $$ | j + s | j + s - 1 \rangle $$ is a linear combination of
$$ |j, j - 1 \rangle |s, s \rangle $$ and $$ |j, j \rangle |s, s - 1 \rangle $$, which spans a 2-dimensional space. These two vectors
can form another linear combination that is orthogonal to $$ | j + s | j + s - 1 \rangle $$, and this second linear combination can
only be $$ | j + s - 1| j + s - 1 \rangle $$. (To verify this claim, apply $$ J^+ $$ and see it vanish.) Now we can again repeatedly
apply $$ J^- $$ on both sides until they vanish. In doing so, we get the spin $$ j + s - 1 $$ representation, with in total
$$ 2(j + s) - 1 $$ states.

Now we do exactly the same with $$ | j + s - 1 | j + s - 2 \rangle $$, which is a linear combination of three vectors, which spans a
3-dimensional space. These three vectors can form a third linear combination that is orthogonal to both $$ | j + s | j + s - 2 \rangle $$
and $$ | j + s - 1 | j + s - 2 \rangle $$. This third linear combination can only be $$ | j + s - 2 | j + s - 2 \rangle $$...

The whole process stops at the spin $$ | j - s | $$ representation.

As a sanity check, both sides of the equation have $$ (2 j + 1) (2 s + 1) $$ linearly independent states. So everything works out.

### 3.B.

It is easy to show that there is a similarity transformation $$ S $$ such that

$$ S^{-1} (\hat{r} \cdot \sigma) S = \sigma_3. $$

We have

$$
\begin{eqnarray}
  && e^{i \vec{r} \cdot \vec{\sigma}}    \\
  &=& S e^{i |\vec{r}| \sigma_3} S^{-1}    \\
  &=& S \begin{bmatrix} e^{i |\vec{r}|} & 0 \\ 0 & e^{-i |\vec{r}|} \end{bmatrix} S^{-1}    \\
  &=& \cos |\vec{r}| \cdot I + i \sin |\vec{r}| \cdot (\hat{r} \cdot \sigma)
\end{eqnarray}
$$

### 3.C.

Let $$ S = \begin{bmatrix} \frac{1}{\sqrt{2}} & 0 & -\frac{1}{\sqrt{2}} \\ \frac{i}{\sqrt{2}} & 0 & \frac{i}{\sqrt{2}} \\ 0 & -1 & 0 \end{bmatrix} $$.

It is easy to verify that $$ J_i^1 = S^{-1} T_i S $$, where $$ J_i^1 $$ and $$ T_i $$ are the spin-1 representation and the adjoint
representation of the $$ i $$-th generator.

### 3.D.

$$
\begin{bmatrix}
  0 & 0 & 0 & -i \\
  0 & 0 & -i & 0 \\
  0 & i & 0 & 0 \\
  i & 0 & 0 & 0
\end{bmatrix}
$$.

### 3.E.

(a) $$ [\sigma_a, \sigma_b] \eta_c $$

(b) $$ \mathrm{Tr}(\sigma_a \cdot 2 \delta_{bd} I \sigma_c) = 4 \delta_{bd} \mathrm{Tr}(\delta_{ac} I) = 8 \delta_{ac} \delta_{bd} $$

(c) $$ \sigma_1 \sigma_2 \eta_1 \eta_2 - \sigma_2 \sigma_1 \eta_2 \eta_1 = 0 $$

## Chapter 4

### 4.A.

Simply check the [Table of Clebsch-Gordan coefficients](https://en.wikipedia.org/wiki/Table_of_Clebsch%E2%80%93Gordan_coefficients).
We see

$$ A = \langle 3/2, -1/2 | 1/2, 1, 1/2, -1 \rangle k_{\alpha \beta} = \sqrt{\frac{1}{3}} k_{\alpha \beta}. $$

What we want is

$$ \langle 3/2, -3/2 | 1/2, 1, -1/2, -1 \rangle k_{\alpha \beta} = k_{\alpha \beta} = \sqrt{3} A. $$

### 4.B.

We could repeatedly apply $$ L- $$ to both sides of $$ O_{+2} = r_{+1} r_{+1} $$. Or we could again check the
[Table of Clebsch-Gordan coefficients](https://en.wikipedia.org/wiki/Table_of_Clebsch%E2%80%93Gordan_coefficients).

Either way, we get the following.

$$
\begin{eqnarray}
  O_{+2} &=& r_{+1} r_{+1}  \\
  O_{+1} &=& \sqrt{\frac{1}{2}} r_{+1} r_{0} + \sqrt{\frac{1}{2}} r_{0} r_{+1}  \\
  O_{0}  &=& \sqrt{\frac{1}{6}} r_{+1} r_{-1} + \sqrt{\frac{2}{3}} r_{0} r_{0} + \sqrt{\frac{1}{6}} r_{-1} r_{+1}  \\
  O_{-1} &=& \sqrt{\frac{1}{2}} r_{-1} r_{0} + \sqrt{\frac{1}{2}} r_{0} r_{-1}  \\
  O_{-2} &=& r_{-1} r_{-1}
\end{eqnarray}
$$

Note the following relationships.

$$
\begin{eqnarray}
  r_{+1} &=& -(r_1 + i r_2) = -\sqrt{\frac{1}{2}} \sin \theta e^{i \phi}  \\
  r_{0}  &=& r_3 = \cos \theta  \\
  r_{-1} &=& (r_1 - i r_2) = \sqrt{\frac{1}{2}} \sin \theta e^{-i \phi}
\end{eqnarray}
$$

Compare these to the [Table of spherical harmonics](https://en.wikipedia.org/wiki/Table_of_spherical_harmonics)
with $$ l = 2 $$, we see they are identical up to a constant factor.

To generalize this construction to arbitrary $$ l $$, simply notice that

$$ [L^+, (r_{+1})^l] = 0. $$

The operator $$ (r_{+1})^l $$ is therefore the $$ O_{+l} $$ component of a spin $$ l $$ tensor operator. The rest is to
repeat the previous procedure.

### 4.C.

Note the similarity between this problem and 2.A. and 3.B.

We have

$$
\begin{eqnarray}
  &&  e^{i \alpha \hat{\alpha}_a X_a^1}    \\
  &=& I + i \alpha \hat{\alpha}_a X_a^1 - \frac{\alpha^2}{2!} (\hat{\alpha}_a X_a^1)^2 - \frac{i \alpha^3}{3!} \hat{\alpha}_a X_a^1 + \cdots   \\
  &=& I - (\hat{\alpha}_a X_a^1)^2 + i (\alpha - \frac{\alpha^3}{3!} + \cdots) \hat{\alpha}_a X_a^1 + (1 - \frac{\alpha^2}{2!} + \cdots) (\hat{\alpha}_a X_a^1)^2   \\
  &=& I - (\hat{\alpha}_a X_a^1)^2 + i \sin \alpha \hat{\alpha}_a X_a^1 + \cos \alpha (\hat{\alpha}_a X_a^1)^2.
\end{eqnarray}
$$

## Chapter 5

### 5.A.

Since pions are bosons, the full wave function (i.e. a wave function including position, spin, isospin, and whatever else) must be
symmetric. Since they are in an s-wave state, the wave function is symmetrical in the exchange of the position variables. Since pions
have spin 0, the wave function must also be symmetrical  in the exchange of isospin.

This means the total isospin has to be 2 (with 5 eigenstates) or 0 (with 1 eigenstate).

### 5.B.

It is not very clear whether it is asked to prove something similar to (5.17), or $$ [T_a, T_b] = \epsilon_{abc} T_c $$.

I assume it is the latter. (The former case is actual easier any way.)

We start with

$$
\begin{eqnarray}
  &&  [T_a, T_b]  \\
  &=& \sum_{x, m, m', \alpha} \sum_{y, n, n', \beta} [J^{j_x}_a]_{m m'} [J^{j_y}_b]_{n n'} [a^\dagger_{x, m, \alpha} a_{x, m', \alpha}, a^\dagger_{y, n, \beta} a_{y, n', \beta}].
\end{eqnarray}
$$

The commutator can be calculated as follows. (We take the commutator and plus sign if at least one of the two parts are bosons,
the anti-commutator and the plus sign if both are fermions.)

$$
\begin{eqnarray}
  &&  [a^\dagger_{x, m, \alpha} a_{x, m', \alpha}, a^\dagger_{y, n, \beta} a_{y, n', \beta}]  \\
  &=& a^\dagger_{x, m, \alpha} [a_{x, m', \alpha}, a^\dagger_{y, n, \beta} a_{y, n', \beta}] + [a^\dagger_{x, m, \alpha}, a^\dagger_{y, n, \beta} a_{y, n', \beta}] a_{x, m', \alpha}  \\
  &=& a^\dagger_{x, m, \alpha} [a_{x, m', \alpha}, a^\dagger_{y, n, \beta}]_\mp a_{y, n', \beta} \pm a^\dagger_{x, m, \alpha} a^\dagger_{y, n, \beta} [a_{x, m', \alpha}, a_{y, n', \beta}]_\mp  \\
  &&  \pm a^\dagger_{y, n, \beta} [a^\dagger_{x, m, \alpha}, a_{y, n', \beta}]_\mp a_{x, m', \alpha} + [a^\dagger_{x, m, \alpha}, a^\dagger_{y, n, \beta}]_\mp a_{y, n', \beta} a_{x, m', \alpha}  \\
  &=& \delta_{x y} \delta_{\alpha \beta} (\delta_{m' n} a^\dagger_{x, m, \alpha} a_{y, n', \beta} - \delta_{m n'} a^\dagger_{y, n, \beta} a_{x, m', \alpha}).
\end{eqnarray}
$$

We have therefore

$$
\begin{eqnarray}
  &&  [T_a, T_b]  \\
  &=& \sum_{x, m, n', \alpha} [J^{j_x}_a]_{m n} [J^{j_x}_b]_{n n'} a^\dagger_{x, m, \alpha} a_{x, n', \alpha} - [J^{j_x}_b]_{n m} [J^{j_x}_a]_{m m'} a^\dagger_{x, n, \alpha} a_{x, m', \alpha}  \\
  &=& \sum_{x, m, n', \alpha} [J^{j_x}_a, J^{j_x}_b]_{m n'} a^\dagger_{x, m, \alpha} a_{x, n', \alpha}  \\
  &=& \sum_{x, m, n', \alpha} \epsilon_{abc} [J^{j_x}_c]_{m n'} a^\dagger_{x, m, \alpha} a_{x, n', \alpha}  \\
  &=& \epsilon_{abc} T_c.
\end{eqnarray}
$$

5.C.

According to standard perturbation theory, which can be found in any QM book, the probability for a transition from $$ | i \rangle $$
to  $$ | j \rangle $$ to happen is proportional to $$ |\langle j | H | i \rangle|^2 $$.

We can assume $$ H $$ to be isospin invariant. Therefore the probability of $$ \pi^+ P \rightarrow \Delta^{++} $$ is proportional to
$$ |\langle 3/2, 3/2 | 1, 1/2, 1, 1/2 \rangle|^2 = 1 $$, while the probability of $$ \pi^- P \rightarrow \Delta^0 $$ is proportional to
$$ |\langle 3/2, -1/2 | 1, 1/2, -1, 1/2 \rangle|^2 = \frac{1}{3} $$. The ratio of the two is thus 3 to 1.

## Chapter 6

### 6.A.

In the adjoint representation,

$$
\begin{eqnarray}
  &&  H_i | [E_\alpha, E_\beta] \rangle  \\
  &=& [H_i, [E_\alpha, E_\beta]]  \\
  &=& [E_\alpha, [H_i, E_\beta]] + [[H_i, E_\alpha], E_\beta]  \\
  &=& \beta_i [E_\alpha, E_\beta] + \alpha_i [E_\alpha, E_\beta]  \\
  &=& (\alpha + \beta)_i [E_\alpha, E_\beta].
\end{eqnarray}
$$

If $$ \alpha + \beta $$ is a root, $$ [E_\alpha, E_\beta] $$ must be proportional to $$ E_{\alpha + \beta} $$
due to the uniqueness of non-zero roots. If $$ \alpha + \beta $$ is not a root, $$ [E_\alpha, E_\beta] $$ must be 0.

### 6.B.

From 6.A., $$ [E_\alpha, E_{-\alpha - \beta}] $$ is proportional to $$ E_{-\beta} $$. We can calculate the coefficient
as follows.

$$
\begin{eqnarray}
  &&  \langle E_{-\beta} | [E_\alpha, E_{-\alpha - \beta}] \rangle  \\
  &=& \lambda^{-1} \mathrm{Tr} (E^\dagger_{-\beta} [E_\alpha, E_{-\alpha - \beta}])  \\
  &=& \lambda^{-1} \mathrm{Tr} (E_{\beta} [E_\alpha, E_{-\alpha - \beta}])  \\
  &=& \lambda^{-1} \mathrm{Tr} ([E_{\beta}, E_\alpha], E_{-\alpha - \beta})  \\
  &=& -N \lambda^{-1} \mathrm{Tr} (E_{\alpha + \beta}, E^\dagger_{\alpha + \beta})  \\
  &=& -N  \\
\end{eqnarray}
$$

### 6.C.

Since $$ \sigma_3 $$ and $$ \sigma_3 \tau_3 $$ are already diagnoal, we can simply read off the four weights from these two
matrices. They are $$ (\pm 1, \pm 1) $$.

To get the roots, the foolproof method is to write down the two 10-dimensional matrices of $$ H_1 $$ and $$ H_2 $$ in the
adjoint representation and diagonalize them. In practice, the weights of the four-dimensional representation give us an
important hint. The difference between two *adjacient* weights is *often* a root. We know there are two zero roots
corresponding to $$ H_1 $$ and $$ H_2 $$ themselves. The eight remaining are thus $$ (\pm 2, 0), (0, \pm 2), (\pm 2, \pm 2) $$.

## Chapter 7

### 7.A.

We have $$ [T_1, T_4] = i \frac{1}{2} T_7 $$ and $$ [T_4, T_5] = i \frac{1}{2} T_3 + i \frac{\sqrt{3}}{2} T_8 $$, and
therefore $$ f_{147} = \frac{1}{2} $$ and $$ f_{456} = 0 $$.

### 7.B.

$$ T_1 $$, $$ T_2 $$ and $$ T_3 $$ generate an SU(2) subalgebra because they have the right commutation relationship.

Under this subalgebra, the first two eigenvectors (corresponding to the top weights in (7.11)) of the defining representation
form a doublet (spin 1) while the third eigenvectos (corresponding to the bottom weight in (7.11)) form a singlet (spin 0).

The adjoint representation breaks down into the following SU(2) representations.
- Spin 1/2: $$ E_{1/2, \sqrt{3}/2} $$ and $$ E_{-1/2, \sqrt{3}/2} $$ form a doublet.
- Spin 1/2: $$ E_{1/2, -\sqrt{3}/2} $$ and $$ E_{-1/2, -\sqrt{3}/2} $$ form a doublet.
- Spin 1: $$ E_{1, 0} $$, $$ T_{3} $$, and $$ E_{-1, 0} $$ form a triplet.
- Spin 0: $$ T_{0} $$ forms a singlet.

Remember that the generators are the states in the adjoint representation.

### 7.C.

Defining $$ J_1 $$, $$ J_2 $$, $$ J_3 $$ as $$ \lambda_2 $$, $$ \lambda_5 $$, and $$ \lambda_7 $$, we get
the commutation relationship of SU(2).

The eigenvectors of $$ J_3 $$ are $$ (0, 1, i)^T $$, $$ (- \sqrt{2}i, 0, 0)^T $$, and $$ (0, 1, -i)^T $$ with eigenvalues
1, 0, and -1. Applying $$ J^\pm = \frac{1}{\sqrt{2}} (J_1 \pm J_2) $$, we see that they form a triplet of the spin 1
representation of SU(2)

The adjoint representation breaks down into the following SU(2) representations.
- Spin 1: $$ J^+ $$, $$ -J_3 $$, and $$ -J^- $$ form a triplet.
- Spin 2: $$ \lambda_3 - 2i \lambda_6 - \sqrt{3} \lambda_8 $$, $$ 2i \lambda_1 - 2 \lambda_4 $$,
  $$ \sqrt{6} \lambda_3 + \sqrt{2} \lambda_8 $$, $$ 2i \lambda_1 + 2 \lambda_4 $$, and
  $$ \lambda_3 + 2i \lambda_6 - \sqrt{3} \lambda_8 $$ form a quintuplet, with their $$ J_3 $$ value equal to
  2, 1, 0, -1, and 2 respectively.

Remember that the generators are the states in the adjoint representation.

The foolproof way to get the correct result is to write down the matrix of $$ \lambda_7 $$ in the adjoint representation
and diagonalize it.

## Chapter 8

### 8.A.

The simple roots are $$ \alpha_1 = (0, 2) $$ and $$ \alpha_2 = (2, -2) $$. There should be two lines between the two dots
in the Dynkin diagram since $$ \cos \theta_{\alpha_1 \alpha_2} = 135^\circ $$.

The fundamental weights are $$ \mu_1 = (1, 1) $$ and $$ \mu_2 = (2, 0) $$.

### 8.B.

We can take $$ H_1 = \sigma_3 (1 + \eta_1) $$ and $$ H_2 = \sigma_3 (1 - \eta_1) $$ as the two Cartan generators.
Either diagonalizing them in the adjoint representation, or by a bit trials and errors, we can find the following
roots.

$$
\begin{eqnarray}
  E_{1, 0}    &=& \sigma^+ (1 + \eta_1) \\
  E_{0, -1}   &=& \sigma^+ (1 - \eta_1) \\
  E_{0, 1}   &=& \sigma^- (1 - \eta_1) \\
  E_{-1, 0}  &=& \sigma^- (1 + \eta_1)
\end{eqnarray}
$$

The algebra can be split into two mutually commuting subalgebras. The first consists of $$ H_1 $$, $$ E_{1, 0} $$,
and $$ E_{-1, 0} $$, while the second consists of the rest. Therefore the algebra is not simple. We cannot find
a generator that commutes with everything — such a generator would be a zero matrix in the adjoint representation
and we know all the six generators are linearly independent — and therefore the algebra is semisimple.

The Dynkin diagram is made of two isolated dots corresponding to $$ E_{1, 0} $$ and $$ E_{0, 1} $$.

### 8.C.

Below is the Cartan matrix.

$$
\begin{bmatrix}
  2   & -1  & 0   \\
  -1  & 2   & -2  \\
  0   & -1  & 2
\end{bmatrix}
$$

I won't draw the diagram here to get all positive roots. Instead, I will simply list all the positive roots below.

- $$ k = 1 $$:  $$ \begin{bmatrix} 2 & -1 & 0 \end{bmatrix} $$, $$ \begin{bmatrix} -1 & 2 & -2 \end{bmatrix} $$, $$ \begin{bmatrix} 0 & -1 & 2 \end{bmatrix} $$.
- $$ k = 2 $$:  $$ \begin{bmatrix} 1 & 1 & -2 \end{bmatrix} $$, $$ \begin{bmatrix} -1 & 1 & 0 \end{bmatrix} $$.
- $$ k = 3 $$:  $$ \begin{bmatrix} 1 & 0 & 0 \end{bmatrix} $$, $$ \begin{bmatrix} -1 & 0 & 2 \end{bmatrix} $$.
- $$ k = 4 $$:  $$ \begin{bmatrix} 1 & -1 & 2 \end{bmatrix} $$.
- $$ k = 5 $$:  $$ \begin{bmatrix} 0 & 1 & 0 \end{bmatrix} $$.

## Chapter 9

### 9.A.

The general idea is to use commutation rules (6.12), (6.19), and (8.4) to move $$ E_{-\alpha^i} $$s to the left and $$ E_{\alpha^i} $$
to the right. Also note that $$ \mu = \mu_1 + \mu_2 = \alpha_1 + \alpha_2 $$. 

$$
\begin{eqnarray}
  &&  \langle A | A \rangle \\
  &=& \langle \mu | E_{\alpha^2} E_{\alpha^1} | E_{-\alpha^1} E_{-\alpha^2} | \mu \rangle  \\
  &=& (\alpha^1)_i \langle \mu | E_{\alpha^2} H_i E_{-\alpha^2} | \mu \rangle + \langle \mu | E_{\alpha^2} E_{-\alpha^1} E_{\alpha^1} E_{-\alpha^2} | \mu \rangle  \\
  &=& (\alpha^1)_i \langle \mu | E_{\alpha^2} H_i E_{-\alpha^2} | \mu \rangle  \\
  &=& (\alpha^1)_i \left( (-\alpha^2)_i \langle \mu | E_{\alpha^2} E_{-\alpha^2} | \mu \rangle + \langle \mu | E_{\alpha^2} E_{-\alpha^2} H_i | \mu \rangle \right)  \\
  &=& \left( \alpha^1 \cdot (\mu - \alpha^2) \right) \langle \mu | E_{\alpha^2} E_{-\alpha^2} | \mu \rangle  \\
  &=& | \alpha^1 |^2 \left( (\alpha^2)_i \langle \mu | H_i | \mu \rangle + \langle \mu | E_{-\alpha^2} E_{\alpha^2} | \mu \rangle \right)  \\
  &=& \alpha^2 \cdot \mu  \\
  &=& 1/2
\end{eqnarray}
$$

Similarly, we have $$ \langle A | A \rangle = 1/2 $$ and

$$
\begin{eqnarray}
  &&  \langle A | B \rangle \\
  &=& \langle \mu | E_{\alpha^2} E_{\alpha^1} | E_{-\alpha^2} E_{-\alpha^1} | \mu \rangle  \\
  &=& \langle \mu | E_{\alpha^2} E_{-\alpha^2} E_{\alpha^1} E_{-\alpha^1} | \mu \rangle  \\
  &=& (\alpha^2)_i (\alpha^1)_j \langle \mu | H_i H_j | \mu \rangle  \\
  &=& (\alpha^2 \cdot \mu) (\alpha^1 \cdot \mu)  \\
  &=& 1/4.
\end{eqnarray}
$$

Clearly $$ \langle A | A \rangle \langle B | B \rangle \neq \langle A | B \rangle \langle B | A \rangle $$.

(Refer to any elementary QM book for why the equality is needed for $$ | A \rangle $$ and $$ | B \rangle $$ to be linearly dependent.)

### 9.B.

One way to solve this problem is to diagonalize $$ \frac{1}{2} \lambda_3 \sigma_2 $$ and
$$ \frac{1}{2} \lambda_8 \sigma_2 $$ and find out all the weights, which are the weights of (1, 0) and those of
(0, 1), and conclude it can be reduced into (1, 0) and (0, 1).

Or, we could notice the following.
- $$ \frac{1}{2} \lambda_a \otimes \sigma_2 $$ can be replaced by $$ \sigma_2 \otimes \frac{1}{2} \lambda_a $$, and
$$ \frac{1}{2} \lambda_a \otimes I $$ by $$ I \otimes \frac{1}{2} \lambda_a $$. This does not change the commutation
relationships at all.
- There is a similarity transformation S ($$ \frac{1}{\sqrt{2}} \begin{bmatrix} 1 & 1 \\ i & -i \end{bmatrix} $$) that transforms
$$ \sigma_2 $$ to $$ \sigma_3 $$.
- With the similarity transformations $$ S \otimes I $$, both $$ \sigma_3 \otimes \frac{1}{2} \lambda_a $$ for $$ a = 1, 3, 4, 6, 8 $$
and $$ I \otimes \frac{1}{2} \lambda_a $$ for $$ a = 2, 5, 7 $$ become
$$ \begin{bmatrix} \frac{1}{2} \lambda_a & 0 \\ 0 & -\frac{1}{2} \lambda_a^* \end{bmatrix} $$, since the former are real while
the latter are imaginery.

Now we have explicitly block diagonalized the 6-dimensional representation into a $$ 3 $$ and a $$ \bar{3} $$.

### 9.C.

The three weights of 3 are $$ | \mu_1 \rangle $$, $$ | \mu_1 - \alpha_1 \rangle $$, and
$$ | \mu_1 - \alpha_1 - \alpha_2 \rangle $$.

The highest weight of $$ 3 \times 3 $$ is $$ | \mu_{6, 2 \mu_1} \rangle = | \mu_1 \rangle | \mu_1 \rangle $$. Repeatedly applying
one of the two lowering operators on both sides, we get all the states in 6.

$$
\begin{eqnarray}
  &&            | \mu_{6, 2 \mu_1} \rangle = | \mu_1 \rangle | \mu_1 \rangle  \\
  &\Rightarrow& | \mu_{6, 2 \mu_1 - \alpha_1} \rangle = \frac{1}{\sqrt{2}} \left( | \mu_1 - \alpha_1 \rangle | \mu_1 \rangle + | \mu_1 \rangle | \mu_1 - \alpha_1 \rangle \right)  \\
  &\Rightarrow& | \mu_{6, 2 \mu_1 - 2 \alpha_1} \rangle = | \mu_1 - \alpha_1 \rangle | \mu_1 - \alpha_1 \rangle  \\
  &&            | \mu_{6, 2 \mu_1 - \alpha_1 - \alpha_2} \rangle = \frac{1}{\sqrt{2}} \left( | \mu_1 - \alpha_1 - \alpha_2 \rangle | \mu_1 \rangle + | \mu_1 \rangle | \mu_1 - \alpha_1 - \alpha_2 \rangle \right)  \\
  &\Rightarrow& | \mu_{6, 2 \mu_1 - 2 \alpha_1 - \alpha_2} \rangle = \frac{1}{\sqrt{2}} \left( | \mu_1 - \alpha_1 - \alpha_2 \rangle | \mu_1 - \alpha_1 \rangle + | \mu_1 - \alpha_1 \rangle | \mu_1 - \alpha_1 - \alpha_2 \rangle \right)  \\
  &\Rightarrow& | \mu_{6, 2 \mu_1 - 2 \alpha_1 - 2 \alpha_2} \rangle = | \mu_1 - \alpha_1 - \alpha_2 \rangle | \mu_1 - \alpha_1 - \alpha_2 \rangle
\end{eqnarray}
$$

Taking the state orthogonal to $$ | \mu_{6, 2 \mu_1 - \alpha_1} \rangle $$ to be $$ | \mu_{\bar{3}, 2 \mu_1 - \alpha_1} \rangle $$,
and repeatedly applying one of the two lowering operators on both sides, we get all the states in $$ \bar{3} $$.

$$
\begin{eqnarray}
  &&            | \mu_{\bar{3}, 2 \mu_1 - \alpha_1} \rangle = \frac{1}{\sqrt{2}} \left( | \mu_1 - \alpha_1 \rangle | \mu_1 \rangle - | \mu_1 \rangle | \mu_1 - \alpha_1 \rangle \right)  \\
  &\Rightarrow& | \mu_{\bar{3}, 2 \mu_1 - \alpha_1 - \alpha_2} \rangle = \frac{1}{\sqrt{2}} \left( | \mu_1 - \alpha_1 - \alpha_2 \rangle | \mu_1 \rangle - | \mu_1 \rangle | \mu_1 - \alpha_1 - \alpha_2 \rangle \right)  \\
  &\Rightarrow& | \mu_{\bar{3}, 2 \mu_1 - 2 \alpha_1 - \alpha_2} \rangle = \frac{1}{\sqrt{2}} \left( | \mu_1 - \alpha_1 - \alpha_2 \rangle | \mu_1 - \alpha_1 \rangle - | \mu_1 - \alpha_1 \rangle | \mu_1 - \alpha_1 - \alpha_2 \rangle \right)
\end{eqnarray}
$$

## Chapter 10

### 10.A.

See (11.39).

### 10.B.

Applying Eq. (10.36) and Eq. (10.13), we have

$$
\begin{eqnarray}
  &&  \langle u | T_a | v \rangle \\
  &=& \bar{u}^i_j (T_a v)^j_i  \\
  &=& \bar{u}^i_j ([T_a]^j_k v^k_i - [T_a]^k_i v^j_k)  \\
  &=& \frac{1}{2} (\bar{u}^i_j [\lambda_a]^j_k v^k_i) - \frac{1}{2} (\bar{u}^i_j v^j_k [\lambda_a]^k_i)  \\
  &=& \frac{1}{2} \mathrm{Tr}(\bar{u} \lambda_a v) - \frac{1}{2} \mathrm{Tr}(\bar{u} v \lambda_a),
\end{eqnarray}
$$

in agreement with Eq. (10.66). In fact, we could have started with Eq. (10.66) and take two special cases to solve
for $$ \lambda_1 $$ and $$ \lambda_2 $$.

### 10.C.

One can read off the components directly from the solution to 9.C. Simply notice that
$$ | \mu_1 \rangle \equiv |_1 \rangle $$, $$ | \mu_1 - \alpha_1 - \alpha_2 \rangle \equiv |_2 \rangle $$,
and $$ | \mu_1 - \alpha_1 \rangle \equiv |_3 \rangle $$.

### 10.D.a.

$$ \Delta $$ particles form a quartet and have components $$ \Delta^{ijk} $$. In particular, $$ \Delta^{++} $$ has
$$ \Delta^{111} = 1 $$ with the other components being zero, while $$ \Delta^0 $$ has
$$ \Delta^{122} = \Delta^{212} = \Delta^{221} = \frac{1}{\sqrt{3}} $$ with the other components being zero.

Pions form a triplet and have components $$ \pi^{ij} $$. In particular, $$ \pi^+ $$ has
$$ \pi^{11} = 1 $$ with the other components being zero, while $$ \pi^- $$ has
$$ \pi^{22} = 1 $$ with the other components being zero.

Nucleons form a doublet and have components $$ N^i $$. In particular, $$ p $$ has
$$ N^1 = 1 $$ and $$ N^2 = 0 $$.

Now we have

$$ \langle \Delta^{++} | H | \pi^+ p \rangle $$ = $$ \lambda \bar{\Delta}_{ijk} \pi^{ij} N^k = \lambda, $$

and

$$ \langle \Delta^0 | H | \pi^- p \rangle $$ = $$ \lambda \bar{\Delta}_{ijk} \pi^{ij} N^k = \frac{1}{\sqrt{3}} \lambda. $$

Note that we do not have to symmetrize $$ \pi^{ij} N^k $$ because the symmetry of $$ \bar{\Delta}_{ijk} $$
will automatically filter out any non-symmetrical aspect of $$ \pi^{ij} N^k $$. This is generally true
for any symmetrical pattern.

The probability ratio between the two interactions is thus 3:1.

### 10.D.b.

Applying $$ J^+ $$ to both sides of

$$ | \frac{3}{2}, \frac{3}{2} \rangle = \alpha | \frac{3}{2}, \frac{1}{2} \rangle \otimes | 1, 1 \rangle + \beta | \frac{3}{2}, \frac{3}{2} \rangle, \otimes | 1, 0 \rangle $$

we get 

$$ 0 = \sqrt{\frac{3}{2}} \alpha | \frac{3}{2}, \frac{3}{2} \rangle \otimes | 1, 1 \rangle + \beta | \frac{3}{2}, \frac{3}{2} \rangle, \otimes | 1, 1 \rangle. $$

We can therefore choose $$ \alpha = \sqrt{\frac{2}{5}} $$ and $$ \beta = -\sqrt{\frac{3}{5}} $$.

Applying $$ J^- $$ to both sides of

$$ | \frac{3}{2}, \frac{3}{2} \rangle = \sqrt{\frac{2}{5}} | \frac{3}{2}, \frac{1}{2} \rangle \otimes | 1, 1 \rangle - \sqrt{\frac{3}{5}} | \frac{3}{2}, \frac{3}{2} \rangle, \otimes | 1, 0 \rangle $$

we get 

$$
\begin{eqnarray}
  &&  \sqrt{\frac{3}{2}} | \frac{3}{2}, \frac{1}{2} \rangle  \\
  &=& + \sqrt{\frac{2}{5}} \sqrt{2} | \frac{3}{2}, -\frac{1}{2} \rangle \otimes | 1, 1 \rangle + \sqrt{\frac{2}{5}} | \frac{3}{2}, \frac{1}{2} \rangle \otimes | 1, 0 \rangle  \\
  &&  - \sqrt{\frac{3}{5}} \sqrt{\frac{3}{2}} | \frac{3}{2}, \frac{1}{2} \rangle, \otimes | 1, 0 \rangle - \sqrt{\frac{3}{5}} | \frac{3}{2}, \frac{3}{2} \rangle, \otimes | 1, -1 \rangle.
\end{eqnarray}
$$

Therefore the Clebsch-Gordan coefficient $$ \langle \frac{3}{2}, \frac{1}{2} | \frac{3}{2}, 1, \frac{1}{2}, 0 \rangle = - \frac{1}{\sqrt{15}} $$.

We get exactly the same result by using (10.94). (Note the sign is arbitrary.)

### 10.E.a.

The components of pions and nucleons can be read in the solution to 10.D.a. except for $$ \pi^0 $$ and $$ n $$, whose
non-zero components are $$ \pi^{12} = \pi^{21} = \frac{1}{\sqrt{2}} $$ and $$ N^2 = 1 $$.

For $$ \langle \pi^+ p | H_I | \pi^+ p \rangle $$, we have $$ i = j = k = 1 $$ giving us two non-zero terms

$$ A_{+p} = A_1 + A_2. $$

For $$ \langle \pi^- p | H_I | \pi^- p \rangle $$, we have $$ j = k = 2 $$ and $$ l = 1 $$ gives the only non-zero term

$$ A_{-p} = A_1. $$

For $$ \langle \pi^0 n | H_I | \pi^- p \rangle $$, we have $$ j = l = 2 $$ and $$ k = 1 $$ gives the only non-zero term

$$ A_{0n} = \frac{1}{\sqrt{2}} A_2. $$

Therefore we have

$$ A_{+p} = A_{-p} + \sqrt{2} A_{0n}. $$

### 10.E.b.

We have the following.

$$
\begin{eqnarray}
  | \pi^+ p \rangle &=& |1, 1 \rangle \otimes | \frac{1}{2}, \frac{1}{2} \rangle = | \frac{3}{2}, \frac{3}{2} \rangle  \\
  | \pi^- p \rangle &=& |1, -1 \rangle \otimes | \frac{1}{2}, \frac{1}{2} \rangle = \sqrt{\frac{1}{3}} | \frac{3}{2}, -\frac{1}{2} \rangle + \sqrt{\frac{2}{3}} | \frac{1}{2}, -\frac{1}{2} \rangle  \\
  | \pi^0 n \rangle &=& |1, 0 \rangle \otimes | \frac{1}{2}, -\frac{1}{2} \rangle = \sqrt{\frac{2}{3}} | \frac{3}{2}, -\frac{1}{2} \rangle - \sqrt{\frac{1}{3}} | \frac{1}{2}, -\frac{1}{2} \rangle
\end{eqnarray}
$$

Therefore we have

$$
\begin{eqnarray}
  A_{+p} &=& A_{I=3/2}  \\
  A_{-p} &=& \frac{1}{3} A_{I=3/2} + \frac{2}{3} A_{I=1/2}  \\
  A_{0n} &=& \frac{\sqrt{2}}{3} A_{I=3/2} - \frac{\sqrt{2}}{3} A_{I=1/2}
\end{eqnarray}
$$

Again we have

$$ A_{+p} = A_{-p} + \sqrt{2} A_{0n}. $$
