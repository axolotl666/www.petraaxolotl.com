---
title: "Lie Algebras in Particle Physics"
Description: "Errata, Notes, and Problem Solutions to Lie Algebras in Particle Physics"
date: 2017-01-02T14:59:31-08:00
---

This is a great book. However, a better book for beginners may be Zee's _Group Theory in a Nutshell for Physicists_. Zee's book is mathematically
less rigorous and sometimes made me cringe a bit. But it has a much better overview of finite groups, which are quite important for a beginner to
build intuition. It also covers Lorentz's group, which is obviously important for special relativity and quantum field theory.

# Errata and Notes

## Chapter 1

### 1.4 Irreducible representations

#### (1.10)

It would be much easier if we were talking about (unitary) matrices instead of (unitary) operators. The former can be considered independent
entities and we are free to do similarity transformations. The latter operates on vectors, which usually have pre-defined concepts such as inner
products (and therefore orthogonality). In this sense, we cannot make a non-unitary operator unitary through a similarity transformation.
We can only make the matrix form of a non-unitary operator unitary by choosing a non-orthonormal basis. But this does not make the operator
itself unitary.

Since $$ S $$ in (1.10) is in general not unitary, two equivalent representations do not just differ by *a trivial choice of basis*
physically. 

#### Definition of reducibility

The defintion of reducible representations should be those that have a *non-trivial* invariant subspace. Obviously $$ \\{ 0 \\} $$ and the
original vector space are always invariant subspaces for any representations.

#### (1.13)

The statement that *$$ D_j(g) $$ is irreducible* implies that $$ D_j(g) $$ should be understood as an operator defined on a non-trivial
subspace $$ X_j $$ of the original space $$ X $$, and $$ X_i $$ and $$ X_j $$ are orthogonal to each other for $$ i \neq j $$
and $$ \cup_j X_j = X $$.

### 1.9 Useful theorems

#### Theorem 1.1

$$ U $$ is implicitly assumed to be unitary in (1.28) and this guarantees that $$ X $$ is hermitian in (1.31). The proof that a unitary
$$ U $$ always exists to diagonalize a hermitian matrix can be found in most QM books. In fact $$ U $$ can be simply made of orthonormal
eigenvectors of $$ S $$ juxtaposed next to each other.

#### Theorem 1.2

The proof implicitly uses the following fact.

*If two representations $$ D_1 $$ and $$ D_2 $$ are equivalent, $$ D_1 $$ is reducible if and only if $$ D_2 $$ is reducible.* The proof is
as follows.

If $$ D_1 $$ is reducible, there exists a non-trivial subspace $$ X $$ whose projector $$ P $$ satisfies (1.11)

$$ P D_1(g) P = D_1(g) P, \forall g \in G. $$

Since $$ D_1 $$ and $$ D_2 $$ are equivalent, there exists an invertible transformation $$ S $$ such that
$$ D_1(g) = S^{-1} D_2(g) S, \forall g \in G $$, therefore

$$ P S^{-1} D_2(g) S P = S^{-1} D_2(g) S P, $$

or equivalently

$$ S P S^{-1} D_2(g) S P S^{-1} = D_2(g) S P S^{-1}, $$

where $$ S P S^{-1} $$ is the projector of subspace $$ S X $$, which has the same dimensionality of the original invariant subspace $$ X $$.
Obviously $$ S X $$ is non-trivial since there exists $$ v \not \in X $$ and it is easy to show that $$ S v \not \in S X $$. This proves that
$$ D_2 $$ is also reducible.

Similarly, one can easily prove that if two representations $$ D_1 $$ and $$ D_2 $$ are equivalent, $$ D_1 $$ is completely reducible
if and only if $$ D_2 $$ is completely reducible.

### 1.10 Subgroups

The statement *Every element of $$ G $$ must belong to one and only one coset* is made without proof. The *one* part is rather obvious since
$$ e $$ must be in the subgroup. Below is the proof of the *only one* part.

Assuming that $$ x \in H g_1 $$ and $$ x \in H g_2 $$, we want to prove that $$ H g_1 = H g_2 $$ by showing that $$ H g_1 \subset H g_2 $$ and
$$ H g_2 \subset H g_1 $$.

There must exist $$ x_1 \in H $$ and $$ x_2 \in H $$ such that $$ x = x_1 g_1 = x_2 g_2 $$. This implies that $$ g_1 = {x_1}^{-1} x_2 g_2 $$.

For any $$ y \in H g_1 $$, there exists $$ y_1 \in H $$ such that $$ y = y_1 g_1 = y_1 {x_1}^{-1} x_2 g_2 $$. Note that $$ y_1 $$,
$$ {x_1}^{-1} $$ and $$ x_2 $$ are all elements of $$ H $$ and therefore $$ y_1 {x_1}^{-1} x_2 \in H $$ and $$ y \in H g_2 $$. This proves
$$ H g_1 \subset H g_2 $$.

$$ H g_2 \subset H g_1 $$ can be proved similarly.

### 1.11 Schur's lemma

#### Theorem 1.3

In the proof it is rather hand-waving to simply state *A similar argument shows that $$ A $$ vanishes if there is a $$ \langle v| $$ which
annihilates $$ A $$*, because it actually requires the irreducibility of $$ D_1^\dagger $$ rather than that of $$ D_1 $$ itself and thus
the following fact.

*A representation $$ D $$ is reducible if and only if its Hermitian conjugate $$ D^\dagger $$ is reducible.*

The proof is as follows. Note that this proof does not require $$ D $$ to be a representation of a finite $$ G $$. It is equally valid
even if $$ G $$ is infinite.

If $$ D $$ is reducible, there is a non-trivial subspace $$ S $$ such that its projector $$ P $$ satisfies $$ P D(g) P = D(g) P, \forall g \in G $$.

Notice that the complementary subspace of $$ S $$ is also non-trivial, and $$ I - P $$ is the projector onto it. We have

$$
\begin{eqnarray}
&& (I - P) D^\dagger (g) (I - P)  \\
&=& [(I - P) D(g) (I - P)]^\dagger  \\
&=& [(I - P) D(g) - I D(g) P + P D(g) P]^\dagger  \\
&=& [(I - P) D(g)]^\dagger  \\
&=& D^\dagger (g) (I - P).
\end{eqnarray}
$$

This proves that $$ D^\dagger $$ is also reducible. The *if* part can be proved similarly. 

#### Clarification on (1.46)

There can be multiple appearances of the same irreducible representation in the block diagnoal form of $$ D $$. They share the same lable $$ a $$
and is distinquished by different values of $$ x $$.

### 1.14 Eigenstates

As for why (1.104) is valid, refer to the derivation up to (1.57).

### 1.16 Example of tensor products

It is worth noting that $$ D_2 $$ as in (1.109) gives us reflections and rotations.

### 1.17 *Finding the normal modes

(1.115): The two $$ -\frac{\sqrt{3}}{6} $$ in the last column should be both $$ -\frac{1}{6} $$.

## Chapter 2

Note that when the author talks about a *unitary representation of the algebra*, he really means that the representation of the
group is unitary (and therefore the representation of the the algebra is hermitian).

### (2.36)

Note that when the group representation is unitary (or equivalent to a unitary representation), $$ T_a $$ is hermitian (or equivalent
to a hermitian matrix). So we have $$ \mathrm{Tr} (T_a T_a) \ge 0 $$.

### (2.39)

Typo. $$ \mathrm{Tr} $$ is missing before two pairs of parentheses.

## Chapter 6

### (6.2)

(2.37) shows that (6.2) can be achieved in the adjoint representation. If fact, once it is
done in the adjoint representation, (6.2) is automatically valid in other irreducible representations.
A proof can be found [here](https://physics.stackexchange.com/a/661122/269697).

Note that since $$ X_{i j} := \mathrm{Tr}(H_i H_j) $$ is a real symmetric matrix (due to $$ H $$'s being hermitian), the similarity
transformation required can be chosen to be real, keeping $$ H $$'s hermitian.

### Under (6.14),

_States corresponding to different weights..._

Keep in mind that one should use (6.7) as the definition of inner products between two states, not the inner products of two 1-D
vectors, although one can prove the equivalence between the two.

### Under (6.22), _root vectors correspond to unique generators._
By definition, root vectors are non-zero vectors.

## Chapter 8

### Under (8.49)

_all 0, -1, -2 or -3..._

Too see why this is true, refer to (8.5) and (8.6).

_It is easy to see that the Cartan matrix is invertible because the a1 are complete and linearly independent._

Row $$ j $$ is simply $$ \alpha_j $$'s coordinates in the $$ e_i = 2 \alpha_i / \alpha_i^2 $$ basis. ($$ e_i $$'s are not orthonormal.)

### (8.63)

$$ 2 \sqrt{\frac{3}{2}} $$ should be $$ \sqrt{2} \sqrt{\frac{3}{2}} $$. All following equations are wrong by a factor of $$ \sqrt{2} $$.

## Chapter 9

### Under (9.11)

_But any such state with a positive $$ \phi $$..._

It is actually a bit more complicated than this. The commutator does not necessarily vanish so we may need to repeat the process
if the commutator itself gives us a positive $$ \phi $$. In the general case, we end up with a sum of (9.11) with only negative $$ \phi $$'s.

### (9.27)

$$ - n \mu^2 - m \mu^1 $$ is clearly a weight due to Weyl reflection.

### Under (9.39)

_is not a root_ should be _is not a weight_.

## Chapter 10

### (10.13)

It would be beneficial for clarity to put $$ T_a v $$ between parentheses, to indicate it is about the components of 
$$ (T_a v) $$. Then (10.13) can be easily derived from (10.8), (10.9), and (10.10).

### Under (10.16)

*the tracelessness of the $$ T_a $$s*

Actually it is the hermiticity, rather than tracelessness, is necessary (used in (10.6)).

The hermiticity is the $$ U $$ in $$ SU(3) $$, while the tracelessness is the $$ S $$.

### Under (10.32)

_The bra transforms under the algebra with an extra minus sign._

See (2.56).

### Under (10.35)

*the tensor $$ \bar{v} $$ is transformed by $$ T_a \bar{v} $$.*

This can be derived in the same way as (10.13).

### Above (10.41)

_a object_ should be _an object_.

### (10.49)

Note the $$ \mu_1 $$ and $$ \mu_2 $$ are the ones in (6.3) and have nothing to do with those in (9.3).

### Above (10.95)

$$ | s_1 - s_2 | $$ should be $$ 2 \cdot \mathrm{min}(s_1, s_2) $$.

### Under (10.96)

*the $$ k $$ index* should be *the $$ j $$ index* for both occurences.

### (10.99)

In the last line, $$ l $$ is the number of $$ 2 $$s in $$ i_1, i_2, \dots, i_k $$, or equivalently, the number of $$ 1 $$s
in $$ j_1, j_2, \dots, j_k $$.

### Under (10.114)

$$ 2 $$ should be $$ p + 2 $$.

### (10.116)

$$ s_1 + s_2 + k + m_1 + m_2 $$ should be $$ s_1 + s_2 - k + m_1 + m_2 $$

### (10.117)

$$ \begin{bmatrix} 2 s_1 + 2 s_2 - 2 \\ s_1 + s_2 - 1 + m_1 + m_2 \end{bmatrix} $$ should be
$$ \begin{bmatrix} 2 s_1 + 2 s_2 - 2 k \\ s_1 + s_2 - k + m_1 + m_2 \end{bmatrix} $$.

### (10.118)

After the first equality, $$ \frac{(2 s_1)! (2 s_2)!}{k!} $$ should be $$ \frac{(2 s_1)! (2 s_2)!}{(2 s_1 - k)! (2 s_2 - k)! k!} $$.

### Under (10.118)

_it is nice example_ should be _it is a nice example_.

### Problem 10.D.b

(10.86) should be (10.94) and $$ \langle 3/2, 1, 3/2, 1/2 | 1, 3/2, 0, 1/2 \rangle $$ should be
$$ \langle 3/2, 1/2 | 1, 3/2, 0, 1/2 \rangle $$.

### Problem 10.E

$$ SU(3) $$ should be $$ SU(2) $$.

## Chapter 11

### (11.12)

_and $$ SU(3) $$_ should be _an $$ SU(3) $$_.

In case it's not obvious, the word *conserve* implies *commute with*.

### Text under (11.17)

For clarity, _the isospin and hypercharge generators_ are $$ T_1 $$, $$ T_2 $$, $$ T_3 $$, and $$ T_8 $$.

*The $$ I = Y = 0 $$ state corresponds to the generator $$ T_8 $$.*

This statement is not accurate. Both $$ T_3 $$ and $$ T_8 $$ have $$ I = Y = 0 $$. But $$ T_3 $$ does not commute with
$$ T_1 $$ or $$ T_2 $$ and is thus not what we are looking for.

### (11.19)

This leads to $$ [T_a, H_MS] = O^i_j [T_a, T_8]^j_i $$, guaranteeing that $$ H_MS .$$ commute with
$$ T_1 $$, $$ T_2 $$, $$ T_3 $$, and $$ T_8 $$.

### (11.25)

It may seem strange why $$ | \Sigma^+ |^2 $$, $$ | \Sigma^- |^2 $$ and $$ | \Sigma^0 |^2 $$ are combined into $$ | \Sigma |^2 $$,
$$ | n |^2 $$ and $$ | p |^2 $$ (or $$ | \Xi^0 |^2 $$ and $$ | \Xi^- |^2 $$) are combined into $$ | Ν |^2 $$ (or $$ | \Xi |^2 $$),
and $$ | \Lambda |^2 $$ reamins as $$ | \Lambda |^2 $$

The reason becomes clear when we look at the $$ M_0 $$ term from the very strong interactions.

$$ \langle B | H_{VS} | B \rangle = M_0 (| \Sigma |^2 + | Ν |^2 + | \Xi |^2+ | \Lambda |^2) $$, where these four items are defined
in the same way.

### Text under (11.33)

To see where the equal spacing is from, notice that $$ \Delta $$, $$ \Sigma $$, $$ \Xi $$ , $$ \Omega $$ particles have
components $$ \{ xxx \} $$, $$ \{ xx3 \} $$, $$ \{ x33 \} $$, and $$ 333 $$ respectively, where $$ x = 1, 2 $$. By expanding
(10.71), one sees clearly that the result can be written as $$ \lambda ( n_{1, 2} - 2 n_3 ) $$ where $$ n_{1, 2} $$ is
how many times 1 or 2 appears and $$ n_3 $$ is how many times 3 appears.

### Section 13.5

The notation $$ SU(N) \otimes SU(M) \otimes U(1) \in SU(N + M) $$ does not make much sense. It would be better to write
$$ SU(N) \times SU(M) \times U(1) \subset SU(N + M) $$. To be even more rigorous, one should write
$$ SU(N) \times SU(M) \times U(1) / Z_{\textrm{LCM}(N, N)} \subset SU(N + M) $$ where $$ \textrm{LCM}(N, N) $$ is the
least common multiple of $$ N $$ and $$ M $$.

The operation is called [direct product](https://en.wikipedia.org/wiki/Direct_product_of_groups)
for the group (or [direct sum](https://en.wikipedia.org/wiki/Lie_algebra#Direct_sum_and_semidirect_product) for the algebra).

The tensor product introduced in Section 1.15 and further elaborated in Section 3.4 is conceptually quite different. It is
a product between two representations of the *same* group.

## Chapter 14

### Text under (14.13)

The reason why the $$ n = 2 $$ states _under angular momentum transforms like $$ 5 + 1 $$_ is that the states in the defining
representation of $$ SU(3) $$ are linear combinations of the eigenstates of the spin-1 representation of $$ SU(2) $$ defined by
$$ L1 $$, $$ L2 $$, $$ L3 $$. In $$ SU(2) $$, we have $$ 3 \otimes 3 = 5 \oplus 3 \oplus 1 $$. The $$ 3 $$ on the right hand side
is antisymmetric in $$ SU(2) $$ and does not appear in the symmetric $$ 6 $$ in $$ SU(3) $$. (It appears in the $$ \bar{3} $$ in
$$ SU(3) $$ instead.)

With 3 indices it would be $$ 7 + 3 $$; 4 indices $$ 9 + 5 + 1 $$; etc.

### Text under (14.20)

This is essentially $$ 8 \otimes 1 \otimes 1 \otimes 1 \cdots $$, which is still an $$ 8 $$.

### Text under (14.21)

It should be $$ b^\dagger_2 $$ instead of $$ b^\dagger_3 $$ But it has not consequence in the following discussion.

## Chapter 15

### Section 15.2

It should be written as $$ SU(N) \times SU(M) \subset SU(N M) $$.

### (15.13)

We can also use (11.15) and write it by brute force as (without normalization)

$$ \left(dsu - sdu + sud - usd + uds - dus - 3(uds - dus) \right) (| +-+ \rangle - | -++ \rangle). $$

The result is the same as the three symmetric pairs in $$ \Lambda $$ vanish when they are multiplied with
the anitsymmetric spin terms and cyclic permutations are added.

### (15.14)

Again, we could also write by brute force

$$ (uud - udu)(|++-\rangle - |+-+\rangle), $$

giving us the same result when cyclic terms are added.

### (15.32)

$$ m $$ should be $$ m_p $$. Note it should be $$ m_p $$ instead of $$ m_\Lambda $$ (and also $$ e $$ on the left
hand side of (15.31) instead of 0, the electric charge of $$ \Lambda $$) as a matter of definition. 

## Chapter 16

### (16.9)

Keep in mind that what it really means is $$ T_a = T^A_a \otimes I + I \otimes T^B_a $$.

### Text under (16.10)

A more precise statement would be as follows.

*Eigenstates that do not share the same eigenvalue belong to different irreducible representations of color $$ SU(3) $$
because of Schur's lemma.*

## Chapter 17

### Discussion on $$ \Sigma - \Lambda $$ splitting under (17.2)<a name="Sigma-Lambda"></a>

_the spins must be aligned, because the state must also be symmetric in spin space_

This statement is incorrect. See Solution to 15.A. The conclusion that the $$ \Lambda $$ has lower energy is however correct,
because on average (from Solution to 15.A.), the factor for $$ \Sigma^0 $$ as in (17.1) is

$$ \frac{4}{6} (\frac{1}{m_{u,d} m_{u,d}} - 2 \frac{1}{m_{u,d} m_s}) + \frac{1}{6} (-\frac{1}{m_{u,d} m_{u,d}}) + \frac{1}{6} (-\frac{1}{m_{u,d} m_{u,d}}) = \frac{1}{3} \frac{1}{m_{u,d} m_{u,d}} - \frac{4}{3} \frac{1}{m_{u,d} m_s}, $$

while that for $$ \Lambda $$ is (from 15.13))

$$ -\frac{1}{m_{u,d} m_{u,d}}. $$

The former is clearly greater than the latter.

## Chapter 19

### (19.15)

Here is why $$ (\sigma_1 + i \sigma_2) \otimes S_{kl} $$ has $$ H_n = \sqrt\frac{2}{{n}} $$ and $$ H_m = [\nu^k + \nu^l]_m $$.
Note that $$ H_m $$ here is really $$ \begin{bmatrix} H_m & 0 \\ 0 & -H_m \end{bmatrix} $$.

We have

$$
\begin{eqnarray}
  &&  [H_n, (\sigma_1 + i \sigma_2) \otimes S_{kl}]  \\
  &=& \frac{1}{\sqrt{2n}} (\begin{bmatrix} 1 & 0 \\ 0 & -1 \end{bmatrix} \begin{bmatrix} 0 & S_{kl} \\ 0 & 0 \end{bmatrix} - \begin{bmatrix} 0 & S_{kl} \\ 0 & 0 \end{bmatrix} \begin{bmatrix} 1 & 0 \\ 0 & -1 \end{bmatrix})  \\
  &=& \sqrt{\frac{2}{n}} \begin{bmatrix} 0 & S_{kl} \\ 0 & 0 \end{bmatrix},
\end{eqnarray}
$$

and

$$
\begin{eqnarray}
  &&  [H_m, (\sigma_1 + i \sigma_2) \otimes S_{kl}]  \\
  &=& \begin{bmatrix} H_m & 0 \\ 0 & -H_m \end{bmatrix} \begin{bmatrix} 0 & S_{kl} \\ 0 & 0 \end{bmatrix} - \begin{bmatrix} 0 & S_{kl} \\ 0 & 0 \end{bmatrix} \begin{bmatrix} H_m & 0 \\ 0 & -H_m \end{bmatrix}  \\
  &=& \begin{bmatrix} 0 & H_m S_{kl} + S_{kl} H_m \\ 0 & 0 \end{bmatrix}
\end{eqnarray}
$$

$$ H_m S_{kl} $$ has only two non-zero elements $$ [H_m S_{kl}]_{kl} = [H_m]_{kk} = {\nu^k}_m $$ and $$ [H_m S_{kl}]_{lk} = [H_m]_{ll} = {\nu^l}_m $$, while
$$ S_{kl} H_m $$ also has only two non-zero elements $$ [S_{kl} H_m]_{kl} = [H_m]_{ll} = {\nu^l}_m $$ and $$ [S_{kl} H_m]_{lk} = [H_m]_{kk} = {\nu^k}_m $$.
Therefore $$ H_m S_{kl} + S_{kl} H_m = ({\nu^k}_m + {\nu^l}_m) S_{kl} = [{\nu^k} + {\nu^l}]_m S_{kl} $$ and we have

$$ [H_m, (\sigma_1 + i \sigma_2) \otimes S_{kl}] = [{\nu^k} + {\nu^l}]_m (\sigma_1 + i \sigma_2) \otimes S_{kl}. $$

It is similar for $$ (\sigma_1 - i \sigma_2) \otimes S_{kl} $$.

### Text under (19.24)

_The action of ($$ i $$ times) the $$ Sp(2N) $$ generators on the complex $$ 2N $$-vector defined by the first column of
(19.24) is equivalent to the action of (19.22) on (19.24)._

To make this statement true, we need to modify the $$ Sp(2N) $$ generators as in (19.11) to the following form.

$$ \begin{array}{} A \otimes 1 & -S_1 \otimes \sigma_1 & -S_2 \otimes \sigma_2 & -S_3 \otimes \sigma_3 \end{array} $$

Note that $$ A $$ purely imaginery while $$ S_1 $$, $$ S_2 $$, and $$ S_3 $$ are real.

Let $$ Q $$ be a quaternionic $$ N $$-vector with $$ Q^i = B^i + j_1 C^i_1 + j_2 C^i_2 + j_3 C^i_3 $$, and
$$ X $$ be an $$ Sp(2N) $$ generator with $$ i X = iA + j_1 S_1 + j_2 S_2 + j_2 S_3 $$. We have

$$
\begin{eqnarray}
  &&  [(i X) Q]^i  \\
  &=& [i X]^i_j Q^j  \\
  &=& \left( [i A]^i_j + j_1 [S_1]^i_j + j_2 [S_2]^i_j + j_3 [S_3]^i_j \right) \left( B^j + j_1 C^j_1 + j_2 C^j_2 + j_3 C^j_3 \right)  \\
  &=& \left( [i A]^i_j B^j - [S_1]^i_j C^j_1 - [S_2]^i_j C^j_2 - [S_3]^i_j C^j_3 \right) +  \\
  &&  j_1 \left( [i A]^i_j C^j_1 + [S_1]^i_j B^j + [S_2]^i_j C^j_3 - [S_3]^i_j C^j_2 \right) +  \\
  &&  j_2 \left( [i A]^i_j C^j_2 - [S_1]^i_j C^j_3 + [S_2]^i_j B^j + [S_3]^i_j C^j_1 \right) +  \\
  &&  j_3 \left( [i A]^i_j C^j_3 + [S_1]^i_j C^j_2 - [S_2]^i_j C^j_1 + [S_3]^i_j B^j \right).
\end{eqnarray}
$$

The corresponding action of ($$ i $$ times) the $$ Sp(2N) $$ generators, as defined above, on the complex
$$ 2N $$-vector defined by the first column of (19.24), denoted as $$ q $$ with $$ q^{i, 1} = B^i - i C^i_3 $$
and $$ q^{i, 2} = C^i_2 - i C^i_1 $$, is as follows.

$$
\begin{eqnarray}
  &&  [(i X) q]^{i, 1}  \\
  &=& [i X]^{i, 1}_{j, y} Q^{j, y}  \\
  &=& i \left( [A]^i_j I^1_y - [S_1]^i_j [\sigma_1]^1_y - [S_2]^i_j [\sigma_2]^1_y - [S_3]^i_j [\sigma_3]^1_y \right) q^{j, y}  \\
  &=& [i A]^i_j q^{j, 1} - i [S_1]^i_j q^{j, 2} - [S_2]^i_j q^{j, 2} - i [S_3]^i_j q^{j, 1}  \\
  &=& \left( [i A]^i_j B^j - [S_1]^i_j C^j_1 - [S_2]^i_j C^j_2 - [S_3]^i_j C^j_3 \right) +  \\
  &&  - i \left( [i A]^i_j C^j_3 + [S_1]^i_j C^j_2 - [S_2]^i_j C^j_1 + [S_3]^i_j B^j \right),
\end{eqnarray}
$$

and similarly

$$
\begin{eqnarray}
  &&  [(i X) q]^{i, 2}  \\
  &=& \left( [i A]^i_j C^j_2 - [S_1]^i_j C^j_3 + [S_2]^i_j B^j + [S_3]^i_j C^j_1 \right) +  \\
  &&  - i \left( [i A]^i_j C^j_1 + [S_1]^i_j B^j + [S_2]^i_j C^j_3 - [S_3]^i_j C^j_2 \right).
\end{eqnarray}
$$

Now the equivalence should be obvious.

## Chapter 20

### Discussion on *decomposable* and *semi-simple*

_is has no Abelian invariant subalgebra_ should be _it has no Abelian invariant subalgebra_.

### Discussion under (20.19)

_If we remove any vector from an extended $$ \Pi $$-system, the remaining vectors are linearly independent._

The reason why this is true is that all coefficients (as defined in (8.12)) of $$ - \alpha_0 $$, the highest root,
are strictly positive.

If a coefficient $$ k_i = 0 $$, we can show that $$ k_j = 0 $$ for all $$ \alpha_j $$ connected to $$ \alpha_i $$,
because a) $$ (\alpha_i) \cdot (- \alpha_0) \ge 0 $$ since $$ p_i $$ as in (6.36) must be 0 for $$ (- \alpha_0 $$ to
be the highest, and b) $$ \alpha_i \cdot \alpha_j \le 0 $$ with strict inequality if $$ \alpha_j $$ is connected to
$$ \alpha_i $$. Since the algebra is simple (i.e. all those $$ \alpha $$’s are connected), this would eventually lead to
$$ \alpha_0 = 0 $$.

Below are the coefficients of the highest root of the four families of the classical Lie algebras.
- A: $$ (1, 1, \dots, 1) $$,
- B: $$ (1, 2, 2, \dots, 2) $$,
- C: $$ (2, 2, \dots, 2, 1) $$,
- D: $$ (1, 2, 2, \dots, 2, 1, 1) $$.

In [SageMath](https://www.sagemath.org), on can use the following code to get the coefficients of the highest root
of an algebra (e.g., $$ F_4 $$ here).

```LieAlgebra(QQ, cartan_type=['F', 4]).highest_root_basis_elt()```

With these coefficients, we can easily verify the extended diagrams in (20.20).

Building the whole algebra *up* from the lowest root is essentially the same as building the whole adjoint
representation (i.e. the root set) *down* from the highest root. The removal of one of the simple roots will
thus lead to a subset of the root set of the original algebra.

## Chapter 21

### (21.6)

(21.5) also requires

$$ [H_j, E_{\eta e^k + \eta’ e^l}] = (\eta [e^k]_j + \eta' [e^l]_j) E_{\eta e^k + \eta' e^l} = (\eta \delta_{jk} + \eta' \delta_{jl}) E_{\eta e^k + \eta' e^l}, $$

which can be shown through a straightforward but tedious calculation using (21.3).

### (21.11)

It would be better to write the last equation as

$$ \sigma^i_a | x e^j \rangle = \delta_{ij} | x' e^j \rangle [\sigma_a]_{x' x}. $$

Also note that $$ x $$ and $$ x' $$ should be treated as labels/indices rather than numbers.

### (21.46)

As explained in the footnote, we have

$$ R = A + i B = O d O^T, $$

where $$ A $$ and $$ i B $$ are the real and imaginary parts of $$ R $$ and they are simultaneously diagonalized by
an orthogonal (and thus real) matrix $$ O $$.

From $$ R^* R = I $$. we have $$ O d^* O^T O d O^T = I \rightarrow d^* d = I $$. Since $$ d $$ is diagnoal, we have

$$ d = \mathrm{diag}(d_1, d_2, \dots, d_n), $$

where $$ d_1, d_2, \dots, d_n $$ are complex numbers of unit length. We now define

$$ \sqrt{d} = \mathrm{diag}(\sqrt{d_1}, \sqrt{d_2}, \dots, \sqrt{d_n}). $$

(We do not care which of the two roots of $$ d_1 $$, $$ d_2 $$, etc., we take.) Note that $$ \sqrt{d} $$ is also
unitary.

Now we have

$$ R = O \sqrt{d} \sqrt{d} O^T = (O \sqrt{d}) (O \sqrt{d})^T, $$

where $$ O \sqrt{d} $$ is unitary.

## Chapter 23

### Statement following (23.4)

The statement is correct except for $$ SO(3) $$ and $$ SO(4) $$, of which $$ D^1 $$ is not the defining representation.

### (23.5)

Note that this particular choice of $$ \Gamma $$ matrices, together with (23.2), give us exactly the same generator matrices
as in (21.30). This is important later when we combine $$ \Gamma $$s with $$ R $$, whose form was derived based on the
particular forms of (21.30).

### Paragraph following (23.6)

_These elements of the Clifford algebra are actually proportional to $$ M_{j,2n + 2} $$ in the spinor representation
that we constructed._

This can be verified by using (21.3), (22.14), (22.16) and (22.18).

_These generators transform like the components of a vector under the $$ SO(2n + 1) $$ subgroup of $$ SO(2n + 2) $$._

This is just a restatement of (23.3).

### Statement following (23.14)

A less philosophical explanation for the lack of a definite symmetry property is that the upper and lower indices
transform differently so any symmetry or antisymmetry would not be preserved any way when we change the basis.

### Paragraph following (23.17)

_These products must be either symmetric or antisymmetric..._

Each $$ \Gamma_l R $$, being the tensor product of a bunch of Pauli matrices, is certainly either symmetric or
antisymmetric. (See (23.5) and (21.55)).

By choosing $$ j = l $$ in (23.17) we can see that $$ \Gamma_l R $$ and $$ \Gamma _k R $$ must share the same symmetry property.

### Paragraph following (23.31)

The notation of $$ D^1 $$, $$ D^2 $$, $$ D^3 $$, etc. for $$ SO(3) $$, $$ SO(5) $$, $$ S(7) $$, etc. is wrong.
See Page 122 for the definition of $$ D^j $$.

For $$ SO(3) $$, the vector tensor is 3-dimensional, while $$ D^1 $$, which is the spinor representation,
is 2-dimensional.

For $$ SO(5) $$, the antisymmetric tensor with 2 vector indices are 10-dimensional, while $$ D^2 $$ is 4-dimensional.

In fact, the antisymmetric tensor with $$ n $$ vector indices of $$ SO(2n + 1) $$ is $$ 2 D^n $$.

### Paragraph following (23.42)

_The symmetry factor for $$ (k) $$ is $$ (-1)^{(n - k)(n - k - 1)/2} $$_

It should be $$ (-1)^{(n - k)(n - k + 1)/2} $$ in general, although for $$ n = 5 $$ it does not matter.

Note the reasoning starts from (23.26), rather than (23.34).

We are treating $$ SO(2n) $$ as a subalgebra of $$ SU(2n + 1) $$, rather than a superalgebra of $$ SO(2n - 1) $$.
This is the opposite approach to that in Chapter 22.

When we treat $$ SO(2n) $$ as a subalgebra of $$ SO(2n + 1) $$, $$ D^n $$ of $$ SO(2n + 1) $$ breaks down into
$$ D^n \oplus D^{n - 1} $$ of $$ SO(2n) $$. The symmetry factor of $$ D^n \otimes D^n $$, and of $$ D^{n - 1} \otimes D^{n - 1} $$
in $$ SO(2n) $$, must be by definition the same as that of $$ D^n \otimes D^n $$ in $$ SO(2n + 1) $$.

To reach the same conclusion starting from (23.34), we would have to find a unitary matrix and put it to the right to
$$ P_+ K P_- $$ to change the basis of $$ \bar{D}^{n - 1} $$ to the basis of $$ D^n $$.

## Chapter 24

### (24.13) and (24.14)

It may be helpful to review Problems 12.A. and 18.C. to see why it is so.

### (24.19)

It should be $$ \begin{bmatrix} A_2 I_2 + S_2 \sigma_2 & 0 \\ 0 & A_3 I_2 + S_3 \sigma_2 \end{bmatrix} $$.

## Chapter 26

### Statement following (26.5)

$$ e^j $$ should be $$ \nu^j$$.

### (26.17)

Either $$ T^* $$ should be replaced by $$ T $$, or $$ \alpha' $$ and $$ \beta' $$ should be switched.

### Discussion following (26.20)

$$ 2 \nu^1 $$ should be $$ 2 \mu^1 $$.

# Solutions

## Chapter 1

### 1.A.

Trivial. Simply use the fact that every element appears once and only once in every row and column of the multiplication table.

### 1.B.

The only two possibilities are given below, corresponding to $$ Z_4 $$ and $$ Z_2 \times Z_2 $$ respectively.

$$
\begin{array}
\hline
 e & a & b & c  \\ \hline
 a & b & c & e  \\ \hline
 b & c & e & a  \\ \hline
 c & e & a & b  \\ \hline
\end{array}
$$

$$
\begin{array}
\hline
 e & a & b & c  \\ \hline
 a & e & c & b  \\ \hline
 b & c & e & a  \\ \hline
 c & b & a & e  \\ \hline
\end{array}
$$

### 1.C.

If the representation (1.135) of the permutation group is irreducible, (1.79) would apply.

### 1.D.

Applying Schur's lemma, we have $$ S^{-1} A = \lambda I $$, or $$ A = \lambda S $$.

### 1.E.

It is essentially the group $$ A_4 $$, made of permutations with an even number of 2-cycles. There are in total $$ 4! / 2 = 12 $$ elements.

There are four conjugacy classes.
- {e},
- {(12)(34), (13)(24), (14)(23)},
- {(123), (243), (134), (142)},
- {(132), (234), (143), (124).

From (1.74), we know the four irreducible representations must be of 1-, 1-, 1-. and 3-dimensional. By applying (1.79) and (1.87), we get the
following character table.

$$
\begin{array}
\hline
 \text{} & e & (12)(34) & (123)    & (321)     \\ \hline
 D_1     & 1 & 1        & 1        & 1         \\ \hline
 D_{1'}  & 1 & 1        & \omega   & \omega^2  \\ \hline
 D_{1''} & 1 & 1        & \omega^2 & \omega    \\ \hline
 D_3     & 3 & -1       & 0        & 0         \\ \hline
\end{array}
$$

### 1.F.

The group is a subset of $$ S_4 $$, with the following conjugacy classes.
- {e},
- {(13)(24)},
- {(13), (24)},
- {(12)(34), (14)(23)},
- {(1234), (4321)}.

Below is the character table.

$$
\begin{array}
\hline
 \text{}    & e & (13)(24) & (13)     & (12)(34) & (1234)    \\ \hline
 D_1        & 1 & 1        & 1        & 1        & 1         \\ \hline
 D_{1'}     & 1 & 1        & 1        & -1       & -1        \\ \hline
 D_{1''}    & 1 & 1        & -1       & 1        & -1        \\ \hline
 D_{1'''}   & 1 & 1        & -1       & -1       & 1         \\ \hline
 D_2        & 2 & -2       & 0        & 0        & 0         \\ \hline
\end{array}
$$

The $$ D_2 $$ representation is as follows.
- $$ \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} $$,
- $$ \begin{bmatrix} -1 & 0 \\ 0 & -1 \end{bmatrix} $$,
- $$ \begin{bmatrix} 0 & -1 \\ -1 & 0 \end{bmatrix} $$, $$ \begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix} $$,
- $$ \begin{bmatrix} -1 & 0 \\ 0 & 1 \end{bmatrix} $$, $$ \begin{bmatrix} 1 & 0 \\ 0 & -1 \end{bmatrix} $$,
- $$ \begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix} $$, $$ \begin{bmatrix} 0 & 1 \\ -1 & 0 \end{bmatrix} $$.

The representation that describes the system is $$ D_4 \otimes D_2 $$, where $$ D_4 $$ is the defining representation. Its character table
is as follows.

$$
\begin{array}
\hline
 \text{} & e & (13)(24) & (13)     & (12)(34) & (1234)    \\ \hline
 D_8     & 8 & 0        & 0        & 0        & 0         \\ \hline
\end{array}
$$

Applying (1.88), we see that in $$ D_8 $$, $$ D_1 $$, $$ D_{1'} $$, $$ D_{1''} $$, and $$ D_{1'''} $$ each appears once, while $$ D_2 $$ appears twice.

The projection onto $$ D_1 $$ is

$$
\begin{eqnarray}
  P_1 &=& \frac{1}{8} \sum_{g \in G} \chi_1 (g)^* D_8(g)    \\
  &=& \frac{1}{8} \begin{bmatrix} 1 & 1 & -1 & 1 & -1 & -1 & 1 & -1 \end{bmatrix} ^T \begin{bmatrix} 1 & 1 & -1 & 1 & -1 & -1 & 1 & -1 \end{bmatrix},
\end{eqnarray}
$$

the so-called "breathing mode".

The projection onto $$ D_{1'} $$ is

$$
\begin{eqnarray}
  P_{1'} &=& \frac{1}{8} \sum_{g \in G} \chi_{1'} (g)^* D_8(g)    \\
  &=& \frac{1}{8} \begin{bmatrix} 1 & 1 & 1 & -1 & -1 & -1 & -1 & 1 \end{bmatrix} ^T \begin{bmatrix} 1 & 1 & 1 & -1 & -1 & -1 & -1 & 1 \end{bmatrix},
\end{eqnarray}
$$

where 1 and 3 are pulled away from each other while 2 and 4 are pushed towards each other (or the opposite).

The projection onto $$ D_{1''} $$ is

$$
\begin{eqnarray}
  P_{1''} &=& \frac{1}{8} \sum_{g \in G} \chi_{1''} (g)^* D_8(g)    \\
  &=& \frac{1}{8} \begin{bmatrix} 1 & -1 & -1 & -1 & -1 & 1 & 1 & 1 \end{bmatrix} ^T \begin{bmatrix} 1 & -1 & -1 & -1 & -1 & 1 & 1 & 1 \end{bmatrix},
\end{eqnarray}
$$

where the four points are pulled away from each other horizontally but pushed towards each other vertically (or the opposite).

The projection onto $$ D_{1'''} $$ is

$$
\begin{eqnarray}
  P_{1'''} &=& \frac{1}{8} \sum_{g \in G} \chi_{1'''} (g)^* D_8(g)    \\
  &=& \frac{1}{8} \begin{bmatrix} 1 & -1 & 1 & 1 & -1 & 1 & -1 & -1 \end{bmatrix} ^T \begin{bmatrix} 1 & -1 & 1 & 1 & -1 & 1 & -1 & -1 \end{bmatrix},
\end{eqnarray}
$$

which describes a rotation around the center.

The projection onto $$ D_2 $$ is

$$
\begin{eqnarray}
  P_2 &=& \frac{1}{8} \sum_{g \in G} \chi_2
   (g)^* D_8(g)    \\
  &=& \frac{1}{4} \begin{bmatrix} 1 & 1 \\ 1 & 1 \end{bmatrix} \otimes I_4,
\end{eqnarray}
$$

where $$ I_4 $$ is the four-dimensional identity matrix. It is equal to the sum of two matrices similar to (1.122) and (1.123), which describe
translations in the $$ x $$ direction and translations in the $$ y $$ direction.

## Chapter 2

### 2.A.

Taylor expanding $$ e^{i \alpha A} $$ and noticing that $$ A^n = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 0 & 0 \\ 0 & 0 & 1 \end{bmatrix} $$
with $$ n = 2, 4, \dots $$ while $$ A^n = A $$ with $$ n = 1, 3, \dots $$, we have

$$
\begin{eqnarray}
  e^{i \alpha A} &=& I - A^2 + A^2 + i \alpha A - \frac{\alpha^2 A^2}{2!} - \frac{i \alpha^2 A^3}{3!} + \cdots    \\
  &=& I - A^2 + A^2 \cos \alpha + i A \sin \alpha    \\
  &=& \begin{bmatrix} \cos \alpha & 0 & i \sin \alpha \\ 0 & 1 & 0 \\ i \sin \alpha & 0 & \cos \alpha \end{bmatrix}.
\end{eqnarray}
$$

### 2.B.

Applying (2.44), we have

$$
\begin{eqnarray}
  &&  e^{i \alpha A} B e^{-i \alpha A}    \\
  &=& B + i \alpha [A, B] - \frac{\alpha^2}{2} [A, [A, B]] - \frac{i \alpha^3}{3!} [A, [A, [A, B]]] + \cdots    \\
  &=& B + i \alpha B - \frac{\alpha^2}{2} B - \frac{i \alpha^3}{3!} B + \cdots    \\
  &=& B e^{i \alpha}.
\end{eqnarray}
$$

### 2.C.

To save some space, let's notate $$ \alpha_a X_a $$ as $$ A $$ and $$ \beta_a X_a $$ as $$ B $$.

We have

$$
\begin{eqnarray}
  K &=& e^{i A} e^{i B} - 1    \\
  &=& (1 + i A - \frac{1}{2} A^2 - \frac{i}{3!} A^3 + \cdots)    \\
  &&  (1 + i B - \frac{1}{2} B^2 - \frac{i}{3!} B^3 + \cdots) -1    \\
  &=& i A + i B - A B - \frac{1}{2} A^2 - \frac{1}{2} B^2    \\
  &&  - \frac{i}{3!} A^3 - \frac{i}{3!} B^3 - \frac{i}{2} A A B - \frac{i}{2} A B B + \cdots.    \\
\end{eqnarray}
$$

This gives

$$
\begin{eqnarray}
  i \delta_a X_a &=& K - \frac{1}{2} K^2 + \frac{1}{3} K^3    \\
  &=& i A + i B - A B - \frac{1}{2} A^2 - \frac{1}{2} B^2    \\
  &&  - \frac{i}{3!} A^3 - \frac{i}{3!} B^3 - \frac{i}{2} A A B - \frac{i}{2} A B B    \\
  &&  + \frac{1}{2}(A + B)^2 + \frac{i}{2} A A B + \frac{i}{2} A B A + \frac{i}{2} B A B + \frac{i}{2} A B B    \\
  &&  + \frac{i}{2} A^3 + \frac{i}{2} B^3 + \frac{i}{4} A B B + \frac{i}{4} B A A + \frac{i}{4} A A B + \frac{i}{4} B B A     \\
  &&  - \frac{i}{3} (A + B)^3 + \cdots     \\
  &=& i A + i B - \frac{1}{2} [A, B] + \frac{i}{12} [[A, B], A - B] + \cdots     \\
  &=& i X_a \left( \alpha_a + \beta_a - \frac{1}{2} \alpha_r \beta_s f_{rsa} - \frac{1}{12} \alpha_r \beta_s (\alpha_m - \beta_m) f_{rst} f_{tma} \right) + \cdots.     \\
\end{eqnarray}
$$

We can now write

$$ \delta_a = \alpha_a + \beta_a - \frac{1}{2} \alpha_r \beta_s f_{rsa} - \frac{1}{12} \alpha_r \beta_s (\alpha_m - \beta_m) f_{rst} f_{tma} + \cdots. $$

## Chapter 3

### 3.A.

We start with the highest weight $$ | j + s | j + s \rangle = |j, j \rangle |s, s \rangle $$ and repeatedly apply $$ J^- $$ on both
sides until they vanish. We get the spin $$ j + s $$ representation, with in total $$ 2(j + s) + 1 $$ states.

We will notice that $$ | j + s | j + s - 1 \rangle $$ is a linear combination of
$$ |j, j - 1 \rangle |s, s \rangle $$ and $$ |j, j \rangle |s, s - 1 \rangle $$, which spans a 2-dimensional space. These two vectors
can form another linear combination that is orthogonal to $$ | j + s | j + s - 1 \rangle $$, and this second linear combination can
only be $$ | j + s - 1| j + s - 1 \rangle $$. (To verify this claim, apply $$ J^+ $$ and see it vanish.) Now we can again repeatedly
apply $$ J^- $$ on both sides until they vanish. In doing so, we get the spin $$ j + s - 1 $$ representation, with in total
$$ 2(j + s) - 1 $$ states.

Now we do exactly the same with $$ | j + s - 1 | j + s - 2 \rangle $$, which is a linear combination of three vectors, which spans a
3-dimensional space. These three vectors can form a third linear combination that is orthogonal to both $$ | j + s | j + s - 2 \rangle $$
and $$ | j + s - 1 | j + s - 2 \rangle $$. This third linear combination can only be $$ | j + s - 2 | j + s - 2 \rangle $$...

The whole process stops at the spin $$ | j - s | $$ representation.

As a sanity check, both sides of the equation have $$ (2 j + 1) (2 s + 1) $$ linearly independent states. So everything works out.

### 3.B.

It is easy to show that there is a similarity transformation $$ S $$ such that

$$ S^{-1} (\hat{r} \cdot \sigma) S = \sigma_3. $$

We have

$$
\begin{eqnarray}
  && e^{i \vec{r} \cdot \vec{\sigma}}    \\
  &=& S e^{i |\vec{r}| \sigma_3} S^{-1}    \\
  &=& S \begin{bmatrix} e^{i |\vec{r}|} & 0 \\ 0 & e^{-i |\vec{r}|} \end{bmatrix} S^{-1}    \\
  &=& \cos |\vec{r}| \cdot I + i \sin |\vec{r}| \cdot (\hat{r} \cdot \sigma)
\end{eqnarray}
$$

### 3.C.

Let $$ S = \begin{bmatrix} \frac{1}{\sqrt{2}} & 0 & -\frac{1}{\sqrt{2}} \\ \frac{i}{\sqrt{2}} & 0 & \frac{i}{\sqrt{2}} \\ 0 & -1 & 0 \end{bmatrix} $$.

It is easy to verify that $$ J_i^1 = S^{-1} T_i S $$, where $$ J_i^1 $$ and $$ T_i $$ are the spin-1 representation and the adjoint
representation of the $$ i $$-th generator.

### 3.D.

$$
\begin{bmatrix}
  0 & 0 & 0 & -i \\
  0 & 0 & -i & 0 \\
  0 & i & 0 & 0 \\
  i & 0 & 0 & 0
\end{bmatrix}
$$.

### 3.E.

(a) $$ [\sigma_a, \sigma_b] \eta_c $$

(b) $$ \mathrm{Tr}(\sigma_a \cdot 2 \delta_{bd} I \sigma_c) = 4 \delta_{bd} \mathrm{Tr}(\delta_{ac} I) = 8 \delta_{ac} \delta_{bd} $$

(c) $$ \sigma_1 \sigma_2 \eta_1 \eta_2 - \sigma_2 \sigma_1 \eta_2 \eta_1 = 0 $$

## Chapter 4

### 4.A.

Simply check the [Table of Clebsch-Gordan coefficients](https://en.wikipedia.org/wiki/Table_of_Clebsch%E2%80%93Gordan_coefficients).
We see

$$ A = \langle 3/2, -1/2 | 1/2, 1, 1/2, -1 \rangle k_{\alpha \beta} = \sqrt{\frac{1}{3}} k_{\alpha \beta}. $$

What we want is

$$ \langle 3/2, -3/2 | 1/2, 1, -1/2, -1 \rangle k_{\alpha \beta} = k_{\alpha \beta} = \sqrt{3} A. $$

### 4.B.

We could repeatedly apply $$ L- $$ to both sides of $$ O_{+2} = r_{+1} r_{+1} $$. Or we could again check the
[Table of Clebsch-Gordan coefficients](https://en.wikipedia.org/wiki/Table_of_Clebsch%E2%80%93Gordan_coefficients).

Either way, we get the following.

$$
\begin{eqnarray}
  O_{+2} &=& r_{+1} r_{+1}  \\
  O_{+1} &=& \sqrt{\frac{1}{2}} r_{+1} r_{0} + \sqrt{\frac{1}{2}} r_{0} r_{+1}  \\
  O_{0}  &=& \sqrt{\frac{1}{6}} r_{+1} r_{-1} + \sqrt{\frac{2}{3}} r_{0} r_{0} + \sqrt{\frac{1}{6}} r_{-1} r_{+1}  \\
  O_{-1} &=& \sqrt{\frac{1}{2}} r_{-1} r_{0} + \sqrt{\frac{1}{2}} r_{0} r_{-1}  \\
  O_{-2} &=& r_{-1} r_{-1}
\end{eqnarray}
$$

Note the following relationships.

$$
\begin{eqnarray}
  r_{+1} &=& -(r_1 + i r_2) = -\sqrt{\frac{1}{2}} \sin \theta e^{i \phi}  \\
  r_{0}  &=& r_3 = \cos \theta  \\
  r_{-1} &=& (r_1 - i r_2) = \sqrt{\frac{1}{2}} \sin \theta e^{-i \phi}
\end{eqnarray}
$$

Compare these to the [Table of spherical harmonics](https://en.wikipedia.org/wiki/Table_of_spherical_harmonics)
with $$ l = 2 $$, we see they are identical up to a constant factor.

To generalize this construction to arbitrary $$ l $$, simply notice that

$$ [L^+, (r_{+1})^l] = 0. $$

The operator $$ (r_{+1})^l $$ is therefore the $$ O_{+l} $$ component of a spin $$ l $$ tensor operator. The rest is to
repeat the previous procedure.

### 4.C.

Note the similarity between this problem and 2.A. and 3.B.

We have

$$
\begin{eqnarray}
  &&  e^{i \alpha \hat{\alpha}_a X_a^1}    \\
  &=& I + i \alpha \hat{\alpha}_a X_a^1 - \frac{\alpha^2}{2!} (\hat{\alpha}_a X_a^1)^2 - \frac{i \alpha^3}{3!} \hat{\alpha}_a X_a^1 + \cdots   \\
  &=& I - (\hat{\alpha}_a X_a^1)^2 + i (\alpha - \frac{\alpha^3}{3!} + \cdots) \hat{\alpha}_a X_a^1 + (1 - \frac{\alpha^2}{2!} + \cdots) (\hat{\alpha}_a X_a^1)^2   \\
  &=& I - (\hat{\alpha}_a X_a^1)^2 + i \sin \alpha \hat{\alpha}_a X_a^1 + \cos \alpha (\hat{\alpha}_a X_a^1)^2.
\end{eqnarray}
$$

## Chapter 5

### 5.A.

Since pions are bosons, the full wave function (i.e. a wave function including position, spin, isospin, and whatever else) must be
symmetric. Since they are in an s-wave state, the wave function is symmetrical in the exchange of the position variables. Since pions
have spin 0, the wave function must also be symmetrical  in the exchange of isospin.

This means the total isospin has to be 2 (with 5 eigenstates) or 0 (with 1 eigenstate).

### 5.B.

It is not very clear whether it is asked to prove something similar to (5.17), or $$ [T_a, T_b] = \epsilon_{abc} T_c $$.

I assume it is the latter. (The former case is actual easier any way.)

We start with

$$
\begin{eqnarray}
  &&  [T_a, T_b]  \\
  &=& \sum_{x, m, m', \alpha} \sum_{y, n, n', \beta} [J^{j_x}_a]_{m m'} [J^{j_y}_b]_{n n'} [a^\dagger_{x, m, \alpha} a_{x, m', \alpha}, a^\dagger_{y, n, \beta} a_{y, n', \beta}].
\end{eqnarray}
$$

The commutator can be calculated as follows. (We take the commutator and plus sign if at least one of the two parts are bosons,
the anti-commutator and the plus sign if both are fermions.)

$$
\begin{eqnarray}
  &&  [a^\dagger_{x, m, \alpha} a_{x, m', \alpha}, a^\dagger_{y, n, \beta} a_{y, n', \beta}]  \\
  &=& a^\dagger_{x, m, \alpha} [a_{x, m', \alpha}, a^\dagger_{y, n, \beta} a_{y, n', \beta}] + [a^\dagger_{x, m, \alpha}, a^\dagger_{y, n, \beta} a_{y, n', \beta}] a_{x, m', \alpha}  \\
  &=& a^\dagger_{x, m, \alpha} [a_{x, m', \alpha}, a^\dagger_{y, n, \beta}]_\mp a_{y, n', \beta} \pm a^\dagger_{x, m, \alpha} a^\dagger_{y, n, \beta} [a_{x, m', \alpha}, a_{y, n', \beta}]_\mp  \\
  &&  \pm a^\dagger_{y, n, \beta} [a^\dagger_{x, m, \alpha}, a_{y, n', \beta}]_\mp a_{x, m', \alpha} + [a^\dagger_{x, m, \alpha}, a^\dagger_{y, n, \beta}]_\mp a_{y, n', \beta} a_{x, m', \alpha}  \\
  &=& \delta_{x y} \delta_{\alpha \beta} (\delta_{m' n} a^\dagger_{x, m, \alpha} a_{y, n', \beta} - \delta_{m n'} a^\dagger_{y, n, \beta} a_{x, m', \alpha}).
\end{eqnarray}
$$

We have therefore

$$
\begin{eqnarray}
  &&  [T_a, T_b]  \\
  &=& \sum_{x, m, n', \alpha} [J^{j_x}_a]_{m n} [J^{j_x}_b]_{n n'} a^\dagger_{x, m, \alpha} a_{x, n', \alpha} - [J^{j_x}_b]_{n m} [J^{j_x}_a]_{m m'} a^\dagger_{x, n, \alpha} a_{x, m', \alpha}  \\
  &=& \sum_{x, m, n', \alpha} [J^{j_x}_a, J^{j_x}_b]_{m n'} a^\dagger_{x, m, \alpha} a_{x, n', \alpha}  \\
  &=& \sum_{x, m, n', \alpha} \epsilon_{abc} [J^{j_x}_c]_{m n'} a^\dagger_{x, m, \alpha} a_{x, n', \alpha}  \\
  &=& \epsilon_{abc} T_c.
\end{eqnarray}
$$

5.C.

According to standard perturbation theory, which can be found in any QM book, the probability for a transition from $$ | i \rangle $$
to  $$ | j \rangle $$ to happen is proportional to $$ |\langle j | H | i \rangle|^2 $$.

We can assume $$ H $$ to be isospin invariant. Therefore the probability of $$ \pi^+ P \rightarrow \Delta^{++} $$ is proportional to
$$ |\langle 3/2, 3/2 | 1, 1/2, 1, 1/2 \rangle|^2 = 1 $$, while the probability of $$ \pi^- P \rightarrow \Delta^0 $$ is proportional to
$$ |\langle 3/2, -1/2 | 1, 1/2, -1, 1/2 \rangle|^2 = \frac{1}{3} $$. The ratio of the two is thus 3 to 1.

## Chapter 6

### 6.A.

In the adjoint representation,

$$
\begin{eqnarray}
  &&  H_i | [E_\alpha, E_\beta] \rangle  \\
  &=& [H_i, [E_\alpha, E_\beta]]  \\
  &=& [E_\alpha, [H_i, E_\beta]] + [[H_i, E_\alpha], E_\beta]  \\
  &=& \beta_i [E_\alpha, E_\beta] + \alpha_i [E_\alpha, E_\beta]  \\
  &=& (\alpha + \beta)_i [E_\alpha, E_\beta].
\end{eqnarray}
$$

If $$ \alpha + \beta $$ vanishes, $$ [E_\alpha, E_\beta] $$ is a Cartan generator. If it is a root,
$$ [E_\alpha, E_\beta] $$ must be proportional to $$ E_{\alpha + \beta} $$ due to the uniqueness of non-zero roots.
Otherwise, $$ [E_\alpha, E_\beta] $$ must be 0.

### 6.B.

From 6.A., $$ [E_\alpha, E_{-\alpha - \beta}] $$ is proportional to $$ E_{-\beta} $$. We can calculate the coefficient
as follows.

$$
\begin{eqnarray}
  &&  \langle E_{-\beta} | [E_\alpha, E_{-\alpha - \beta}] \rangle  \\
  &=& \lambda^{-1} \mathrm{Tr} (E^\dagger_{-\beta} [E_\alpha, E_{-\alpha - \beta}])  \\
  &=& \lambda^{-1} \mathrm{Tr} (E_{\beta} [E_\alpha, E_{-\alpha - \beta}])  \\
  &=& \lambda^{-1} \mathrm{Tr} ([E_{\beta}, E_\alpha], E_{-\alpha - \beta})  \\
  &=& -N \lambda^{-1} \mathrm{Tr} (E_{\alpha + \beta}, E^\dagger_{\alpha + \beta})  \\
  &=& -N  \\
\end{eqnarray}
$$

### 6.C.

Since $$ \sigma_3 $$ and $$ \sigma_3 \tau_3 $$ are already diagnoal, we can simply read off the four weights from these two
matrices. They are $$ (\pm 1, \pm 1) $$.

To get the roots, the foolproof method is to write down the two 10-dimensional matrices of $$ H_1 $$ and $$ H_2 $$ in the
adjoint representation and diagonalize them. In practice, the weights of the four-dimensional representation give us an
important hint. The difference between two *adjacient* weights is *often* a root. We know there are two zero roots
corresponding to $$ H_1 $$ and $$ H_2 $$ themselves. The eight remaining are thus $$ (\pm 2, 0), (0, \pm 2), (\pm 2, \pm 2) $$.

## Chapter 7

### 7.A.

We have $$ [T_1, T_4] = i \frac{1}{2} T_7 $$ and $$ [T_4, T_5] = i \frac{1}{2} T_3 + i \frac{\sqrt{3}}{2} T_8 $$, and
therefore $$ f_{147} = \frac{1}{2} $$ and $$ f_{456} = 0 $$.

### 7.B.

$$ T_1 $$, $$ T_2 $$ and $$ T_3 $$ generate an SU(2) subalgebra because they have the right commutation relationship.

Under this subalgebra, the first two eigenvectors (corresponding to the top weights in (7.11)) of the defining representation
form a doublet (spin 1) while the third eigenvectos (corresponding to the bottom weight in (7.11)) form a singlet (spin 1/2).

The adjoint representation breaks down into the following SU(2) representations.
- Spin 1/2: $$ E_{1/2, \sqrt{3}/2} $$ and $$ E_{-1/2, \sqrt{3}/2} $$ form a doublet.
- Spin 1/2: $$ E_{1/2, -\sqrt{3}/2} $$ and $$ E_{-1/2, -\sqrt{3}/2} $$ form a doublet.
- Spin 1: $$ E_{1, 0} $$, $$ T_{3} $$, and $$ E_{-1, 0} $$ form a triplet.
- Spin 0: $$ T_{0} $$ forms a singlet.

Remember that the generators are the states in the adjoint representation.

### 7.C.

Defining $$ J_1 $$, $$ J_2 $$, $$ J_3 $$ as $$ \lambda_2 $$, $$ \lambda_5 $$, and $$ \lambda_7 $$, we get
the commutation relationship of SU(2).

The eigenvectors of $$ J_3 $$ are $$ (0, 1, i)^T $$, $$ (- \sqrt{2}i, 0, 0)^T $$, and $$ (0, 1, -i)^T $$ with eigenvalues
1, 0, and -1. Applying $$ J^\pm = \frac{1}{\sqrt{2}} (J_1 \pm J_2) $$, we see that they form a triplet of the spin 1
representation of SU(2)

The adjoint representation breaks down into the following SU(2) representations.
- Spin 1: $$ J^+ $$, $$ -J_3 $$, and $$ -J^- $$ form a triplet.
- Spin 2: $$ \lambda_3 - 2i \lambda_6 - \sqrt{3} \lambda_8 $$, $$ 2i \lambda_1 - 2 \lambda_4 $$,
  $$ \sqrt{6} \lambda_3 + \sqrt{2} \lambda_8 $$, $$ 2i \lambda_1 + 2 \lambda_4 $$, and
  $$ \lambda_3 + 2i \lambda_6 - \sqrt{3} \lambda_8 $$ form a quintuplet, with their $$ J_3 $$ value equal to
  2, 1, 0, -1, and 2 respectively.

Remember that the generators are the states in the adjoint representation.

The foolproof way to get the correct result is to write down the matrix of $$ \lambda_7 $$ in the adjoint representation
and diagonalize it.

## Chapter 8

### 8.A.

The simple roots are $$ \alpha_1 = (0, 2) $$ and $$ \alpha_2 = (2, -2) $$. There should be two lines between the two dots
in the Dynkin diagram since $$ \cos \theta_{\alpha_1 \alpha_2} = 135^\circ $$.

The fundamental weights are $$ \mu_1 = (1, 1) $$ and $$ \mu_2 = (2, 0) $$.

### 8.B.

We can take $$ H_1 = \sigma_3 (1 + \eta_1) $$ and $$ H_2 = \sigma_3 (1 - \eta_1) $$ as the two Cartan generators.
Either diagonalizing them in the adjoint representation, or by a bit trials and errors, we can find the following
roots.

$$
\begin{eqnarray}
  E_{1, 0}    &=& \sigma^+ (1 + \eta_1) \\
  E_{0, -1}   &=& \sigma^+ (1 - \eta_1) \\
  E_{0, 1}   &=& \sigma^- (1 - \eta_1) \\
  E_{-1, 0}  &=& \sigma^- (1 + \eta_1)
\end{eqnarray}
$$

The algebra can be split into two mutually commuting subalgebras. The first consists of $$ H_1 $$, $$ E_{1, 0} $$,
and $$ E_{-1, 0} $$, while the second consists of the rest. Therefore the algebra is not simple. We cannot find
a generator that commutes with everything — such a generator would be a zero matrix in the adjoint representation
and we know all the six generators are linearly independent — and therefore the algebra is semisimple.

The Dynkin diagram is made of two isolated dots corresponding to $$ E_{1, 0} $$ and $$ E_{0, 1} $$.

### 8.C.

Below is the Cartan matrix.

$$
\begin{bmatrix}
  2   & -1  & 0   \\
  -1  & 2   & -2  \\
  0   & -1  & 2
\end{bmatrix}
$$

I won't draw the diagram here to get all positive roots. Instead, I will simply list all the positive roots below.

- $$ k = 1 $$:  $$ \begin{bmatrix} 2 & -1 & 0 \end{bmatrix} $$, $$ \begin{bmatrix} -1 & 2 & -2 \end{bmatrix} $$, $$ \begin{bmatrix} 0 & -1 & 2 \end{bmatrix} $$.
- $$ k = 2 $$:  $$ \begin{bmatrix} 1 & 1 & -2 \end{bmatrix} $$, $$ \begin{bmatrix} -1 & 1 & 0 \end{bmatrix} $$.
- $$ k = 3 $$:  $$ \begin{bmatrix} 1 & 0 & 0 \end{bmatrix} $$, $$ \begin{bmatrix} -1 & 0 & 2 \end{bmatrix} $$.
- $$ k = 4 $$:  $$ \begin{bmatrix} 1 & -1 & 2 \end{bmatrix} $$.
- $$ k = 5 $$:  $$ \begin{bmatrix} 0 & 1 & 0 \end{bmatrix} $$.

## Chapter 9

### 9.A.

The general idea is to use commutation rules (6.12), (6.19), and (8.4) to move $$ E_{-\alpha^i} $$s to the left and $$ E_{\alpha^i} $$
to the right. Also note that $$ \mu = \mu_1 + \mu_2 = \alpha_1 + \alpha_2 $$. 

$$
\begin{eqnarray}
  &&  \langle A | A \rangle \\
  &=& \langle \mu | E_{\alpha^2} E_{\alpha^1} | E_{-\alpha^1} E_{-\alpha^2} | \mu \rangle  \\
  &=& (\alpha^1)_i \langle \mu | E_{\alpha^2} H_i E_{-\alpha^2} | \mu \rangle + \langle \mu | E_{\alpha^2} E_{-\alpha^1} E_{\alpha^1} E_{-\alpha^2} | \mu \rangle  \\
  &=& (\alpha^1)_i \langle \mu | E_{\alpha^2} H_i E_{-\alpha^2} | \mu \rangle  \\
  &=& (\alpha^1)_i \left( (-\alpha^2)_i \langle \mu | E_{\alpha^2} E_{-\alpha^2} | \mu \rangle + \langle \mu | E_{\alpha^2} E_{-\alpha^2} H_i | \mu \rangle \right)  \\
  &=& \left( \alpha^1 \cdot (\mu - \alpha^2) \right) \langle \mu | E_{\alpha^2} E_{-\alpha^2} | \mu \rangle  \\
  &=& | \alpha^1 |^2 \left( (\alpha^2)_i \langle \mu | H_i | \mu \rangle + \langle \mu | E_{-\alpha^2} E_{\alpha^2} | \mu \rangle \right)  \\
  &=& \alpha^2 \cdot \mu  \\
  &=& 1/2
\end{eqnarray}
$$

Similarly, we have $$ \langle B | B \rangle = 1/2 $$ and

$$
\begin{eqnarray}
  &&  \langle A | B \rangle \\
  &=& \langle \mu | E_{\alpha^2} E_{\alpha^1} | E_{-\alpha^2} E_{-\alpha^1} | \mu \rangle  \\
  &=& \langle \mu | E_{\alpha^2} E_{-\alpha^2} E_{\alpha^1} E_{-\alpha^1} | \mu \rangle  \\
  &=& (\alpha^2)_i (\alpha^1)_j \langle \mu | H_i H_j | \mu \rangle  \\
  &=& (\alpha^2 \cdot \mu) (\alpha^1 \cdot \mu)  \\
  &=& 1/4.
\end{eqnarray}
$$

Clearly $$ \langle A | A \rangle \langle B | B \rangle \neq \langle A | B \rangle \langle B | A \rangle $$.

(Refer to any elementary QM book for why the equality is needed for $$ | A \rangle $$ and $$ | B \rangle $$ to be linearly dependent.)

### 9.B.

One way to solve this problem is to diagonalize $$ \frac{1}{2} \lambda_3 \sigma_2 $$ and
$$ \frac{1}{2} \lambda_8 \sigma_2 $$ and find out all the weights, which are the weights of (1, 0) and those of
(0, 1), and conclude it can be reduced into (1, 0) and (0, 1) of SU(3).

Or, we could notice the following.
- $$ \frac{1}{2} \lambda_a \otimes \sigma_2 $$ can be replaced by $$ \sigma_2 \otimes \frac{1}{2} \lambda_a $$, and
$$ \frac{1}{2} \lambda_a \otimes I $$ by $$ I \otimes \frac{1}{2} \lambda_a $$. This does not change the commutation
relationships at all.
- There is a similarity transformation S ($$ \frac{1}{\sqrt{2}} \begin{bmatrix} 1 & 1 \\ i & -i \end{bmatrix} $$) that transforms
$$ \sigma_2 $$ to $$ \sigma_3 $$.
- With the similarity transformations $$ S \otimes I $$, both $$ \sigma_3 \otimes \frac{1}{2} \lambda_a $$ for $$ a = 1, 3, 4, 6, 8 $$
and $$ I \otimes \frac{1}{2} \lambda_a $$ for $$ a = 2, 5, 7 $$ become
$$ \begin{bmatrix} \frac{1}{2} \lambda_a & 0 \\ 0 & -\frac{1}{2} \lambda_a^* \end{bmatrix} $$, since the former are real while
the latter are imaginery.

Now we have explicitly block diagonalized the 6-dimensional representation into a $$ 3 $$ and a $$ \bar{3} $$.

### 9.C.

The three weights of 3 are $$ | \mu_1 \rangle $$, $$ | \mu_1 - \alpha_1 \rangle $$, and
$$ | \mu_1 - \alpha_1 - \alpha_2 \rangle $$.

The highest weight of $$ 3 \times 3 $$ is $$ | \mu_{6, 2 \mu_1} \rangle = | \mu_1 \rangle | \mu_1 \rangle $$. Repeatedly applying
one of the two lowering operators on both sides, we get all the states in 6.

$$
\begin{eqnarray}
  &&            | \mu_{6, 2 \mu_1} \rangle = | \mu_1 \rangle | \mu_1 \rangle  \\
  &\Rightarrow& | \mu_{6, 2 \mu_1 - \alpha_1} \rangle = \frac{1}{\sqrt{2}} \left( | \mu_1 - \alpha_1 \rangle | \mu_1 \rangle + | \mu_1 \rangle | \mu_1 - \alpha_1 \rangle \right)  \\
  &\Rightarrow& | \mu_{6, 2 \mu_1 - 2 \alpha_1} \rangle = | \mu_1 - \alpha_1 \rangle | \mu_1 - \alpha_1 \rangle  \\
  &&            | \mu_{6, 2 \mu_1 - \alpha_1 - \alpha_2} \rangle = \frac{1}{\sqrt{2}} \left( | \mu_1 - \alpha_1 - \alpha_2 \rangle | \mu_1 \rangle + | \mu_1 \rangle | \mu_1 - \alpha_1 - \alpha_2 \rangle \right)  \\
  &\Rightarrow& | \mu_{6, 2 \mu_1 - 2 \alpha_1 - \alpha_2} \rangle = \frac{1}{\sqrt{2}} \left( | \mu_1 - \alpha_1 - \alpha_2 \rangle | \mu_1 - \alpha_1 \rangle + | \mu_1 - \alpha_1 \rangle | \mu_1 - \alpha_1 - \alpha_2 \rangle \right)  \\
  &\Rightarrow& | \mu_{6, 2 \mu_1 - 2 \alpha_1 - 2 \alpha_2} \rangle = | \mu_1 - \alpha_1 - \alpha_2 \rangle | \mu_1 - \alpha_1 - \alpha_2 \rangle
\end{eqnarray}
$$

Taking the state orthogonal to $$ | \mu_{6, 2 \mu_1 - \alpha_1} \rangle $$ to be $$ | \mu_{\bar{3}, 2 \mu_1 - \alpha_1} \rangle $$,
and repeatedly applying one of the two lowering operators on both sides, we get all the states in $$ \bar{3} $$.

$$
\begin{eqnarray}
  &&            | \mu_{\bar{3}, 2 \mu_1 - \alpha_1} \rangle = \frac{1}{\sqrt{2}} \left( | \mu_1 - \alpha_1 \rangle | \mu_1 \rangle - | \mu_1 \rangle | \mu_1 - \alpha_1 \rangle \right)  \\
  &\Rightarrow& | \mu_{\bar{3}, 2 \mu_1 - \alpha_1 - \alpha_2} \rangle = \frac{1}{\sqrt{2}} \left( | \mu_1 - \alpha_1 - \alpha_2 \rangle | \mu_1 \rangle - | \mu_1 \rangle | \mu_1 - \alpha_1 - \alpha_2 \rangle \right)  \\
  &\Rightarrow& | \mu_{\bar{3}, 2 \mu_1 - 2 \alpha_1 - \alpha_2} \rangle = \frac{1}{\sqrt{2}} \left( | \mu_1 - \alpha_1 - \alpha_2 \rangle | \mu_1 - \alpha_1 \rangle - | \mu_1 - \alpha_1 \rangle | \mu_1 - \alpha_1 - \alpha_2 \rangle \right)
\end{eqnarray}
$$

## Chapter 10

### 10.A.

See (11.39).

### 10.B.

Applying (10.36) and (10.13), we have

$$
\begin{eqnarray}
  &&  \langle u | T_a | v \rangle \\
  &=& \bar{u}^i_j (T_a v)^j_i  \\
  &=& \bar{u}^i_j ([T_a]^j_k v^k_i - [T_a]^k_i v^j_k)  \\
  &=& \frac{1}{2} (\bar{u}^i_j [\lambda_a]^j_k v^k_i) - \frac{1}{2} (\bar{u}^i_j v^j_k [\lambda_a]^k_i)  \\
  &=& \frac{1}{2} \mathrm{Tr}(\bar{u} \lambda_a v) - \frac{1}{2} \mathrm{Tr}(\bar{u} v \lambda_a),
\end{eqnarray}
$$

in agreement with (10.66). In fact, we could have started with (10.66) and take two special cases to solve
for $$ \lambda_1 $$ and $$ \lambda_2 $$.

### 10.C.

One can read off the components directly from the solution to 9.C. Simply notice that
$$ | \mu_1 \rangle \equiv |_1 \rangle $$, $$ | \mu_1 - \alpha_1 - \alpha_2 \rangle \equiv |_2 \rangle $$,
and $$ | \mu_1 - \alpha_1 \rangle \equiv |_3 \rangle $$.

### 10.D.a.

$$ \Delta $$ particles form a quartet and have components $$ \Delta^{ijk} $$. In particular, $$ \Delta^{++} $$ has
$$ \Delta^{111} = 1 $$ with the other components being zero, while $$ \Delta^0 $$ has
$$ \Delta^{122} = \Delta^{212} = \Delta^{221} = \frac{1}{\sqrt{3}} $$ with the other components being zero.

Pions form a triplet and have components $$ \pi^{ij} $$. In particular, $$ \pi^+ $$ has
$$ \pi^{11} = 1 $$ with the other components being zero, while $$ \pi^- $$ has
$$ \pi^{22} = 1 $$ with the other components being zero.

Nucleons form a doublet and have components $$ N^i $$. In particular, $$ p $$ has
$$ N^1 = 1 $$ and $$ N^2 = 0 $$.

Now we have

$$ \langle \Delta^{++} | H | \pi^+ p \rangle $$ = $$ \lambda \bar{\Delta}_{ijk} \pi^{ij} N^k = \lambda, $$

and

$$ \langle \Delta^0 | H | \pi^- p \rangle $$ = $$ \lambda \bar{\Delta}_{ijk} \pi^{ij} N^k = \frac{1}{\sqrt{3}} \lambda. $$

Note that we do not have to symmetrize $$ \pi^{ij} N^k $$ because the symmetry of $$ \bar{\Delta}_{ijk} $$
will automatically filter out any non-symmetrical aspect of $$ \pi^{ij} N^k $$. This is generally true
for any symmetrical pattern.

The probability ratio between the two interactions is thus 3:1.

### 10.D.b.

Applying $$ J^+ $$ to both sides of

$$ | \frac{3}{2}, \frac{3}{2} \rangle = \alpha | \frac{3}{2}, \frac{1}{2} \rangle \otimes | 1, 1 \rangle + \beta | \frac{3}{2}, \frac{3}{2} \rangle, \otimes | 1, 0 \rangle $$

we get 

$$ 0 = \sqrt{\frac{3}{2}} \alpha | \frac{3}{2}, \frac{3}{2} \rangle \otimes | 1, 1 \rangle + \beta | \frac{3}{2}, \frac{3}{2} \rangle, \otimes | 1, 1 \rangle. $$

We can therefore choose $$ \alpha = \sqrt{\frac{2}{5}} $$ and $$ \beta = -\sqrt{\frac{3}{5}} $$.

Applying $$ J^- $$ to both sides of

$$ | \frac{3}{2}, \frac{3}{2} \rangle = \sqrt{\frac{2}{5}} | \frac{3}{2}, \frac{1}{2} \rangle \otimes | 1, 1 \rangle - \sqrt{\frac{3}{5}} | \frac{3}{2}, \frac{3}{2} \rangle, \otimes | 1, 0 \rangle $$

we get 

$$
\begin{eqnarray}
  &&  \sqrt{\frac{3}{2}} | \frac{3}{2}, \frac{1}{2} \rangle  \\
  &=& + \sqrt{\frac{2}{5}} \sqrt{2} | \frac{3}{2}, -\frac{1}{2} \rangle \otimes | 1, 1 \rangle + \sqrt{\frac{2}{5}} | \frac{3}{2}, \frac{1}{2} \rangle \otimes | 1, 0 \rangle  \\
  &&  - \sqrt{\frac{3}{5}} \sqrt{\frac{3}{2}} | \frac{3}{2}, \frac{1}{2} \rangle, \otimes | 1, 0 \rangle - \sqrt{\frac{3}{5}} | \frac{3}{2}, \frac{3}{2} \rangle, \otimes | 1, -1 \rangle.
\end{eqnarray}
$$

Therefore the Clebsch-Gordan coefficient $$ \langle \frac{3}{2}, \frac{1}{2} | \frac{3}{2}, 1, \frac{1}{2}, 0 \rangle = - \frac{1}{\sqrt{15}} $$.

We get exactly the same result by using (10.94). (Note the sign is arbitrary.)

### 10.E.a.

The components of pions and nucleons can be read in the solution to 10.D.a. except for $$ \pi^0 $$ and $$ n $$, whose
non-zero components are $$ \pi^{12} = \pi^{21} = \frac{1}{\sqrt{2}} $$ and $$ N^2 = 1 $$.

For $$ \langle \pi^+ p | H_I | \pi^+ p \rangle $$, we have $$ j = k = l = 1 $$ giving us two non-zero terms

$$ A_{+p} = A_1 + A_2. $$

For $$ \langle \pi^- p | H_I | \pi^- p \rangle $$, we have $$ j = k = 2 $$ and $$ l = 1 $$ gives the only non-zero term

$$ A_{-p} = A_1. $$

For $$ \langle \pi^0 n | H_I | \pi^- p \rangle $$, we have $$ j = l = 2 $$ and $$ k = 1 $$ gives the only non-zero term

$$ A_{0n} = \frac{1}{\sqrt{2}} A_2. $$

Therefore we have

$$ A_{+p} = A_{-p} + \sqrt{2} A_{0n}. $$

### 10.E.b.

We have the following.

$$
\begin{eqnarray}
  | \pi^+ p \rangle &=& |1, 1 \rangle \otimes | \frac{1}{2}, \frac{1}{2} \rangle = | \frac{3}{2}, \frac{3}{2} \rangle  \\
  | \pi^- p \rangle &=& |1, -1 \rangle \otimes | \frac{1}{2}, \frac{1}{2} \rangle = \sqrt{\frac{1}{3}} | \frac{3}{2}, -\frac{1}{2} \rangle + \sqrt{\frac{2}{3}} | \frac{1}{2}, -\frac{1}{2} \rangle  \\
  | \pi^0 n \rangle &=& |1, 0 \rangle \otimes | \frac{1}{2}, -\frac{1}{2} \rangle = \sqrt{\frac{2}{3}} | \frac{3}{2}, -\frac{1}{2} \rangle - \sqrt{\frac{1}{3}} | \frac{1}{2}, -\frac{1}{2} \rangle
\end{eqnarray}
$$

Therefore we have

$$
\begin{eqnarray}
  A_{+p} &=& A_{I=3/2}  \\
  A_{-p} &=& \frac{1}{3} A_{I=3/2} + \frac{2}{3} A_{I=1/2}  \\
  A_{0n} &=& \frac{\sqrt{2}}{3} A_{I=3/2} - \frac{\sqrt{2}}{3} A_{I=1/2}
\end{eqnarray}
$$

Again we have

$$ A_{+p} = A_{-p} + \sqrt{2} A_{0n}. $$

## Chapter 11

### 11.A.

The reasoning following (11.33) works exactly the same way for a 6 as for a 10. Hence equal spacing.

### 11.B.

$$ \Delta^+ $$ has non-zero components $$ \Delta^{211} = \Delta^{121} = \Delta^{112} = \frac{1}{\sqrt{3}} $$.

$$ \pi^0 $$ has non-zero components $$ \pi^2_2 = \frac{1}{\sqrt{2}} $$ and $$ \pi^1_1 = -\frac{1}{\sqrt{2}} $$.

$$ P $$ has non-zero components $$ P^1_3 = -1 $$.

$$ \Sigma^{*0} $$ has non-zero components $$ \Sigma^{123} = \Sigma^{132} = \Sigma^{213} = \Sigma^{231} = \Sigma^{312} = \Sigma^{321} = \frac{1}{\sqrt{6}} $$.

$$ K^- $$ has non-zero components $$ K^3_1 = -1 $$.

10 appears only once in $$ 8 \times 8 $$. Thus the result depends only on one unknown variable. See (12.14) for more details.

Now we have

$$ \langle \Delta^+ | H | \pi^0 P \rangle = \lambda \epsilon^{mnk} \bar{\Delta}_{ijk} \pi^i_m P^j_n = \lambda (\bar{\Delta}_{211} \pi^2_2 P^1_3 - \bar{\Delta}_{112} \pi^1_1 P^1_3) = -\sqrt{\frac{2}{3}} \lambda, $$

and

$$ \langle \Sigma^{*0} | H | K^- P \rangle = \lambda \epsilon^{mnk} \bar{\Sigma}_{ijk} K^i_m P^j_n = - \lambda \bar{\Sigma}_{312} K^3_1 P^1_3 = -\frac{1}{\sqrt{6}} \lambda. $$

Therefore the the probability ratio is $$ (\sqrt{\frac{2}{3}} / \frac{1}{\sqrt{6}})^2 = 4 $$.

### 11.C.

From (11.42) and Eqs. (11.24), we have

$$
\begin{eqnarray}
  \mu(P) &=&        +\frac{2}{3} \alpha - \frac{1}{3} \beta  \\
  \mu(N) &=&        -\frac{1}{3} \alpha - \frac{1}{3} \beta  \\
  \mu(\Sigma^+) &=& +\frac{2}{3} \alpha - \frac{1}{3} \beta  \\
  \mu(\Sigma^0) &=& +\frac{1}{6} \alpha + \frac{1}{6} \beta  \\
  \mu(\Sigma^-) &=& -\frac{1}{3} \alpha + \frac{2}{3} \beta  \\
  \mu(\Xi^0)    &=& -\frac{1}{3} \alpha - \frac{1}{3} \beta  \\
  \mu(\Xi^-)    &=& -\frac{1}{3} \alpha + \frac{2}{3} \beta  \\
  \mu(\Lambda)  &=& -\frac{1}{6} \alpha - \frac{1}{6} \beta.
\end{eqnarray}
$$

Since $$ \alpha $$ and $$ \beta $$ can be solved in terms of $$ \mu(P) $$ and $$ \mu(N) $$, all other magnetic moments
can be expressed in terms of $$ \mu(P) $$ and $$ \mu(N) $$.

## Chapter 12

### 12.A.

We have

$$
\begin{eqnarray}
  &&  \begin{array}{|l|l|l|} \hline x & x & x \\ \hline y \\ \hline \end{array} \otimes \begin{array}{|l|l|l|} \hline a & a & a \\ \hline b \\ \hline \end{array}  \\
  &=& \begin{array}{|l|l|l|l|l|l|} \hline x & x & x & a & a & a \\ \hline y & b \\ \hline \end{array} \oplus \begin{array}{|l|l|l|l|l|l|} \hline x & x & x & a & a & a \\ \hline y & \\ \hline b & \\ \hline \end{array} \oplus  \\
  &&  \begin{array}{|l|l|l|l|l|} \hline x & x & x & a & a \\ \hline y & a & b \\ \hline \end{array} \oplus \begin{array}{|l|l|l|l|l|} \hline x & x & x & a & a \\ \hline y & a \\ \hline b & \\ \hline \end{array} \oplus  \\
  &&  \begin{array}{|l|l|l|l|l|} \hline x & x & x & a & a \\ \hline y & b \\ \hline a & \\ \hline \end{array} \oplus  \\
  &&  \begin{array}{|l|l|l|l|} \hline x & x & x & a \\ \hline y & a & a & b \\ \hline \end{array} \oplus \begin{array}{|l|l|l|l|} \hline x & x & x & a \\ \hline y & a & a \\ \hline b & \\ \hline \end{array} \oplus  \\
  &&  \begin{array}{|l|l|l|l|} \hline x & x & x & a \\ \hline y & a & b \\ \hline a & \\ \hline \end{array} \oplus \begin{array}{|l|l|l|l|} \hline x & x & x & a \\ \hline y & a \\ \hline a & b \\ \hline \end{array} \oplus  \\
  &&  \begin{array}{|l|l|l|} \hline x & x & x \\ \hline y & a & a \\ \hline a & b \\ \hline \end{array}.
\end{eqnarray}
$$

Eliminating columns with three boxes, we have

$$ (2, 1) \otimes (2, 1) = (4, 2) \oplus (5, 0) \oplus (2, 3) \oplus (3, 1) \oplus (3, 1) \oplus (0, 4) \oplus (1, 2) \oplus (1, 2) \oplus (2, 0) \oplus (0, 1). $$

When $$ u $$ and $$ v $$ are of the *same* representation, we can talk of the symmetric part and the antisymmetric part of
the tensor product of $$ u $$ and $$ v $$. When we exchange $$ u $$ and $$ v $$, the symmetric part will remain the same while
the antisymmetric part will change sign. (Clearly, it does not make sense to talke about symmetry or antisymmetry if
$$ u $$ and $$ v $$ are of different representations.)

To see which decomposed irreducible representations appear symmetrically or antisymmetrically in the tensor product, we simply swap
the corresponding labels of $$ u $$ and $$ v $$ and then try to get back to the original label setup by swapping two labels either in
the same row or in the same column. The sign remains the same when a swap is made in the same row, while the sign changes when a swap
is made in the same column.

For example, $$ \begin{array}{|l|l|l|l|l|} \hline x & x & x & a & a \\ \hline y & a \\ \hline b & \\ \hline \end{array} $$ becomes
$$ \begin{array}{|l|l|l|l|l|} \hline a & a & a & x & x \\ \hline b & x \\ \hline y & \\ \hline \end{array} $$. To get it back to
its original setup, we first swap $$ a $$ and $$ x $$ in the second column (with a sign change), then re-arrange the first row,
and finally exchange $$ b $$ and $$ y $$ in the first column (with another sign change). The whole process changes the sign twice
and therefore this representations appears symmetrically.

Going through this process one by one, we have

$$
\begin{eqnarray}
  [(2, 1) \otimes (2, 1)]_S    &=& (4, 2) \oplus (3, 1)_S \oplus (0, 4) \oplus (1, 2)_S \oplus (0, 1),  \\
  [(2, 1) \otimes (2, 1)]_{AS} &=& (5, 0) \oplus (2, 3) \oplus (3, 1)_{AS} \oplus (1, 2)_{AS} \oplus (2, 0).
\end{eqnarray}
$$

### 12.B.

We have

$$ \begin{array}{|l|l|l|} \hline x & x & x \\ \hline \end{array} \otimes \begin{array}{|l|l|} \hline a & a \\ \hline b \\ \hline \end{array} = \begin{array}{|l|l|l|l|l|} \hline x & x & x & a & a \\ \hline b & \\ \hline \end{array} \oplus \begin{array}{|l|l|l|l|} \hline x & x & x & a \\ \hline a & b \\ \hline \end{array} \oplus \begin{array}{|l|l|l|l|} \hline x & x & x & a \\ \hline a \\ \hline b \\ \hline \end{array} \oplus \begin{array}{|l|l|l|} \hline x & x & x \\ \hline a & a \\ \hline b \\ \hline \end{array}, $$

or equivalently,

$$ 10 \otimes 8 = 35 + 27 + 10 + 8. $$

### 12.C.

The general claim that *for any Lie group, the tensor product of the adjoint representation with any
arbitrary nontrivial representation $$ D $$ must contain $$ D $$* can be proved as follows.

Here we assume the Lie algebra to be simple and of rank $$ n $$ and $$ D $$ to be irreducible.

Without loss of generality, we can label $$ D $$ by its Dynkin coefficients $$ (l^1, l^2, \dots, l^m, 0, \dots, 0) $$,
with $$ l^1, l^2, \dots, l^m > 0 $$. (We can always reorder the simple roots to get the Dynkin coefficients in this form.)
We denote the highest weight of $$ D $$ as $$ \mu $$.

Our goal is to a find a state $$ | v \rangle \in \mathrm{adj} \otimes D $$ such that
$$ H_i | v \rangle = \mu_i $$ and $$ E_{\alpha^i} | v \rangle = 0 $$ for any $$ i = 1 \dots n $$. This will allow us to apply
the lowering operators repeatedly on $$ | v \rangle $$ to construct a representation $$ D' $$ in the space of
$$ \mathrm{adj} \otimes D $$. $$ D' $$ are $$ D $$ are then the same representation since they share the highest weight.

To satisfy the requirement $$ H_i | v \rangle = \mu_i $$, we can have

$$ | v \rangle = \sum_{i = 1}^m a_i | E_{\alpha^i} \rangle \otimes | \mu - \alpha^i \rangle + \sum_{j = 1}^n b_j | H_j \rangle \otimes | \mu \rangle. $$

Note that each $$ | \mu - \alpha^i \rangle \in D $$ is unique because $$ \mu $$ is the highest weight.

The requirement $$ E_{\alpha^i} | v \rangle = 0 $$ leads to the following simultaneous equations.

$$
\begin{eqnarray}
  a_i \sqrt{\frac{\mu_1}{2}} &-& \sum_{j = 1}^n b_j (\alpha^1)_j &=& 0  \\
  \cdots  \\
  a_m \sqrt{\frac{\mu_m}{2}} &-& \sum_{j = 1}^n b_j (\alpha^m)_j &=& 0  \\
  &-& \sum_{j = 1}^n b_j (\alpha^{m + 1})_j &=& 0  \\
  \cdots  \\
  &-& \sum_{j = 1}^n b_j (\alpha^n)_j &=& 0
\end{eqnarray}
$$

There are $$ n $$ equations with $$ m + n $$ free variables. We have thus $$ m $$ unique (i.e. linearly independent) solutions.
In fact, this suggests that $$ D $$ should appear $$ m $$ times in $$ \mathrm{adj} \otimes D $$.

For the particular case of $$ SU(3) $$, the Young Tableau of $$ D $$ has three possible forms.
- $$ \mu = (l^1, 0) $$: $$ \begin{array}{|l|l|l|} \hline x & \cdots & x \\ \hline \end{array} $$
- $$ \mu = (0, l^2) $$: $$ \begin{array}{|l|l|l|} \hline x & \cdots & x \\ \hline y & \cdots & y \\ \hline \end{array} $$
- $$ \mu = (l^1, l^2) $$: $$ \begin{array}{|l|l|l|l|l|} \hline x & \cdots & x & \cdots & x \\ \hline y & \cdots & y \\ \hline \end{array} $$

To decompose their tensor product with 8, we add two $$ a $$-boxes and one $$ b $$-box. In the first two cases above,
we must add the first $$ a $$ to row one, the second $$ a $$ to row two, and the $$ b $$ to row three to get $$ D $$ again.
In the third case, we can either do the same or add the $$ a $$ to row one, the second $$ a $$ to row three, and the $$ b $$
to row two. The number of apperances of $$ D $$ is consistent with our discussion in the general case.

## Chapter 13

### 13.A.

We are _effectively_ decomposing $$ SU(n) \rightarrow SU(n - 1) \times SU(1) \times U(1) $$. Of course
$$ SU(1) $$ does not exist. But this explains why only two possibilities exist to decompose the fundamental
representation $$ D^m $$ of $$ SU(n) $$, whose tableau is made of one column with $$ m $$ boxes.
- $$ (D^m)_m $$: $$ [m] \rightarrow ([m], [0]) $$,
- $$ (D^{m - 1})_{m - n} $$: $$ [m] \rightarrow ([m - 1], [1]) $$.

### 13.B.

Without actually drawing tableuax, we have

$$ [3] \otimes [1] = [3, 1] \oplus [4], $$

or 

$$ 10 \otimes 5 = 45 \oplus \bar{5}. $$

### 13.C.

Without actually drawing tableuax, we have

$$ [3, 1] \otimes [2, 1] = [3, 2, 1, 1] \oplus [4, 1, 1, 1] \oplus [3, 2, 2] \oplus [3, 3, 1] \oplus [4, 2, 1] \oplus [4, 2, 1] \oplus [5, 1, 1] \oplus [4, 3] \oplus [5, 2]. $$

### 13.D.

The fact that there is only one column in the tableau of any fundamental representation of $$ SU(N + M) $$ dictates
that there can be only one column in the tableau of $$ SU(N) $$ and only one column in the tableau of $$ SU(M) $$ in the
decomposition. Therefore we have

$$ [m] \rightarrow \sum_{i + j = m} ([i] , [j])_{M i - N j}. $$

Similarly, we have for the adjoint representation

$$
\begin{eqnarray}
  && [N + M - 1, 1]  \\
  &\rightarrow& \sum_{i + j = M + N} ([i], [j])_{M i - N j} + \sum_{i + j = M + N} ([i - 1, 1], [j])_{M i - N j} + \sum_{i + j = M + N} ([i], [j - 1, 1])_{M i - N j}  \\
  &=& ([N], [M])_0 \oplus ([N - 1, 1], [M])_0 \oplus ([N, 1], [M - 1])_{M + N} \oplus ([N - 1], [M, 1])_{-M - N} \oplus ([N], [M - 1, 1])_0.
\end{eqnarray}
$$

Double-checking the dimensionality, we have $$ (N + M)^2 - 1 = 1 + N^2 - 1 + N M + N M + M^2 - 1 $$. So it checks out.

13.E.

Without actually drawing tableuax, we have

$$ [2] \otimes [1, 1] = [2, 1, 1] \oplus [3, 1], $$

or 

$$ \frac{N (N-1)}{2} \cdot \frac{(N+1) N}{2} = \frac{(N+2)(N+1)N(N-1)}{8} + \frac{(N+1)N(N-1)(N-2)}{8}. $$

## Chapter 14

### 14.A.

The conclusion itself is rather obvious, since we know that $$ a^\dagger_i $$ transform like a $$ (1, 0) $$ and $$ a_i $$
transform like a $$ (0, 1) $$, and $$ O^k_{i j} $$ is obtained by symmetrizing the two $$ (1, 0) $$s and subtracting the trace,
exactly how one is supposed to get the $$ (2, 1) $$ representation.

To actually *show* it, we commute $$ Q_a $$ as defined in (14.8) with $$ O^k_{i j} $$. After some rather tedious
calculation, which I will not copy here from my notebook, we get the following result.

$$ [Q_a, O^k_{i j}] = [T_a]_{m i} Q^k_{m j} + [T_a]_{m j} Q^k_{i m} - [T_a^*]_{m k} Q^m_{i j} $$

### 14.B.

We could use brute force to figure out the only non-zero term is obtained by have $$ i = 3 $$ and $$ j = k = 1 $$.

We could also notice that $$ O^k_{i j} $$ transform like a $$ (2, 1) $$, $$ a^\dagger_i (a^\dagger_l b^\dagger_l) | 0 \rangle $$
transform like a $$ (1, 0) $$, and $$ a^\dagger_j a^\dagger_k (a^\dagger_l b^\dagger_l) | 0 \rangle $$ transform like a $$ (2, 0) $$,
and conclude that $$ O^k_{i j}  a^\dagger_i (a^\dagger_l b^\dagger_l) | 0 \rangle $$ can be decompose into
$$ (3, 1) \oplus (0, 2) \oplus (2, 0) $$ using Young tableaux, which also tell us $$ (2, 0) $$ is obtained by
contracting the $$ i $$ in $$ a^\dagger_i (a^\dagger_l b^\dagger_l) | 0 \rangle $$ with the $$ k $$ in $$ O^k_{m n} $$.

Either way, the only non-zero element is

$$
\begin{eqnarray}
  &&  \langle 0 | (a_k b_k) a_1 a_1 a^\dagger_1 a^\dagger_1 a_3 a^\dagger_3 (a^\dagger_l b^\dagger_l) | 0 \rangle \\
  &=& \langle 0 | (a_k b_k) a_1 a_1 a^\dagger_1 a^\dagger_1 [a_3, a^\dagger_3] (a^\dagger_l b^\dagger_l) | 0 \rangle + \langle 0 | (a_k b_k) a_1 a_1 a^\dagger_1 a^\dagger_1 a^\dagger_3 a_3 (a^\dagger_l b^\dagger_l) | 0 \rangle \\
  &=& \langle 0 | a_1 a_1 (a_k b_k) (a^\dagger_l b^\dagger_l) a^\dagger_1 a^\dagger_1 | 0 \rangle + \langle 0 | (a_k b_k) a_1 a_1 a^\dagger_1 a^\dagger_1 a^\dagger_3 b^\dagger_3) | 0 \rangle \\
  &=& \langle 0 | a_1 a_1 (a_k b_k) (a^\dagger_k b^\dagger_k) a^\dagger_1 a^\dagger_1 | 0 \rangle + \langle 0 | (a_3 b_3) a_1 a_1 a^\dagger_1 a^\dagger_1 a^\dagger_3 b^\dagger_3) | 0 \rangle \\
  &=& \langle 0 | a_1 a_1 a_1 a^\dagger_1 a^\dagger_1 a^\dagger_1 | 0 \rangle + 2 \langle 0 | a_1 a_1 a^\dagger_1 a^\dagger_1 | 0 \rangle + \langle 0 | a_1 a_1 a^\dagger_1 a^\dagger_1 | 0 \rangle \\
  &=& 6 \langle 0 | a_1 a_1 a^\dagger_1 a^\dagger_1 | 0 \rangle  \\
  &=& 12.
\end{eqnarray}
$$

### 14.C.

We have

$$
\begin{eqnarray}
  &&  2 Q_2  \\
  &=& -i a^\dagger_1 a_2 + i a^\dagger_2 a_1  \\
  &=& -\frac{i}{2 m \omega} (m \omega r_1 - i p_1) (m \omega r_2 + i p_2) + \frac{i}{2 m \omega} (m \omega r_2 - i p_2) (m \omega r_1 + i p_1) \\
  &=& - p_1 r_2 + r_1 p_2  \\
  &=& L_3.
\end{eqnarray}
$$

The other two in (14.13) can be shown similarly.

### 14.D.

We have

$$
\begin{eqnarray}
  &&  [Q_\alpha, a_k b_k] \\
  &=& [T_\alpha]_{i j} [a^\dagger_i a_j, a_k b_k] - [T^*_\alpha]_{i j} [b^\dagger_i b_j, a_k b_k] \\
  &=& -\delta_{i k} [T_\alpha]_{i j} a_j b_k + \delta_{i k} [T^*_\alpha]_{i j} a_k b_j \\
  &=& -[T_\alpha]_{j i} a_i b_j + [T^*_\alpha]_{i j} a_i b_j \\
  &=& 0.
\end{eqnarray}
$$

The last equality is due to the hermiticity of $$ T_\alpha $$.

## Chapter 15

### 15.A.

$$
\begin{eqnarray}
  | \Sigma^+, 1/2 \rangle &=& \frac{\sqrt{2}}{6} \bigg\{ |uus\rangle (2 |++-\rangle - |+-+\rangle - |-++\rangle ) + \textrm{cyclic permutations} \bigg\}  \\
  | \Sigma^0, 1/2 \rangle &=& \frac{1}{6} \bigg\{ (|uds\rangle + |dus\rangle) (2 |++-\rangle - |+-+\rangle - |-++\rangle ) + \textrm{cyclic permutations} \bigg\}  \\
  | \Sigma^-, 1/2 \rangle &=& \frac{\sqrt{2}}{6} \bigg\{ |dds\rangle (2 |++-\rangle - |+-+\rangle - |-++\rangle ) + \textrm{cyclic permutations} \bigg\}  \\
  | \Xi^0, 1/2 \rangle &=& \frac{\sqrt{2}}{6} \bigg\{ |ssu\rangle (2 |++-\rangle - |+-+\rangle - |-++\rangle ) + \textrm{cyclic permutations} \bigg\}  \\
  | \Xi^-, 1/2 \rangle &=& \frac{\sqrt{2}}{6} \bigg\{ |ssd\rangle (2 |++-\rangle - |+-+\rangle - |-++\rangle ) + \textrm{cyclic permutations} \bigg\}  \\
\end{eqnarray}
$$

### 15.B.a.

For each particle except $$ \Sigma^0 $$, we only need to change the $$ Q $$ in (15.20) accordingly to get the correct ratio.
For $$ \Sigma^0 $$, we simply do it twice: once for $$ |uds\rangle $$ and once for $$ |dus\rangle $$.

$$
\begin{eqnarray}
  \mu_{\Sigma^+} / \mu_P &=& 3 \frac{2}{36} \left(\frac{2}{3} 4 + \frac{2}{3} 4 - \frac{1}{3} (-2)\right) &=& 1  \\
  \mu_{\Sigma^0} / \mu_P &=& 3 \frac{1}{36} \left((\frac{2}{3} - \frac{1}{3}) 4 + (-\frac{1}{3} + \frac{2}{3}) 4 - (\frac{1}{3} + \frac{1}{3}) (-2)\right) &=& \frac{1}{3}  \\
  \mu_{\Sigma^-} / \mu_P &=& 3 \frac{2}{36} \left(-\frac{1}{3} 4 - \frac{1}{3} 4 - \frac{1}{3} (-2)\right) &=& -\frac{1}{3}  \\
  \mu_{\Xi^0} / \mu_P &=& 3 \frac{2}{36} \left(-\frac{1}{3} 4 - \frac{1}{3} 4 + \frac{2}{3} (-2)\right) &=& -\frac{2}{3}  \\
  \mu_{\Xi^-} / \mu_P &=& 3 \frac{2}{36} \left(-\frac{1}{3} 4 - \frac{1}{3} 4 - \frac{1}{3} (-2)\right) &=& -\frac{1}{3}  \\
\end{eqnarray}
$$

### 15.B.b.

Again, we can almost read off results from (15.20). Now we also need to include the mass in the factors together with $$ Q $$.

$$
\begin{eqnarray}
  \mu_{\Sigma^+} &=& 3 \frac{2}{36} \left(\frac{2}{3} \frac{m_P}{m} 4 + \frac{2}{3} \frac{m_P}{m} 4 - \frac{1}{3} \frac{m_P}{m_s} (-2)\right) &=& \frac{8}{9} \frac{m_P}{m} + \frac{1}{9} \frac{m_P}{m_s} &=& 2.88 \\
  \mu_{\Sigma^0} &=& 3 \frac{1}{36} \left((\frac{2}{3} - \frac{1}{3}) \frac{m_P}{m} 4 + (-\frac{1}{3} + \frac{2}{3}) \frac{m_P}{m} 4 - (\frac{1}{3} + \frac{1}{3}) \frac{m_P}{m_s} (-2)\right) &=& \frac{2}{9} \frac{m_P}{m} + \frac{1}{9} \frac{m_P}{m_s} &=& 0.88 \\
  \mu_{\Sigma^-} &=& 3 \frac{2}{36} \left(-\frac{1}{3} \frac{m_P}{m} 4 - \frac{1}{3} \frac{m_P}{m} 4 - \frac{1}{3} \frac{m_P}{m_s} (-2)\right) &=& -\frac{4}{9} \frac{m_P}{m} + \frac{1}{9} \frac{m_P}{m_s} &=& -1.12 \\
  \mu_{\Xi^0} &=& 3 \frac{2}{36} \left(-\frac{1}{3} \frac{m_P}{m_s} 4 - \frac{1}{3} \frac{m_P}{m_s} 4 + \frac{2}{3} \frac{m_P}{m} (-2)\right) &=& -\frac{4}{9} \frac{m_P}{m_s} - \frac{2}{9} \frac{m_P}{m_s} &=& -1.52 \\
  \mu_{\Xi^-} &=& 3 \frac{2}{36} \left(-\frac{1}{3} \frac{m_P}{m_s} 4 - \frac{1}{3} \frac{m_P}{m_s} 4 - \frac{1}{3} \frac{m_P}{m} (-2)\right) &=& -\frac{4}{9} \frac{m_P}{m_s} + \frac{1}{9} \frac{m_P}{m} &=& -0.52 \\
\end{eqnarray}
$$

### 15.C.

Note that the spin parts in (15.13) are simply bystanders as far as isospin is concerned. We have

$$ I^+ (|uds\rangle - |dus\rangle ) = \frac{1}{\sqrt{2}} (|uus\rangle - |uus\rangle ) = 0 $$

and

$$ I^- (|uds\rangle - |dus\rangle ) = \frac{1}{\sqrt{2}} (|dds\rangle - |dds\rangle ) = 0. $$

Idem for the cyclic permutations.

## Chapter 16

### 16.A.

The foolproof way is to treat (16.8) as the tensor product of two matrices $$ T^A_a \otimes T^B_a $$ and
diagnoalize the result. $$ - T_a T^*_a $$ for $$ q\bar{q} $$ pairs and $$ T_a T_a $$ for $$ qq $$ pairs,
where $$ T_a $$ are those defined in (7.6). This gives us both the eigenvalues and eigenvectors.

Or, knowing the eigenvectors of the decompositions of $$ 3 \times 3 = 6 + \bar{3} $$ and
$$ 3 \times \bar{3} = 8 + 1 $$, we can simply take a few special values to get the general result.

- $$ q\bar{q} $$ traceless pairs: $$ - \langle 1, 2 | T_a T^*_a | 1, 2 \rangle = \frac{1}{6} $$
- $$ q\bar{q} $$ trace pairs: $$ - \frac{1}{3} (\langle 1, 1 | + \langle 2, 2 | + \langle 3, 3 |) T_a T^*_a (| 1, 1 \rangle + | 2, 2 \rangle + | 3, 3 \rangle) = - \frac{4}{3} $$
- $$ qq $$ symmetric pairs: $$ \langle 1, 1 | T_a T_a | 1, 1 \rangle = \frac{1}{3} $$
- $$ qq $$ antisymmetric pairs: $$ \frac{1}{2} (\langle 1, 2 | - \langle 2, 1 |) T_a T_a (| 1, 2 \rangle - | 2, 1 \rangle) = -\frac{2}{3} $$

### 16.B.

We expect the bound states to be colorless singlets. There are the following possibilities, which all transform
like a $$ \bar{6} $$ under Gell-Mann's $$ SU(3) $$. (They share the same Young tableau made of two 3-box columns.)
- 1 quix and 2 antiquarks.
- 1 quix, 2 quarks, and 1 antiquark.
- 1 quix and 4 quarks.

### 16.C.

To prove (a), notice that $$ T^{D_1 \oplus D_2}_a = \begin{bmatrix} T^{D_1}_a & 0 \\ 0 & T^{D_2}_a \end{bmatrix} $$.

To prove (b), notice that $$ T^{D_1 \otimes D_2}_a = T^{D_1}_a \otimes I + I \otimes T^{D_2}_a $$ and that
$$ \mathrm{Tr} (A \otimes B) = \mathrm{Tr} (A) \mathrm{Tr} (B) $$ = 0 when $$ A $$ and $$ B $$ are generators and thus traceless.

For $$ C(8) $$, We have

$$
\begin{eqnarray}
  &&    3 \otimes \bar{3} = 8 \oplus 1  \\
  &\to& 3 \cdot k_{\bar{3}} + k_3 \cdot 3 = k_8 + k_1  \\
  &\to& k_8 = 3 \cdot k_{\bar{3}} + k_3 \cdot 3 = 3 \cdot \frac{1}{2} + \frac{1}{2} \cdot 3 = 3  \\
  &\to& C(8) = 8 k_8 / 8 = 3.
\end{eqnarray}
$$

For $$ C(6) $$, We have

$$
\begin{eqnarray}
  &&    3 \otimes 3 = 6 \oplus \bar{3}  \\
  &\to& 3 \cdot k_{3} + k_3 \cdot 3 = k_6 + k_\bar{3}  \\
  &\to& k_6 = 3 \cdot k_{3} + k_3 \cdot 3 - k_\bar{3}  = 3 \cdot \frac{1}{2} + \frac{1}{2} \cdot 3 - \frac{1}{2} = \frac{5}{2}  \\
  &\to& C(6) = 8 k_6 / 6 = \frac{10}{3}.
\end{eqnarray}
$$

For $$ C(10) $$, We have

$$
\begin{eqnarray}
  &&    6 \otimes 3 = 10 \oplus 8  \\
  &\to& 6 \cdot k_{3} + k_6 \cdot 3 = k_{10} + k_8  \\
  &\to& k_{10} = 6 \cdot k_{3} + k_6 \cdot 3 - k_\bar{8}  = 6 \cdot \frac{1}{2} + \frac{5}{2} \cdot 3 - 3 = \frac{15}{2}  \\
  &\to& C(10) = 8 k_{10} / 10 = 6.
\end{eqnarray}
$$

## Chapter 17

### 17.A.

The $$ qq $$ pair must form a $$ 6 $$ in the color $$ SU(3) $$ and has thus color symmetry. Combined with space symmetry,
the two quarks must be antisymmetric in flavor-spin since quarks are fermions. We have therefore two possibilities.
- Spin-1: $$ qq $$ symmetric in spin and antisymmetric in flavor. The $$ qq\bar{Q} $$ form a $$ \bar{3} $$ in flavor $$ SU(3) $$.
- Spin-0: $$ qq $$ antisymmetric in spin and symmetric in flavor. The $$ qq\bar{Q} $$ form a $$ 6 $$ in flavor $$ SU(3) $$.

### 17.B.

Both $$ \rho $$ and $$ \pi $$ are made of quark-antiquark pairs $$ u $$ and $$ d $$ and their antiparticles.
The $$ \rho - \pi $$ splitting has the a factor of $$ \frac{1}{m_{u,d} m_{u,d}} $$.

Similarly, the $$ K^* - K $$ splitting has a factor of $$ \frac{1}{m_{u,d} m_s} $$. Therefore we have

$$ \frac{m_{u,d}}{m_s} = \frac{m_{K^*} - m_K}{m_\rho - m_\pi} = \frac{892 - 494}{775 - 140} = 0.63. $$

The factors for $$ \Sigma^0 $$ and $$ \Lambda $$ are given in this [note](#Sigma-Lambda).

The factor for $$ \Sigma^{*0} $$ is $$ \frac{1}{m_{u,d} m_{u,d}} + 2 \frac{1}{m_{u,d} m_s} $$.

Now we have

$$
  \frac{m_{\Sigma^0} - m_\Lambda}{m_{\Sigma^{*0}} - m_{\Sigma^0}} =
  \frac{\frac{4}{3} (\frac{1}{m_{u,d} m_{u,d}} - \frac{1}{m_{u,d} m_s})}{\frac{2}{3} \frac{1}{m_{u,d} m_{u,d}} + \frac{10}{3} \frac{1}{m_{u,d} m_s} } =
  \frac{2(1 - \frac{m_{u,d}}{m_s})}{1 + 5 \frac{m_{u,d}}{m_s} } =
  \frac{1193 - 1116}{1384 - 1193} = 0.4031.
$$

Rearranging the terms, we have

$$ \frac{m_{u,d}}{m_s} = \frac{2 - \frac{m_{\Sigma^0} - m_\Lambda}{m_{\Sigma^{*0}} - m_{\Sigma^0}}}{5 \frac{m_{\Sigma^0} - m_\Lambda}{m_{\Sigma^{*0}} - m_{\Sigma^0}} + 2} = 0.40. $$

### 17.C.

The $$ qq $$ pair must form a $$ \bar{3} $$ in the color $$ SU(3) $$ and has thus color antisymmetry. Combined with
space symmetry, the two quarks must be symmetric in flavor-spin since quarks are fermions. We have therefore two possibilities.
- Spin-$$ \frac{3}{2} $$: $$ qq $$ symmetric in both spin and flavor. The $$ qqc $$ form a $$ 6 $$ in flavor $$ SU(3) $$.
- Spin-$$ \frac{1}{2} $$: $$ qq $$ antisymmetric in both spin and flavor. The $$ qqc $$ form a $$ \bar{3} $$ in flavor $$ SU(3) $$.

As far as mass splitting is concerned, we cannot reply on (17.2) because now the four types of quarks have very different masses.

## Chapter 18

### 18.A.

$$ u^\dagger $$ transforms like $$ 1_{2/3} $$ (with the notation from Section 12.3) while $$ \bar{u}^\dagger $$
transforms like $$ 2_{-1/6} $$. Their tensor product transforms like $$ 2_{1/2} $$, which is exactly how the
Higgs field transforms.

Similary, the tensor product of the representation of $$ d^\dagger $$ and that of $$ \bar{d}^\dagger $$, as well as the
tensor product of the representation of $$ e^\dagger $$ and that of $$ \bar{e}^\dagger $$, transforms like the complex
conjugate of the Higgs field.

### 18.B.

We have

$$
\begin{eqnarray}
  &&   {\left[ (3, 1)_{-1/3} \oplus (1, 2)_{1/2} \right] \otimes \left[ (3, 1)_{-1/3} \oplus (1, 2)_{1/2} \right]}_S  \\
  &=& (6, 1)_{-2/3} \oplus (1, 3)_1 \oplus (3, 2)_{1/6}.
\end{eqnarray}
$$

Note that the $$ (3, 2)_{1/6} $$ here is different to that in (18.22).

### 18.C.

The symmetric tensor product of (18.21) with itself, treated in $$ SU(5) $$, is as follows, with dimensionality 55.
(See Solution to Problem 12.A. for a more detailed discussion on the symmetry of tensor products.)

$$ ([3] \otimes [3])_S = ([3], [3]) \oplus ([5], [1]). $$

Breaking $$ SU(5) $$ down into $$ SU(3) \times SU(2) \times U(1) $$, We have

$$
\begin{eqnarray}
  &&   {\left[ (3, 1)_{2/3} \oplus (1, 1)_{-1} \oplus (\bar{3}, 2)_{-1/6} \right] \otimes \left[ (3, 1)_{2/3} \oplus (1, 1)_{-1} \oplus (\bar{3}, 2)_{-1/6} \right]}_S  \\
  &=& (6, 1)_{4/3} \oplus (3, 1)_{-1/3} \oplus (8, 2)_{1/2} \oplus (1, 2)_{1/2} \oplus (1, 1)_{-2} \oplus (\bar{3}, 2)_{-7/6} \oplus (\bar{6}, 3)_{-1/3} \oplus (3, 1)_{-1/3}.
\end{eqnarray}
$$

### 18.D.

$$ p $$ is colorless with charge 1, $$ O $$ is colorless with charge 0, and $$ \pi^0 $$ and $$ \bar{e} $$ together are
colorless with charge 1. So everything is okay as far as color and charge are concerned.

There are a few things to be said about $$ O $$.
- The annihilation operator of a particle is also the creation operator of its antiparticle.
- The antiparticle of a right-handed particle is left-handed.
- $$ O $$ should really be $$ {\bar{e}_R}^\dagger \epsilon^{abc} {u_L}_a {u_R}_b {d_R}_c $$. The four operators in $$ O $$,
from left right, are in $$ 5 $$, $$ \bar{10} $$, $$ 10 $$, and $$ \bar{5} $$. The last three annihilates the two up quarks in the
proton with opposite helicity, and creates a left-handed anti-down quark, which is then combined with the right-handed
down quark in the proton to form $$ \pi^0 $$.

[This paper](http://math.ucr.edu/home/baez/guts.pdf) provides useful details to understand the Standard Model and
the $$ SU(5) $$ GUT.

### 18.D.

We have

$$
\begin{eqnarray}
  &&   5 \otimes \bar{10}  \\
  &=&  \left[ (3, 1)_{-\frac{1}{3}} \oplus (1, 2)_{\frac{1}{2}} \right] \left[ (3, 1)_{\frac{2}{3}} \oplus (1, 1)_{-1} \oplus (\bar{3}, 2)_{-\frac{1}{6}} \right]   \\
  &=&  (6, 1)_{\frac{1}{3}} \oplus (\bar{3}, 1)_{\frac{1}{3}} \oplus (3, 1)_{-\frac{4}{3}} \oplus (8, 2)_{-\frac{1}{2}} \oplus (1, 2)_{-\frac{1}{2}} \oplus  \\
  &&   (3, 2)_{\frac{7}{6}} \oplus (1, 2)_{-\frac{1}{2}} \oplus (\bar{3}, 3)_{\frac{1}{3}} \oplus (\bar{3}, 1)_{\frac{1}{3}}  \\
  &=&  \bar{5} + \bar{45}
\end{eqnarray}
$$

Subtracting $$ \bar{5} = (\bar{3}, 1)_{\frac{1}{3}} \oplus (1, 2)_{-\frac{1}{2}} $$ and taking the complex conjugate, we have

$$ 45 = (\bar{6}, 1)_{-\frac{1}{3}} \oplus (\bar{3}, 1)_{\frac{4}{3}} \oplus (8, 2)_{\frac{1}{2}} \oplus (\bar{3}, 2)_{-\frac{7}{6}} \oplus (1, 2)_{\frac{1}{2}} \oplus (3, 3)_{-\frac{1}{3}} \oplus (3, 1)_{-\frac{1}{3}}. $$

The method described in Section 13.5 gives the same result.

For $$ 50 $$, we can exploit $$ \bar{10} \otimes \bar{10} = 5 \oplus 45 \oplus 50 $$ but the method described in
Section 13.5 is actually easier. Either way, we get

$$ 50 = (6, 1)_{\frac{4}{3}} \oplus (3, 1)_{-\frac{1}{3}} \oplus (1, 1)_{-2} \oplus (8, 2)_{\frac{1}{2}} \oplus (\bar{3}, 2)_{-\frac{7}{6}} \oplus (\bar{6}, 3)_{-\frac{1}{3}}. $$

As claimed in Section 18.7, 45, but not 50, has a component that transforms like the $$ SU(3) \times SU(2) \times U(1) $$
Higgs field – the $$ (1, 2)_\frac{1}{2} $$.

## Chapter 19

### 19.A.

First we need to check that these generators close under commutation. It is obvious between $$ \sigma_a $$, $$ \tau_a $$,
and $$ \eta_a $$, and between one of these and $$ \sigma_a \tau_b \eta_c $$.

Now we check how it works between $$ \sigma_a \tau_b \eta_c $$ and $$ \sigma_{a'} \tau_{b'} \eta_{c'} $$. There are four
possibilities.
- They do not share any common component, the commucator has the same form $$ \sigma_a \tau_b \eta_c $$;
- They share one component, e.g. $$ \sigma_1 $$, they commute;
- They share two components, e.g. $$ \sigma_1 \tau_1 $$, the commucator has the form $$ \sigma_a $$, $$ \tau_a $$, or $$ \eta_a $$;
- They share three components, they commute.

We can take $$ \sigma_3 $$, $$ \tau_3 $$, $$ \eta_3 $$, and $$ \sigma_3 \tau_3 \eta_3 $$ as the four Cartan generators
$$ H_1 $$, $$ H_2 $$, $$ H_3 $$, and $$ H_4 $$. (Note that (6.2) is satisfied so they are correctly normalized relative to
each other.) We could go through the weights of the defining representation and take the differences to get the roots.
But let us try to find the roots directly. Other than the four Cartan generators, we want linear combinations of the other
generators $$ x $$ satisfying $$ [H_i, x] = \alpha(i, x) x $$.

Our first guess would be the following linear combinations.

$$
\begin{array}{}
  \sigma^\pm & \tau^\pm & \eta^\pm  \\
  \sigma^\pm \tau_3 \eta_3 & \sigma_3 \tau^\pm \eta_3 & \sigma_3 \tau_3 \eta^\pm  \\
  \sigma_3 \tau^\pm \eta^{\pm'} & \sigma^\pm \tau_3 \eta^{\pm'} & \sigma^\pm \tau^{\pm'} \eta_3  \\
  \sigma^\pm \tau^{\pm'} \eta^{\pm''}
\end{array}
$$

There are $$ 6 + 12 + 6 + 8 = 32 $$ of these. So the number is right.

$$ H_1 $$, $$ H_2 $$, and $$ H_3 $$ commuting with these generators do give the desired outcome. But we have

$$
\begin{eqnarray}
  [H_4, \sigma^\pm] &=& \pm 2 \sigma^\pm \tau_3 \eta_3  \\
  [H_4, \sigma^\pm \tau_3 \eta_3 ] &=& \pm 2 \sigma^\pm  \\
  [H_4, \sigma_3 \tau^\pm \eta^{\pm'} ] &=& 0  \\
  [H_4, \sigma^\pm \tau^{\pm'} \eta^{\pm''} ] &=& \pm \pm' \pm'' 2 \sigma^\pm \tau^{\pm'} \eta^{\pm''},
\end{eqnarray}
$$

where the first two commucators do not give us the right form. We need to change the first two rows
of generators to the following form.

$$
\begin{array}{}
  \sigma^\pm \pm' \sigma^\pm \tau_3 \eta_3 & \tau^\pm \pm' \sigma_3 \tau^\pm \eta_3 & \eta^\pm \pm' \sigma_3 \tau_3 \eta^\pm,
\end{array}
$$

which give us

$$
\begin{eqnarray}
  [H_4, \sigma^\pm \pm' \sigma^\pm \tau_3 \eta_3] &=& \pm \pm' 2 (\sigma^\pm \pm' \sigma^\pm \tau_3 \eta_3).
\end{eqnarray}
$$

To get the roots, we only need to focus on the following generators. The rest are obtained through permutating the
first three components of $$ \alpha $$. ($$ (a, b, b, c) $$ becomes $$ (b, a, b, c) $$ and $$ (b, b, a, c) $$.)
- $$ \sigma^\pm \pm' \sigma^\pm \tau_3 \eta_3 $$: $$ \alpha = (\pm 2, 0, 0, \pm \pm' 2) $$,
- $$ \sigma_3 \tau^\pm \eta^{\pm'} $$: $$ \alpha = (0, \pm 2, \pm' 2, 0) $$,
- $$ \sigma^\pm \tau^{\pm'} \eta^{\pm''} $$: $$ \alpha = (\pm 2, \pm' 2, \pm'' 2, \pm \pm' \pm'' 2 ) $$.

The simple roots are

$$
\begin{array}{}
  \alpha_1 = (0, 0, 2, 2) & \alpha_2 = (0, 2, -2, 0) & \alpha_3 = (0, 0, 2, -2) & \alpha_4 = (2, -2, -2, 2).
\end{array}
$$

This gives us $$ Sp(8) $$ or $$ C_4 $$.

### 19.B.

We can take exactly the same $$ H_1 $$, $$ H_2 $$, $$ H_3 $$, and $$ H_4 $$ as in the Solution to 19.A. and
the following linear combinations of the other generators.

$$
\begin{array}{}
  \sigma^\pm \pm' \sigma^\pm \tau_3 \eta_3 & \tau^\pm \pm' \sigma_3 \tau^\pm \eta_3  \\
  \sigma_3 \eta^\pm \pm' \tau_3 \eta^\pm  \\
  \sigma^\pm \eta^{\pm'} & \tau^\pm \eta^{\pm'}  \\
  \sigma^\pm \tau^{\pm'} \eta_3
\end{array}
$$

There are $$ 8 + 4 + 8 + 4 = 24 $$ of these. So the number is right.

Below are the roots, presented in the same order as the generators above.

$$
\begin{array}{}
  (\pm 2, 0, 0, \pm \pm' 2) & (0, \pm 2, 0, \pm \pm' 2)  \\
  (0, 0, \pm 2, \pm \pm' 2)  \\
  (\pm 2, 0, \pm' 2, 0) & (0, \pm 2, \pm' 2, 0)  \\
  (\pm 2, \pm' 2, 0, 0)
\end{array}
$$

The simple roots are

$$
\begin{array}{}
  \alpha_1 = (2, -2, 0, 0) & \alpha_2 = (0, 2, -2, 0) & \alpha_3 = (0, 0, 2, -2) & \alpha_4 = (0, 0, 2, 2).
\end{array}
$$

This gives us $$ SO(8) $$ or $$ D_4 $$.

## Chapter 20

### 20.A.

Let's say the $$ \Pi $$-system can be decomposed into subsystems $$ \Pi_1 $$ and $$ \Pi_2 $$.

We can show all the postive roots are made either exclusively of simple roots in $$ \Pi_1 $$, or exclusively of
simple roots in $$ \Pi_2 $$, by constructing the whole algebra following the procedure described in Section 8.2,
point 6. (8.18) guarantees that $$ \alpha + \beta $$ is not a root if $$ \alpha \in \Pi_1 $$ and
$$ \beta \in \Pi_2 $$ because then we have $$ \alpha \cdot \beta = 0 $$.

This shows that not only the simple roots, but also all non-zero roots, can be split into two orthogonal subsets
of dimensions $$ M $$ and $$ N $$ respectively. This also implies any root from one set commutes from any root
from the other set.

The Cartan genrators can be linearly recombined in such a way that the first $$ M $$ Cartan generators commute
with all the generators in the second set, while the other $$ N $$ Cartan generators commute with all the generators
in the first set. (This is mearly a matter of elementary linear algebra.)

Now we have decomposed the whole root system into two subsystems.

### 20.B.

We have the following possibilities.
- $$ E_6 $$ by removing any of the three branch circles,
- $$ A_1 \times A_5 $$ by removing any of the three circles between a branch and the center,
- $$ A_2 \times A_2 \times A_2 $$ by removing the circle in the center.

### 20.C.

Noticing $$ SO(12) = D_6 $$, we have the following possibilities.
- $$ D_6 $$ by removing any of the four branch circles,
- $$ A_1 \times A_1 \times D_4 $$ by removing any of the two branching circles,
- $$ A_3 \times A_3 $$ by removing the circle in the center.

$$ D_4 $$ can be further broken down to $$ A_1 \times A_1 \times A_1 \times A_1 $$.

## Chapter 21

### 21.A.

We can simply replace (21.12) by

$$
\begin{eqnarray}
  H_1 = \frac{1}{2} \sigma_2  \\
  H_2 = \frac{1}{2} \tau_2,
\end{eqnarray}
$$

and (21.30) by

$$
\begin{eqnarray}
  M_{1, 5} &=& \frac{1}{2} \sigma_3  \\
  M_{2, 5} &=& \frac{1}{2} \sigma_1  \\
  M_{3, 5} &=& \frac{1}{2} \sigma_2 \tau_3  \\
  M_{4, 5} &=& \frac{1}{2} \sigma_2 \tau_1.
\end{eqnarray}
$$

Essentially we are permutating $$ 3 \rightarrow 2 \rightarrow 1 \rightarrow 3 $$ compared to the representation
given in the book. Note that $$ \pm \frac{e^1}{2} $$ and $$ \pm \frac{e^2}{2} $$ are now the eigenvectors of
$$ \sigma_2 $$ and $$ \tau_2 $$.

It is straightforward to verify the commutation relations (21.3), and that the generators
acting on the states give the expected results.

(21.54) now becomes

$$
\begin{eqnarray}
  -r^1 {\sigma_{1,3}}^* {r^1}^{-1} &=& \sigma_{1,3}  \\
  -r^1 {\sigma_2}^* {r^1}^{-1} \cdot r^2 {\tau_{1,3}}^* {r^2}^{-1} &=& \sigma_2 \tau_{1,3}.
\end{eqnarray}
$$

$$ r^1 $$ must anticommute with both $$ \sigma_1 $$ and $$ \sigma_3 $$. Thus we can take $$ r^1 = \sigma_2 $$.
$$ r^2 $$ must commute with both $$ \tau_1 $$ and $$ \tau_3 $$. Thus we can take $$ r^2 = I $$.
Therefore we have $$ R = \sigma_2 \otimes I $$, an antisymmetric matrix.

### 21.B.

For convenience, we can choose to remove the first two dimensions of the $$ SO(2 n + 1) $$ by setting
every generator $$ M_{ij} = 0 $$ if $$ i \in \{ 1, 2 \} $$ or $$ j \in \{ 1, 2 \} $$ in the defining representation.

Now $$ | \pm e^1/2 \rangle $$ in (21.11) become two singletons unaffected by any generators. Essentially, the
$$ 2^n $$-dimensional spinor representation of $$ SO(2 n + 1) $$ breaks down to two $$ 2^{n - 1} $$-dimensional
spinor representation of $$ SO(2 n - 1) $$.

## Chapter 22

### 22.A.

Without loss of generality, we can assign the first $$ 2m $$ dimensions to $$ SO(2m) $$ and the rest
to $$ SO(2n - 2m) $$. Any $$ M_{i j} $$ where $$ i \le 2m $$ and $$ j > 2m $$ or the other way around
is set to 0. It is easy to see that all the $$ SO(2m) $$ generators, as well as all the $$ SO(2n - 2m) $$
generators, are contained in the remaining $$ M_{ij} $$s. Since the rank remains the same,
$$ SO(2m) \times SO(2n - 2m) $$ is a regular maximal subalgebra of $$ SO(2n) $$.

The roots of the $$ SO(2m) \times SO(2n - 2m) $$ subalgebra have the form $$ \pm e^j \pm e^k $$, where $$ j \neq k $$
and $$ j, k \le m $$ or $$ j, k > m $$. A root operates either exclusively on the first $$ m $$, or exclusively
on the last $$ n - m $$, dimensions of the spinor representations. Therefore $$ \prod_{j = 1}^m \eta_j $$ and
$$ \prod_{j = m + 1}^n \eta_j $$ will not change value where $$ \eta_j $$ are defined in (22.5).

Therefore we have

$$
\begin{eqnarray}
  D^{n - 1} &\rightarrow& D^{m - 1} \otimes D^{n - m} \oplus D^m \otimes D^{n - m - 1}  \\
  D^n &\rightarrow& D^m \otimes D^{n - m} \oplus D^{m - 1} \otimes D^{n - m - 1}.
\end{eqnarray}
$$

### 22.B.

Without loss of generality, we can assign the first $$ 2m $$ dimensions to $$ SO(2m) $$ and the rest
to $$ SO(2n - 2m + 1) $$. Any $$ M_{i j} $$ where $$ i \le 2m $$ and $$ j > 2m $$ or the other way around
is set to 0. It is easy to see that all the $$ SO(2m) $$ generators, as well as all the $$ SO(2n - 2m + 1) $$
generators, are contained in the remaining $$ M_{ij} $$s. Since the rank remains the same,
$$ SO(2m) \times SO(2n - 2m + 1) $$ is a regular maximal subalgebra of $$ SO(2n + 1) $$.

The roots of the $$ SO(2m) \times SO(2n - 2m + 1) $$ subalgebra fall under one of the two categories.
- $$ \pm e^j \pm e^k $$, where $$ j \neq k $$ and $$ j, k \le m $$, or
- $$ \pm e^j \pm e^k $$, where $$ j \neq k $$ and $$ j, k > m $$ or $$ \pm e^j $$ where $$ j > m $$.

A root in the first category operates exclusively on the first $$ m $$ dimensions of the spinor
representation, and $$ \prod_{j = 1}^m \eta_j $$ will not change value where $$ \eta_j $$ are defined in (22.5).

A root in the second category operates exclusively on the last $$ n - m $$ dimensions of the spinor
representation, and $$ \prod_{j = m + 1}^n \eta_j $$ can change value where $$ \eta_j $$ are defined in (22.5).

Therefore we have

$$ D^n \rightarrow D^{m - 1} \otimes D^{n - m} \oplus D^m \otimes D^{n - m}. $$

### 22.C.

The fact that the Dynkin diagram of $$ SO(4) $$ is made of two disconnected dots, combined with the solution to
Problem 20.A, is a proof that $$ SO(4) $$ is not simple.

From (22.3), we know the four roots of $$ SO(4) $$ are $$ \pm e^1 \pm e^2 $$. We can replace the Cartan generators
$$ H_1 $$ and $$ H_2 $$ by the following

$$
\begin{eqnarray}
  H_x = \frac{1}{2} (H_1 + H_2)  \\
  H_y = \frac{1}{2} (H_1 - H_2).
\end{eqnarray}
$$

It is easy to verify that they commute with each other. Furthermore, we have

$$
\begin{array}{}
  [H_x, E_{+ e^1 + e^2}] = +E_{+ e^1 + e^2} & [H_y, E_{+ e^1 + e^2}] = 0  \\
  [H_x, E_{- e^1 - e^2}] = -E_{- e^1 - e^2} & [H_y, E_{- e^1 - e^2}] = 0  \\
  [H_x, E_{+ e^1 - e^2}] = 0 & [H_y, E_{+ e^1 - e^2}] = +E_{+ e^1 - e^2}  \\
  [H_x, E_{- e^1 + e^2}] = 0 & [H_y, E_{- e^1 + e^2}] = -E_{- e^1 + e^2}.
\end{array}
$$

So $$ H_x $$, $$ E_{+ e^1 + e^2} $$, and $$ E_{- e^1 - e^2} $$ form an $$ SU(2) $$, while
So $$ H_y $$, $$ E_{+ e^1 - e^2} $$, and $$ E_{- e^1 + e^2} $$ form another $$ SU(2) $$.
These two $$ SU(2) $$ commute with each other.

The arguments in the chapter apply because nowhere the algebra is assumed to be simple.
In fact, we have $$ \mu_1 = \frac{1}{2} \alpha_1 $$ and $$ \mu_2 = \frac{1}{2} \alpha_2 $$, forming
two independent $$ SU(2) $$ doublets.

### 22.D.

We can almost read off the answers from the Dynkin diagrams. $$ \alpha_1 $$, $$ \alpha_2 $$, and
$$ \alpha_3 $$ of $$ SO(6) $$ correspond to $$ \alpha_2 $$, $$ \alpha_1 $$, and $$ \alpha_3 $$ of
$$ SU(4) $$. (The order of the two edge circles can be exchanged.)

This is why $$ D^1 $$ of $$ SU(4) $$ corresponds $$ D^2 $$ (or $$ D^3 $$) of $$ SO(6) $$, and $$ D^2 $$
of $$ SU(4) $$ corresponds $$ D^1 $$ of $$ SO(6) $$. The 10 of $$ SU(4) $$, being the symmetric tensor
product of two 4s (i.e. $$ 2 D^1 $$), corresponds to $$ 2 D^2 $$ (or $$ 2 D^3 $$) in $$ SO(6) $$.

Comparing the corresponding simple roots, we have
- $$ SU(4) $$: $$ \alpha^1 = (1, 0, 0) $$, $$ \alpha^2 = (-\frac{1}{2}, \frac{\sqrt{3}}{2}, 0) $$,
$$ \alpha^3 = (0, -\frac{1}{\sqrt{3}}, \frac{2}{\sqrt{6}}) $$.
- $$ SO(6) $$: $$ \alpha^2 = (0, 1, -1) $$, $$ \alpha^1 = (1, -1, 0) $$, $$ \alpha^3 = (0, 1, 1) $$.

(See (13.11) and (22.2).)

To make the correspondence work, we can replace the Cartan generators of $$ SO(6) $$ by the following

$$
\begin{eqnarray}
  H_x &=& \frac{1}{2} H_2 - \frac{1}{2} H_3  \\
  H_y &=& \frac{1}{\sqrt{3}} H_1 - \frac{1}{2 \sqrt{3}} H_2 - \frac{1}{2 \sqrt{3}} H_3  \\
  H_z &=& \frac{1}{\sqrt{6}} H_1 + \frac{1}{\sqrt{6}} H_2 + \frac{1}{\sqrt{6}} H_3.
\end{eqnarray}
$$

It is then easy to verify that the commutation relationships between $$ H_x $$, $$ H_y $$, $$ H_z $$,
$$ \alpha^2 $$, $$ \alpha^1 $$, and $$ \alpha^3 $$ in $$ SO(6) $$ are exactly the same as those between
$$ H_1 $$, $$ H_2 $$, $$ H_3 $$, $$ \alpha^1 $$, $$ \alpha^2 $$, $$ \alpha^3 $$ in $$ SU(4) $$.

## Chapter 23

### 23.A.

From

$$
\begin{eqnarray}
  (1 - 1)^{2n + 1} &=& \sum_{j = 0}^n {2n + 1 \choose 2j + 1} - \sum_{j = 0}^n {2n + 1 \choose 2j} &=& 0  \\
  (1 + 1)^{2n + 1} &=& \sum_{j = 0}^n {2n + 1 \choose 2j + 1} + \sum_{j = 0}^n {2n + 1 \choose 2j} &=& 2^{2n + 1},
\end{eqnarray}  
$$

we have

$$ \sum_{j = 0}^n {2n + 1 \choose 2j + 1} = \sum_{j = 0}^n {2n + 1 \choose 2j} = 2^{2n}. $$

This verifies the dimensionality of (23.49). For (23.50), simply replace $$ 2n $$ by $$ 2n - 1 $$.

### 23.B.

Note that the vector representation of $$ SO(4) $$ is not $$ D^1 $$ .

I do not see how (23.9) can be useful for our purpose here, other than restating (23.3), which says that
$$ \Gamma $$ matrices transform as vector operators. Therefore we only need to see how $$ \Gamma_i $$,
where $$ i = 1 \dots 2n $$, transform under $$ T_a $$, as defined in (23.46).

(23.48) shows that $$ A^\dagger_j $$ transforms like $$ D^1 $$ under $$ SU(n) $$. Notice that each
$$ A^\dagger_j $$ is a particular linear combination of $$ \Gamma_{2j - 1} $$ and $$ \Gamma_{2j} $$. The
final missing piece is that $$ A_j $$ transforms like $$ D^{n - 1} $$, the complex conjugate of $$ D^1 $$,
under $$ SU(n) $$. $$ A_j $$ is another linear combination of $$ \Gamma_{2j - 1} $$ and $$ \Gamma_{2j} $$.

It should be fairly obvious now that the vector representation of $$ SO(2n) $$, made up of $$ \Gamma_i $$s,
transforms like $$ D^1 \oplus D^{n - 1} $$ under $$ SU(n) $$.

One can reach the same conclusion by observing how the $$ D^1 $$ representation of $$ SO(2n) $$ for $$ n \ge 3 $$
is built by raising and lowering $$ \mu^1 $$ with simple roots in $$ SO(2n) $$, and how it breaks into two separate
representations of $$ SU(n) $$ when $$ \alpha^n $$ in $$ SO(2n) $$ is removed. (Also notice that the first
$$ n - 1 $$ simple roots of $$ SO(2n) $$ are also simple roots of $$ SU(n) $$.)

### 23.C.

To determine $$ \lambda $$, We only need to check the value of $$ u^{123} $$ as follows.

$$ u^{123} = \lambda \cdot 3! \cdot u^{456} = \lambda^2 \epsilon^{456123} \cdot (3!)^2 \cdot u^{123} = - \lambda^2 \cdot (3!)^2 \cdot u^{123} $$

Therefore we have

$$ \lambda = \pm \frac{i}{6}. $$

### 23.D.a.

Directly applying (23.49), we have

$$ \begin{array}{} D^7 \rightarrow [1] \oplus [3] \oplus [5] \oplus [7] & D^6 \rightarrow [0] \oplus [2] \oplus [4] \oplus [6] \end {array}. $$

### 23.D.b

This problem is simply Problem 22.A, followed by 23.D.a. We have

$$
\begin{eqnarray}
  D^7 &\rightarrow& D^2 \otimes \bigg( [1] \oplus [3] \oplus [5] \bigg) \oplus D^1 \otimes \bigg( [0] \oplus [2] \oplus [4] \bigg)  \\
  D^6 &\rightarrow& D^2 \otimes \bigg( [0] \oplus [2] \oplus [4] \bigg) \oplus D^1 \otimes \bigg( [1] \oplus [3] \oplus [5] \bigg).
\end{eqnarray}
$$

## Chapter 24

### 24.A.

The first half of this problem can be tackled in exactly the same way as Problems 19.A. and 19.B. It is easy to verify that
the algebra closes.

We can take $$ \frac{1}{2} \sigma_3 $$, $$ \frac{1}{2} \tau_3 $$, $$ \frac{1}{2} \eta_3 $$, $$ \frac{1}{2} \eta_3 \rho_3 $$,
and $$ \frac{1}{2} \sigma_3 \tau_3 \rho_3 $$ as the five Cartan generators $$ H_1 $$, $$ H_2 $$, $$ H_3 $$, $$ H_4 $$, and $$ H_5 $$.
(Note that (6.2) is satisfied so they are correctly normalized relative to each other.) Other than the five Cartan generators,
we want linear combinations of the other generators $$ x $$ satisfying $$ [H_i, x] = \alpha(i, x) x $$.

Below is the full list of such linear combinations.
- $$ (\pm 1, \pm' 1, 0, 0, 0)$$ : $$ \frac{1}{2} \sigma^\pm \tau^{\pm'} \rho_3 $$,
- $$ (\pm 1, 0, \pm' 1, 0, 0)$$ : $$ \frac{1}{2} \sigma^\pm \eta^{\pm'} \rho_2 $$,
- $$ (\pm 1, 0, 0, \pm' 1, 0)$$ : $$ \frac{1}{2} \sigma^\pm (\rho_1 \pm' i \eta_3 \rho_2) $$,
- $$ (\pm 1, 0, 0, 0, \pm \pm' 1)$$ : $$ \frac{1}{2} \sigma^\pm (1 \pm' \tau_3 \rho_3) $$,
- $$ (0, \pm 1, \pm' 1, 0, 0)$$ : $$ \frac{1}{2} \tau^\pm \eta^{\pm'} \rho_1 $$,
- $$ (0, \pm 1, 0, \mp' 1, 0)$$ : $$ \frac{1}{2} \tau^\pm (\rho_2 \pm' i \eta_3 \rho_1) $$,
- $$ (0, \pm 1, 0, 0, \pm \pm' 1)$$ : $$ \frac{1}{2} \tau^\pm (1 \pm' \sigma_3 \rho_3) $$,
- $$ (0, 0, \pm 1, \pm \pm' 1, 0)$$ : $$ \frac{1}{2} \eta^\pm (1 \pm' \rho_3) $$,
- $$ (0, 0, \pm 1, 0, \mp' 1)$$ : $$ \frac{1}{2} \eta^\pm (\sigma_3 \rho_2 \pm' i \tau_3 \rho_1) $$,
- $$ (0, 0, 0, \pm 1, \pm' 1)$$ : $$ \frac{1}{2} \sigma_3(\rho_1 \pm i \eta_3 \rho_2) \pm' \frac{1}{2} \tau_3(i \rho_2 \pm \eta_3 \rho_1) $$.

Comparing this to (22.3) we see it is clearly $$ SO(10) $$.

Among the subset $$ \vec{\eta} (1 + \rho_3) / 4 $$, $$ \eta^\pm (1 + \rho_3) / 4 $$ have root vectors $$ (0, 0, \pm 1, \pm 1, 0) $$,
and $$ \eta_3 (1 + \rho_3) / 4 $$ has root vector $$ (0, 0, 0, 0, 0) $$. This strongly suggest that the generators related to the
1st, 2nd, and 5th dimensions should be used to form the $$ SO(6) = SU(4) $$, while the generators related to the 3rd and the 4th diemensions
should be used to form the $$ SO(4) $$. All the other generators that spans across the two should be removed.

The last step is to rewrite the $$ SO(4) $$ generators as $$ SU(2) \times SU(2) $$ generators, as described in the solution to
Problem 22.C.

To summarize, we have the following generators in the $$ SU(2) \times SU(2) \times SU(4) $$ subalgebra. (The $$ SU(2) $$ generators
are rescaled by a factor of $$ \frac{1}{\sqrt{2}} $$ to conform to the standard $$ SU(2) $$ structure constants.)
- $$ SU(2) $$: $$ \vec{\eta} (1 + \rho_3) / 4 $$,
- $$ SU(2) $$: $$ \vec{\eta} (1 - \rho_3) / 4 $$,
- $$ SU(4) $$: $$ \frac{1}{2} \sigma_3 $$, $$ \frac{1}{2} \tau_3 $$, $$ \frac{1}{2} \sigma_3 \tau_3 \rho_3 $$;
$$ \frac{1}{2} \sigma^\pm \tau^{\pm'} \rho_3 $$, $$ \frac{1}{2} \sigma^\pm (1 \pm' \tau_3 \rho_3) $$,
$$ \frac{1}{2} \tau^\pm (1 \pm' \sigma_3 \rho_3) $$.

### 24.B.

Since the highest weight of $$ D^5 $$ is $$ \mu^5 $$, we know the $$ SO(10) $$ representation with highest weight $$ 2 \mu^5 $$
must be in $$ [D^5 \otimes D^5]_S = (1) \oplus (5) $$. (See Section 23.5.) Therefore the representation that we are looking for is
$$ (5) $$, which has 126 dimensions.

## Chapter 25

### 25.A.

$$ D^1 $$ has weights $$ \pm e^j $$, which break up into two sets:
- $$ \pm e^1 $$ $$ \pm e^2 $$ transforming like $$ (2, 2, 1, 1) $$, and
- $$ \pm e^3 $$ $$ \pm e^4 $$ transforming like $$ (1, 1, 2, 2) $$.

Therefore we have $$ D^1 \rightarrow (2, 2, 1, 1) \oplus (1, 1, 2, 2) $$.

Similarly, we have $$ D^4 \rightarrow (2, 1, 1, 2) \oplus (1, 2, 2, 1) $$.

### 25.B.

An obvious attempt would be to map $$ SO(5) $$ roots $$ \pm e^1 \pm e^2 $$ and $$ \pm e^j $$ into
$$ SO(8) $$'s $$ D^3 $$ weights using only the first and second lines of (25.15) (since $$ e^3 $$
and $$ e^4 $$ do not exist in $$ SO(5) $$). We can then generate a $$ SO(5) $$ vector representation
starting with $$ \mu^3 $$ of $$ SO(8) $$ as the highest weight. The problem is that we get one extra
null weight $$ (0, 0, 0, 0) $$ that is not in $$ D^3 $$ of $$ SO(8) $$. This hints the impossibility
of the task. Below follows a proof.

The vector representation of $$ SO(5) $$ has the following weights: $$ (\pm 1, 0) $$, $$ (0, \pm 1) $$,
and $$ (0, 0) $$. The distances between these weights have ratio $$ 1 : \sqrt{2} : 2 $$.

The distance ratio of the $$ SO(8) $$ spinor representation weights (of either $$ D^3 $$ or $$ D^4 $$)
is $$ 1 : \sqrt{2} $$.

It is therefore impossible to find five eigenvectors in either $$ D^3 $$ or $$ D^4 $$ of $$ SO(8) $$
that transform like $$ D^1 $$ of $$ SO(5) $$.

## Chapter 26

### 26.A.

We can choose the following Cartan generators.

$$ [H_i]_{jk} = \frac{1}{\sqrt{2}} (\delta_{i, j} \delta_{i, k} - \delta_{i + n, j} \delta_{i + n, k}) $$

With this choice, $$ D^1 $$ weights are $$ \pm e^j / \sqrt{2} $$.

### 26.B.

The tensor product $$ D^1 \otimes D^2 $$ can be broken into two parts.

$$ u^\alpha v^{\beta \gamma} = \frac{1}{3} (A^{\alpha \beta \gamma} + M^{\alpha \beta \gamma}), $$

where

$$ A^{\alpha \beta \gamma} = u^\alpha v^{\beta \gamma} + u^\beta v^{\gamma \alpha} + u^\gamma v^{\alpha \beta} $$

is completely antisymmetric and cyclic, and

$$ M^{\alpha \beta \gamma} = 2 u^\alpha v^{\beta \gamma} + u^\beta v^{\alpha \gamma} + u^\gamma v^{ \beta \alpha} $$

has the symmetry property

$$ M^{\alpha \beta \gamma} + M^{\beta \gamma \alpha} + M^{\gamma \alpha \beta} = 0. $$

This would be the end of it if it were $$ SU(6) $$. $$ A^{\alpha \beta \gamma} $$ would have $$ {6 \choose 3} = 20 $$
dimensions, while $$ M^{\alpha \beta \gamma} $$ would have $$ {6 \choose 3} \cdot 2 + 6 \cdot 5 = 70 $$ dimensions.

But since we are dealing with $$ Sp(6) $$ instead, both $$ A^{\alpha \beta \gamma} $$ and $$ M^{\alpha \beta \gamma} $$
can be reduced because the combination

$$ O^\gamma = [R^{-1}]_{\beta \alpha} u^\alpha v^{\beta \gamma}  $$

is a non-zero vector of 6 dimensions. The combination

$$ X^{\alpha \beta \gamma} \equiv A^{\alpha \beta \gamma} - \frac{1}{n} [R]^{\alpha \beta} [R^{-1}]_{\beta' \alpha'} u^{\alpha'} v^{\beta' \gamma} $$

satisfies

$$ [R^{-1}]_{\beta \alpha} X^{\alpha \beta \gamma} = 0. $$

Therefore $$ X^{\alpha \beta \gamma} $$ is the irreducible representation $$ D^3 $$ and has $$ 20 - 6 = 14 $$ dimensions.

Similarly, the combination

$$ Y^{\alpha \beta \gamma} \equiv M^{\alpha \beta \gamma} - \frac{1}{2n} [R]^{\alpha \beta} [R^{-1}]_{\beta' \alpha'} u^{\alpha'} v^{\beta' \gamma} $$

satisfies

$$ [R^{-1}]_{\beta \alpha} Y^{\alpha \beta \gamma} = 0. $$

Therefore $$ Y^{\alpha \beta \gamma} $$ is the irreducible representation $$ D^1 + D^2 $$ and has $$ 70 - 6 = 64 $$ dimensions.

We can write the product in terms of irreducible combination as

$$ u^\alpha v^{\beta \gamma} = \frac{1}{3} X^{\alpha \beta \gamma} + \frac{1}{3} Y^{\alpha \beta \gamma} + \frac{1}{2} [R]^{\alpha \beta} O^\gamma. $$

## Chapter 27

### 27.A.

Using (27.26), (27.27), and (27.28), we have the following in $$ SU(3) $$.

$$
\begin{eqnarray}
  A(6) &=& A(6) + A(\bar{3}) + A(3) = A(6 \oplus \bar{3}) + 1 = A(3 \otimes 3) + 1 = 3 \cdot 1 + 1 \cdot 3 + 1 = 7  \\
  A(8) &=& A(8) + A(1) = A(8 \oplus 1) = A(3 \otimes \bar{3}) = 3 \cdot (-1) + 1 \cdot 3 = 0  \\
  A(10) &=& A(10) + A(8) = A(10 \oplus 8) = A(6 \otimes 3) = 6 \cdot 1 + 7 \cdot 3 = 27
\end{eqnarray}
$$

### 27.B.

In $$ SU(5) $$, we have

$$ 10 = [5 \otimes 5]_{AS}. $$

In the $$ SU(3) $$ subalgebra, we have

$$ [5 \otimes 5]_{AS} = [(3 + 1 + 1) \otimes (3 + 1 + 1)]_{AS} = \bar{3} + 3 + 3, $$

and therefore

$$ A(10)^{SU(5)} = A(\bar{3})^{SU(3)} + A(3)^{SU(3)} + A(3)^{SU(3)} = 1 = A(5)^{SU(5)}. $$

### 27.C.

For (27.27), we have

$$
\begin{eqnarray}
  &&  A(D_1 \oplus D_2)  \\
  &=& \mathrm{Tr} \left( \{ T^{D_1 \oplus D_2}_a, T^{D_1 \oplus D_2}_b \} T^{D_1 \oplus D_2}_c \right) / d^{abc}  \\
  &=& \mathrm{Tr} \left( \{ \begin{bmatrix} T^{D_1}_a & 0 \\ 0 & T^{D_2}_a \end{bmatrix}, \begin{bmatrix} T^{D_1}_b & 0 \\ 0 & T^{D_2}_b \end{bmatrix} \} \begin{bmatrix} T^{D_1}_c & 0 \\ 0 & T^{D_2}_c \end{bmatrix} \right) / d^{abc}  \\
  &=& \mathrm{Tr} \left( \begin{bmatrix} \{ T^{D_1}_a, T^{D_1}_b \} T^{D_1}_c & 0 \\ 0 & T^{D_2}_a, T^{D_2}_b \} T^{D_2}_c \end{bmatrix} \right) / d^{abc}  \\
  &=& \Big( \mathrm{Tr} \left( \{ T^{D_1}_a, T^{D_1}_b \} T^{D_1}_c \right) + \mathrm{Tr} \left( T^{D_2}_a, T^{D_2}_b \} T^{D_2}_c \right) \Big) / d^{abc}  \\
  &=& A(D_1) + A(D_2).
\end{eqnarray}
$$

For (27.28), we have

$$
\begin{eqnarray}
  &&  A(D_1 \otimes D_2)  \\
  &=& \mathrm{Tr} \left( \{ T^{D_1 \otimes D_2}_a, T^{D_1 \otimes D_2}_b \} T^{D_1 \otimes D_2}_c \right) / d^{abc}  \\
  &=& \mathrm{Tr} \left( \{ T^{D_1}_a \otimes I + I \otimes T^{D_2}_a, T^{D_1}_b \otimes I + I \otimes T^{D_2}_b \} (T^{D_1}_c \otimes I + I \otimes T^{D_2}_c) \right) / d^{abc}  \\
  &=& \mathrm{Tr} \Big( \left( \{ T^{D_1}_a, T^{D_1}_b \} \otimes I + I \otimes \{ T^{D_2}_a, T^{D_2}_b \} + 2 T^{D_1}_a \otimes T^{D_2}_b + 2 T^{D_1}_b \otimes T^{D_2}_a \right) (T^{D_1}_c \otimes I + I \otimes T^{D_2}_c) \Big) / d^{abc}  \\
  &=& \Big( \mathrm{Tr} \left( \{ T^{D_1}_a, T^{D_1}_b \} T^{D_1}_c \otimes I \right) + \mathrm{Tr} \left( I \otimes \{ T^{D_2}_a, T^{D_2}_b \} T^{D_2}_c \right) \Big) / d^{abc}  \\
  &=& \mathrm{dim}(D_1) A(D_2) + \mathrm{dim}(D_2) A(D_1),
\end{eqnarray}
$$

where the fourth equality holds because all other terms lead to zero as all generators in both $$ D_1 $$ and $$ D_2 $$
are traceless and $$ \mathrm{Tr}(A \otimes B) = \mathrm{Tr}(A) \cdot \mathrm{Tr}(B) $$.
